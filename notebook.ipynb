{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpennylane\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mqml\u001b[39;00m \n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_circuit\u001b[39m(n_qubits,n_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,circ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimplified_two_design\u001b[39m\u001b[38;5;124m\"\u001b[39m,fim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, shots\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m      7\u001b[0m     dev \u001b[38;5;241m=\u001b[39m qml\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault.qubit.torch\u001b[39m\u001b[38;5;124m\"\u001b[39m, wires\u001b[38;5;241m=\u001b[39mn_qubits, shots\u001b[38;5;241m=\u001b[39mshots)\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\torch\\__init__.py:1854\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m library\n\u001b[0;32m   1853\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m-> 1854\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations\n\u001b[0;32m   1856\u001b[0m \u001b[38;5;66;03m# Enable CUDA Sanitizer\u001b[39;00m\n\u001b[0;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTORCH_CUDA_SANITIZER\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\torch\\_meta_registrations.py:6242\u001b[0m\n\u001b[0;32m   6238\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6239\u001b[0m                 _meta_lib_dont_use_me_use_register_meta\u001b[38;5;241m.\u001b[39mimpl(op_overload, fn)\n\u001b[1;32m-> 6242\u001b[0m activate_meta()\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\torch\\_meta_registrations.py:6239\u001b[0m, in \u001b[0;36mactivate_meta\u001b[1;34m()\u001b[0m\n\u001b[0;32m   6235\u001b[0m     _meta_lib_dont_use_me_use_register_meta_for_quantized\u001b[38;5;241m.\u001b[39mimpl(\n\u001b[0;32m   6236\u001b[0m         op_overload, fn\n\u001b[0;32m   6237\u001b[0m     )\n\u001b[0;32m   6238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 6239\u001b[0m     _meta_lib_dont_use_me_use_register_meta\u001b[38;5;241m.\u001b[39mimpl(op_overload, fn)\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\torch\\library.py:170\u001b[0m, in \u001b[0;36mLibrary.impl\u001b[1;34m(self, op_name, fn, dispatch_key)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm\u001b[38;5;241m.\u001b[39mimpl(name, dispatch_key \u001b[38;5;28;01mif\u001b[39;00m dispatch_key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompositeImplicitAutograd\u001b[39m\u001b[38;5;124m\"\u001b[39m, fn)\n\u001b[1;32m--> 170\u001b[0m _impls\u001b[38;5;241m.\u001b[39madd(key)\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op_impls\u001b[38;5;241m.\u001b[39madd(key)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pennylane as qml \n",
    "import torch\n",
    "\n",
    "def create_circuit(n_qubits,n_layers=None,circ = \"simplified_two_design\",fim=False, shots=None):\n",
    "\n",
    "    dev = qml.device(\"default.qubit.torch\", wires=n_qubits, shots=shots)\n",
    "\n",
    "    def RZRY(params):\n",
    "        #qml.SpecialUnitary(params, wires=range(n_qubits))\n",
    "        #qml.SimplifiedTwoDesign(initial_layer_weights=init_params, weights=params, wires=range(n_qubits))\n",
    "        #qml.AngleEmbedding(params,wires=range(n_qubits))\n",
    "        for q in range(n_qubits):\n",
    "            qml.Hadamard(wires=q)\n",
    "\n",
    "        for w in range(n_layers): \n",
    "            for q in range(n_qubits):\n",
    "                index = w * (2*n_qubits) + q * 2\n",
    "                qml.RZ(params[index],wires=q)\n",
    "                qml.RY(params[index + 1],wires=q)\n",
    "        \n",
    "        qml.broadcast(qml.CNOT , wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "        \n",
    "        return qml.probs(wires=range(n_qubits))\n",
    "\n",
    "    def S2D(init_params,params,measurement_qubits=0,prod_approx=False):\n",
    "        #qml.SpecialUnitary(params, wires=range(n_qubits))\n",
    "        qml.SimplifiedTwoDesign(initial_layer_weights=init_params, weights=params, wires=range(n_qubits))\n",
    "        \n",
    "        #qml.broadcast(qml.CNOT , wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "        if not prod_approx:\n",
    "            return qml.probs(wires=list(range(measurement_qubits)))\n",
    "        else:\n",
    "            return [qml.probs(i) for i in range(measurement_qubits)]\n",
    "\n",
    "    def SU(params):\n",
    "        qml.SpecialUnitary(params, wires=range(n_qubits))\n",
    "        \n",
    "        ZZ = qml.operation.Tensor(qml.PauliZ(0), qml.PauliZ(1))\n",
    "        for i in range(2,n_qubits):\n",
    "            ZZ = qml.operation.Tensor(ZZ, qml.PauliZ(i))\n",
    "\n",
    "        return qml.expval(ZZ)\n",
    "    \n",
    "    def simmpleRZRY(params,cnots=True):\n",
    "        qml.broadcast(qml.Hadamard, wires=range(n_qubits), pattern=\"single\")\n",
    "        qml.broadcast(qml.RZ, wires=range(n_qubits), pattern=\"single\", parameters=params[0])\n",
    "        qml.broadcast(qml.RY, wires=range(n_qubits), pattern=\"single\", parameters=params[1])\n",
    "        if cnots:\n",
    "            qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"chain\")\n",
    "\n",
    "            return qml.expval(qml.PauliZ(n_qubits-1))\n",
    "        else:\n",
    "            ZZ = qml.operation.Tensor(qml.PauliZ(0), qml.PauliZ(1))\n",
    "            for i in range(2,n_qubits):\n",
    "                ZZ = qml.operation.Tensor(ZZ, qml.PauliZ(i))\n",
    "\n",
    "            return qml.expval(ZZ)\n",
    "        \n",
    "    def RY(params,y=True,probs=False,prod=False, entanglement=None):\n",
    "        #qml.broadcast(qml.Hadamard, wires=range(n_qubits), pattern=\"single\")\n",
    "        qml.broadcast(qml.RY, wires=range(n_qubits), pattern=\"single\", parameters=params)\n",
    "        #qml.broadcast(qml.CZ, wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "\n",
    "        if entanglement==\"all_to_all\":\n",
    "            qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "        \n",
    "        if y==True:\n",
    "            #YY = qml.operation.Tensor(qml.PauliY(0), qml.PauliY(1))\n",
    "            YY = [qml.PauliZ(0), qml.PauliZ(1)]\n",
    "            for i in range(2,n_qubits):\n",
    "                #YY = qml.operation.Tensor(YY, qml.PauliY(i))\n",
    "                YY.append(qml.PauliZ(i))\n",
    "            \n",
    "            #return [qml.expval(i) for i in YY]\n",
    "            return qml.expval(YY)\n",
    "\n",
    "        elif probs==False:\n",
    "\n",
    "            ZZ = qml.operation.Tensor(qml.PauliZ(0), qml.PauliZ(1))\n",
    "            #ZZ = [qml.PauliZ(0), qml.PauliZ(1)]\n",
    "            for i in range(2,n_qubits):\n",
    "                ZZ = qml.operation.Tensor(ZZ, qml.PauliZ(i))        \n",
    "                #ZZ.append(qml.PauliZ(i))        \n",
    "\n",
    "            #return [qml.expval(i) for i in ZZ]\n",
    "            return qml.expval(ZZ)\n",
    "\n",
    "        else:\n",
    "            if prod:\n",
    "                return [qml.probs(i) for i in range(n_qubits)]\n",
    "            else:\n",
    "                return qml.probs(wires=range(n_qubits))\n",
    "            \n",
    "        \n",
    "        \n",
    "    def GHZ(params,measurement_qubits=0):\n",
    "        qml.RY(params,wires=0)\n",
    "        qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"chain\")\n",
    "\n",
    "        return qml.probs(wires=range(measurement_qubits))\n",
    "\n",
    "    def random_product_state(params,gate_sequence=None):\n",
    "                \n",
    "        for i in range(n_qubits):\n",
    "            qml.RY(np.pi / 4, wires=i)\n",
    "\n",
    "        for ll in range(len(params)):\n",
    "\n",
    "            for i in range(n_qubits):\n",
    "                gate_sequence[\"{}{}\".format(ll,i)](params[ll][i], wires=i)\n",
    "\n",
    "            #for i in range(n_qubits - 1):\n",
    "                #qml.CZ(wires=[i, i + 1])\n",
    "    def SEL(params, measurement_qubits=0):\n",
    "        qml.StronglyEntanglingLayers(params, wires=range(n_qubits))\n",
    "        return qml.probs(wires=range(measurement_qubits))\n",
    "    \n",
    "    def RL(params, measurement_qubits=0):\n",
    "        qml.RandomLayers(params, ratio_imprim=0.8 ,imprimitive=qml.CZ, wires=range(n_qubits))\n",
    "        return qml.probs(wires=range(measurement_qubits))\n",
    "    \n",
    "    if circ == \"rzry\":\n",
    "        qcircuit = RZRY\n",
    "    elif circ == \"simplified_two_design\":\n",
    "        qcircuit = S2D\n",
    "    elif circ == \"special_unitary\":\n",
    "        qcircuit = SU\n",
    "    elif circ == \"simpleRZRY\":\n",
    "        qcircuit = simmpleRZRY\n",
    "    elif circ == \"RY\":\n",
    "        qcircuit = RY\n",
    "    elif circ == \"ghz\":\n",
    "        qcircuit = GHZ\n",
    "    elif circ == \"random_product_state\":\n",
    "        qcircuit = random_product_state\n",
    "    elif circ == \"SEL\":\n",
    "        qcircuit = SEL\n",
    "    elif circ == \"RL\":\n",
    "        qcircuit = RL\n",
    "    if not fim:\n",
    "        circuit = qml.QNode(qcircuit, dev,interface=\"torch\", diff_method=\"backprop\")\n",
    "    else:\n",
    "        circuit = qml.QNode(qcircuit, dev)\n",
    "\n",
    "    return circuit\n",
    "\n",
    "def compute_gradient(log_prob, w):\n",
    "    \"\"\"Compute gradient of the log probability with respect to weights.\n",
    "    \n",
    "    Args:\n",
    "    - log_prob (torch.Tensor): The log probability tensor.\n",
    "    - w (torch.Tensor): The weights tensor, with requires_grad=True.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The gradient of log_prob with respect to w, flattened.\n",
    "    \"\"\"\n",
    "    if w.grad is not None:\n",
    "        w.grad.zero_()\n",
    "    log_prob.backward(retain_graph=True)\n",
    "    \n",
    "    if w.grad is None:\n",
    "        raise ValueError(\"The gradient for the given log_prob with respect to w is None.\")\n",
    "    \n",
    "    return w.grad.view(-1).detach().numpy()\n",
    "\n",
    "def policy(probs, policy_type=\"contiguous-like\", n_actions=2, n_qubits=1):\n",
    "\n",
    "    if policy_type == \"contiguous-like\":\n",
    "        return probs\n",
    "    elif policy_type == \"parity-like\":\n",
    "        policy = torch.zeros(n_actions)\n",
    "        for i in range(len(probs)):\n",
    "            a=[]\n",
    "            for m in range(int(np.log2(n_actions))):\n",
    "                if m==0:    \n",
    "                    bitstring = np.binary_repr(i,width=n_qubits)\n",
    "                else:\n",
    "                    bitstring = np.binary_repr(i,width=n_qubits)[:-m]\n",
    "                \n",
    "                a.append(bitstring.count(\"1\") % 2)\n",
    "            policy[int(\"\".join(str(x) for x in a),2)] += probs[i]\n",
    "\n",
    "        return policy    \n",
    "    \n",
    "def compute_policy_and_gradient(args):\n",
    "    n_qubits, shapes, type , n_actions, policy_type, clamp = args\n",
    "\n",
    "    if policy_type == \"parity-like\":\n",
    "        measure_qubits = n_qubits\n",
    "    else:\n",
    "        measure_qubits = int(np.log2(n_actions))\n",
    "\n",
    "    qc = create_circuit(n_qubits, circ=type, fim=False, shots=None)\n",
    "\n",
    "    if type == \"simplified_two_design\":\n",
    "        weights = [np.random.uniform(-np.pi,np.pi,size=shape) for shape in shapes]    \n",
    "        weights_tensor_init = torch.tensor(weights[0], requires_grad=False)\n",
    "        weights_tensor_params = torch.tensor(weights[1], requires_grad=True)\n",
    "        \n",
    "        probs = qc(weights_tensor_init,weights_tensor_params, measurement_qubits=measure_qubits)\n",
    "\n",
    "    else:\n",
    "        weights = [np.random.uniform(-np.pi,np.pi,size=shape) for shape in shapes]    \n",
    "        weights_tensor_params = torch.tensor(weights, requires_grad=True)\n",
    "\n",
    "        probs = qc(weights_tensor_params, measurement_qubits=measure_qubits)\n",
    "\n",
    "    pi = policy(probs, policy_type=policy_type, n_actions=n_actions, n_qubits=n_qubits)\n",
    "    if clamp is not None:\n",
    "        pi = torch.clamp(pi, clamp, 1)\n",
    "\n",
    "    dist = torch.distributions.Categorical(probs=pi)\n",
    "    \n",
    "    action = dist.sample()\n",
    "    log_prob = dist.log_prob(action)\n",
    "\n",
    "    gradient_no_clamp = np.linalg.norm(compute_gradient(log_prob, weights_tensor_params), 2)\n",
    "    return gradient_no_clamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(policy, optimizer, env, n_episodes=1000, max_t=1000, gamma=1.0, print_every=5):\n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "    scores = []\n",
    "    average_scores = []\n",
    "    runtime_sum = 0\n",
    "    for e in range(1, n_episodes):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        # Collect trajectory\n",
    "        for t in range(max_t):\n",
    "            # Sample the action from current policy\n",
    "            if t==0:\n",
    "                state_tensor = torch.tensor(state[0]).float()\n",
    "            else:\n",
    "                state_tensor = torch.tensor(state).float()\n",
    "            action, log_prob, _, = policy.sample(state_tensor)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    # Total expected reward\n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "    #standardized returns\n",
    "        R=0\n",
    "        policy_loss = []\n",
    "        returns = []\n",
    "        for r in rewards[::-1]:\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0,R)\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + np.finfo(np.float32).eps)\n",
    "\n",
    "        for log_prob, R in zip(saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "\n",
    "        policy_unsqueezed = [torch.unsqueeze(loss, 0) for loss in policy_loss]\n",
    "        policy_sum = torch.cat(policy_unsqueezed).sum()\n",
    "\n",
    "    # Backpropagation\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        policy_sum.backward()\n",
    "        optimizer.step()\n",
    "        end_time = time.time()\n",
    "        runtime = end_time-start_time\n",
    "        \n",
    "        runtime_sum += runtime\n",
    "        if e % print_every == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tLast {}\\tEpisodes average reward: {:.2f}\\tRuntime: {:.2f}'.format(e, scores_deque[-1], print_every, np.mean(scores_deque), runtime_sum))\n",
    "            runtime_sum = 0\n",
    "        if np.mean(scores_deque) == 500:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(e, np.mean(scores_deque)))\n",
    "            break\n",
    "    return scores, policy.gradient_list, average_scores  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def save_training_data(self):\n",
    "        ''' \n",
    "        Saves training data into json files\n",
    "        '''\n",
    "        current_directory = os.path.dirname(__file__)\n",
    "        folder_name = f\"{str(self.env_name)}_{self.pqc.policy.post_processing}_{self.pqc.circuit.n_layers}\"\n",
    "        folder_path = os.path.join(current_directory, folder_name)\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "        episode_data = [self.scores_deque, \n",
    "                        self.runtime, \n",
    "                        self.loss.item(), \n",
    "                        tensor_to_list(self.pqc.get_gradients()[0]), \n",
    "                        tensor_to_list(self.pqc.get_gradients()[1])]\n",
    "\n",
    "        if folder_path is not None:\n",
    "            file_path = os.path.join(self.folder_path, f\"{self.file_name}.json\")\n",
    "            if os.path.exists(file_path):\n",
    "                with open(file_path, 'r') as f:\n",
    "                    existing_data = json.load(f)\n",
    "                existing_data.append(episode_data)\n",
    "                with open(file_path, 'w') as f:\n",
    "                    json.dump(existing_data, f, indent=4)\n",
    "            else:\n",
    "                with open(file_path, 'w') as f:\n",
    "                    json.dump([episode_data], f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import gym\n",
    "from collections import deque\n",
    "\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "\n",
    "def tensor_to_list(tensor):\n",
    "    \"\"\"\n",
    "    Convert a tensor or numpy array to a nested list.\n",
    "    \"\"\"\n",
    "    if isinstance(tensor, list):\n",
    "        return [tensor_to_list(t) for t in tensor]\n",
    "    elif isinstance(tensor, dict):\n",
    "        return {key: tensor_to_list(value) for key, value in tensor.items()}\n",
    "    elif isinstance(tensor, np.ndarray):\n",
    "        return tensor.tolist()\n",
    "    elif isinstance(tensor, torch.Tensor):\n",
    "        return tensor.tolist()\n",
    "    else:\n",
    "        return tensor\n",
    "\n",
    "def measure_probs(qubits):\n",
    "    return qml.probs(wires=range(qubits)) \n",
    "\n",
    "def measure_expval_pairs(qubits):\n",
    "\n",
    "    pauli_string = qml.PauliZ(0)\n",
    "    for i in range(1, qubits):\n",
    "        pauli_string = pauli_string @ qml.PauliZ(i)\n",
    "    \n",
    "    expvals = []\n",
    "    expvals.append(qml.expval(pauli_string))\n",
    "    expvals.append(qml.expval(-pauli_string))\n",
    "\n",
    "    return expvals\n",
    "    \n",
    "def create_optimizer_with_lr(params, lr_list, use_amsgrad=False):\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': p, 'lr': lr} for p, lr in zip(params, lr_list)\n",
    "    ], amsgrad=use_amsgrad)\n",
    "    return optimizer\n",
    "\n",
    "def get_function_representation(func):\n",
    "    if callable(func):\n",
    "        # Check if the function is a lambda\n",
    "        if func.__name__ == \"<lambda>\":\n",
    "            # Optionally, check if the function has a custom description attribute\n",
    "            return f\"{func.__module__}.<lambda>\" + (getattr(func, 'description', ''))\n",
    "        else:\n",
    "            return f\"{func.__module__}.{func.__name__}\"\n",
    "    return \"Unknown Function Type\"\n",
    "\n",
    "def jerbi_circuit(n_qubits, n_layers, entanglement, shots, input_scaling, diff_method, weight_init, input_init, measure):\n",
    "\n",
    "    if shots is None:\n",
    "        dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "    else:\n",
    "        dev = qml.device(\"default.qubit\", wires=n_qubits, shots=shots)\n",
    "    \n",
    "    if n_layers < 1:\n",
    "        raise ValueError(\"Number of layers can't take values below 1\")\n",
    "    \n",
    "    weight_shapes = {\"params\": (n_layers + 1, n_qubits, 2),\n",
    "                    \"input_params\": (n_layers, n_qubits, 2)}\n",
    "    init_method   = {\"params\": weight_init,\n",
    "                    \"input_params\": input_init}\n",
    "    \n",
    "    @qml.qnode(dev, interface='torch', diff_method=diff_method)\n",
    "    def qnode(inputs, params, input_params):\n",
    "    #in case n_qubits != input length\n",
    "        if n_qubits > len(inputs) and n_qubits % len(inputs) == 0:\n",
    "            multiplier = n_qubits // len(inputs)\n",
    "            inputs = torch.cat([inputs] * multiplier)\n",
    "        elif n_qubits != len(inputs) and n_qubits % len(inputs) != 0:\n",
    "            raise ValueError('Number of qubits cannot be divided by input lenght')\n",
    "\n",
    "    #hadamard\n",
    "        qml.broadcast(qml.Hadamard, wires=range(n_qubits), pattern=\"single\")\n",
    "        \n",
    "        for layer in range(n_layers):\n",
    "            for wire in range(n_qubits):\n",
    "                qml.RZ(params[layer][wire][0], wires=wire)\n",
    "                qml.RY(params[layer][wire][1], wires=wire)\n",
    "\n",
    "            qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=entanglement)\n",
    "\n",
    "            if input_scaling:\n",
    "                for wire in range(n_qubits):\n",
    "                    qml.RY(input_params[layer][wire][0] * inputs[wire], wires=wire)\n",
    "                    qml.RZ(input_params[layer][wire][1] * inputs[wire], wires=wire)\n",
    "            else:\n",
    "                for wire in range(n_qubits):\n",
    "                    qml.RY(inputs[wire], wires=wire)\n",
    "                    qml.RZ(inputs[wire], wires=wire)\n",
    "\n",
    "        for wire in range(n_qubits):\n",
    "            qml.RZ(params[-1][wire][0], wires=wire)\n",
    "            qml.RY(params[-1][wire][1], wires=wire)\n",
    "\n",
    "        return measure(n_qubits)\n",
    "\n",
    "    model = qml.qnn.TorchLayer(qnode, weight_shapes=weight_shapes, init_method=init_method)  \n",
    "    \n",
    "    return model\n",
    "    \n",
    "def S2D(n_qubits, n_layers, shots, input_scaling, diff_method, weight_init, input_init, measure_type, observables, measure_qubits):\n",
    "\n",
    "    dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "    observables = observables if observables is not None else None\n",
    "    \n",
    "    shapes = qml.SimplifiedTwoDesign.shape(n_layers=n_layers, n_wires=n_qubits)\n",
    "\n",
    "    weight_shapes = {\"params\": shapes[1],\n",
    "                     \"input_params\": shapes[0]}\n",
    "    \n",
    "    init_method   = {\"params\": weight_init,\n",
    "                     \"input_params\": input_init}\n",
    "\n",
    "    @qml.qnode(dev, interface='torch', diff_method='parameter-shift')\n",
    "    def qnode(inputs, params, input_params):\n",
    "\n",
    "        return measure_selection(measure_type, observables, measure_qubits)\n",
    "\n",
    "    model = qml.qnn.TorchLayer(qnode, weight_shapes=weight_shapes, init_method=init_method)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircuitGenerator(nn.Module):\n",
    "\n",
    "    def __init__(self, n_qubits, n_layers, entanglement = \"all_to_all\", shots = None, input_scaling = True,\n",
    "                design = 'jerbi_circuit', diff_method = 'backprop', weight_init = torch.nn.init.normal_, \n",
    "                input_init = torch.nn.init.ones_, measure = None):\n",
    "        super(CircuitGenerator, self).__init__()\n",
    "        '''\n",
    "\n",
    "        Creates a parameterized quantum circuit based on the arguments:\n",
    "\n",
    "            n_qubits(int) = Number of qubits\n",
    "            n_layers(int) = Number of layers (0 if no data re-uploading)\n",
    "            entanglement(str) = entaglement pattern (qml.broadcast patterns)\n",
    "            shots(int) = Number of times the circuit gets executed\n",
    "            input_scaling(bool) = Input parameters are used if True (input*input_params)\n",
    "            design(str) = The PQC ansatz design ('jerbi_circuit')\n",
    "            diff_method(str) = Differentiation method ('best', 'backprop', 'parameter-shift', ...)\n",
    "            weight_init (function) = How PQC weights are initialized (.uniform_, .ones_, ...)\n",
    "            input_init (function) = How input weights are initialized (.uniform_, .ones_, ...)\n",
    "            measure (function) = Measure function that takes n_qubits as an argument (measure_probs, measure_expval_pairs, any defined by the user)\n",
    "            \n",
    "        '''\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.entanglement = entanglement\n",
    "        self.shots = shots\n",
    "        self.input_scaling = input_scaling\n",
    "        self.design = design\n",
    "        self.diff_method = diff_method\n",
    "        self.weight_init = weight_init\n",
    "        self.input_init = input_init\n",
    "        if measure is None:\n",
    "            self.measure = measure_probs\n",
    "        else:\n",
    "            self.measure = measure\n",
    "\n",
    "\n",
    "        if self.design == 'jerbi_circuit':\n",
    "            self.circuit = jerbi_circuit(n_qubits = self.n_qubits,\n",
    "                                        n_layers = self.n_layers,\n",
    "                                        entanglement = self.entanglement,\n",
    "                                        shots = self.shots,\n",
    "                                        input_scaling = self.input_scaling,\n",
    "                                        diff_method = self.diff_method,\n",
    "                                        weight_init = self.weight_init,\n",
    "                                        input_init = self.input_init,\n",
    "                                        measure = self.measure)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported circuit type\")\n",
    "\n",
    "    def input(self,inputs):\n",
    "\n",
    "        outputs = self.circuit(inputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyType(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_actions, post_processing = 'raw_contiguous', \n",
    "                 beta_scheduling = False, beta = 1, increase_rate = 0.003, \n",
    "                 output_scaling = False, output_init = torch.nn.init.ones_):\n",
    "        super(PolicyType, self).__init__()\n",
    "\n",
    "        '''\n",
    "        Determines the type of policy used based on the arguments:\n",
    "\n",
    "            n_actions(int) = Number of actions\n",
    "            post_processing(str) = Type of policy ('raw_contiguous', 'raw_parity', 'softmax')\n",
    "            beta_scheduling(bool) = Inverse temperature parameter used in softmax (used if set to True)\n",
    "            beta(float) = Beta parameter or inverse temperature (used only for softmax)\n",
    "            increase_rate(float) = Amount added to beta at the end of each episode (used only for softmax)\n",
    "            output_scaling(bool) = Output parameters are used if True\n",
    "            output_init(function) = How the output parameters are initialized\n",
    "            \n",
    "        '''\n",
    "        self.n_actions = n_actions\n",
    "        self.post_processing = post_processing\n",
    "        self.beta_scheduling = beta_scheduling\n",
    "        self.beta = beta\n",
    "        self.increase_rate = increase_rate\n",
    "        self.output_scaling = output_scaling\n",
    "        self.output_init = output_init\n",
    "\n",
    "        if self.output_scaling == True:\n",
    "            self.output_params = nn.parameter.Parameter(torch.Tensor(self.n_actions), requires_grad=True)\n",
    "            self.output_init(self.output_params)\n",
    "        else:\n",
    "            self.register_parameter('w_input', None)\n",
    "\n",
    "    def input(self,probs):\n",
    "        if self.post_processing == 'raw_contiguous':\n",
    "            policy = self.raw_contiguous(probs)\n",
    "        elif self.post_processing == 'raw_parity':\n",
    "            policy = self.raw_parity(probs)\n",
    "        elif self.post_processing == 'softmax':\n",
    "            policy = self.softmax(probs)\n",
    "        elif self.post_processing == 'softmax_probs':\n",
    "            policy = self.softmax_probs(probs)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid post-processing method specified.\")\n",
    "        return policy\n",
    "\n",
    "    def raw_contiguous(self,probs):\n",
    "        \n",
    "        probs_flatten = probs.flatten()\n",
    "        chunk_size = len(probs_flatten) // self.n_actions\n",
    "        remainder = len(probs_flatten) % self.n_actions\n",
    "\n",
    "        policy = []\n",
    "\n",
    "        for i in range(self.n_actions):\n",
    "            start = i * chunk_size\n",
    "            end = (i + 1) * chunk_size\n",
    "\n",
    "            if i < remainder:\n",
    "                end += 1\n",
    "\n",
    "            # Update the original policy list instead of creating a new one\n",
    "            policy.append(sum(probs_flatten[start:end]))\n",
    "\n",
    "        policy_tensor = torch.stack(policy)\n",
    "        return policy_tensor\n",
    "        \n",
    "    def raw_parity(self,probs):\n",
    "\n",
    "        if self.n_actions % 2 != 0:\n",
    "            raise ValueError('For parity-like policy, n_actions must be an even number')\n",
    "        \n",
    "        probs_flatten = probs.flatten()\n",
    "        policy = torch.zeros(self.n_actions)\n",
    "        counter = 0\n",
    "        for prob in probs_flatten:\n",
    "            policy[counter] += prob\n",
    "            counter += 1\n",
    "            if counter == self.n_actions:\n",
    "                counter = 0\n",
    "        \n",
    "        return policy\n",
    "    \n",
    "    def softmax(self, probs):\n",
    "        \n",
    "        if self.output_scaling == True:\n",
    "            probs *= self.output_params\n",
    "\n",
    "        scaled_output = probs * self.beta\n",
    "        softmax_output = F.softmax(scaled_output, dim=0)\n",
    "        return softmax_output\n",
    "    \n",
    "    def softmax_probs(self, probs):\n",
    "\n",
    "        if self.output_scaling == True:\n",
    "            probs *= self.output_params\n",
    "            \n",
    "        probs_flatten = probs.flatten()\n",
    "        chunk_size = len(probs_flatten) // self.n_actions\n",
    "        remainder = len(probs_flatten) % self.n_actions\n",
    "\n",
    "        policy = []\n",
    "\n",
    "        for i in range(self.n_actions):\n",
    "            start = i * chunk_size\n",
    "            end = (i + 1) * chunk_size\n",
    "\n",
    "            if i < remainder:\n",
    "                end += 1\n",
    "\n",
    "            policy.append(sum(probs_flatten[start:end]))\n",
    "        policy_tensor = torch.stack(policy)\n",
    "        softmax_output = F.softmax(policy_tensor * self.beta, dim=0)\n",
    "        return softmax_output\n",
    "    \n",
    "    def beta_schedule(self):\n",
    "        if self.beta_scheduling == True:\n",
    "            if self.post_processing == 'softmax' or self.post_processing == 'softmax_probs':\n",
    "                self.beta += self.increase_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumPolicyModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, circuit, policy):\n",
    "        super(QuantumPolicyModel, self).__init__()\n",
    "        self.circuit = circuit\n",
    "        self.policy = policy\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        Input state is fed to the circuit - its output is then fed to the post processing \n",
    "        '''\n",
    "        probs = self.circuit.input(inputs)\n",
    "        probs_processed = self.policy.input(probs)\n",
    "        return probs_processed\n",
    "    \n",
    "    def sample(self, inputs):\n",
    "        '''\n",
    "        Samples an action from the action probability distribution aka policy\n",
    "        '''\n",
    "        policy = self.forward(inputs)\n",
    "        dist = torch.distributions.Categorical(policy)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action), policy\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        '''\n",
    "        Returns the values of each set of parameters\n",
    "        '''\n",
    "        parameter_values = [param.clone().detach().numpy().flatten() for param in self.circuit.parameters()]\n",
    "        return parameter_values\n",
    "    \n",
    "    def get_gradients(self):\n",
    "        '''\n",
    "        Returns the gradient values of each set of parameters from both circuit and policy\n",
    "        '''\n",
    "        gradients = []\n",
    "\n",
    "        # Get gradients from circuit parameters\n",
    "        circuit_gradients = [torch.flatten(param.grad.clone().detach()) if param.grad is not None else torch.flatten(torch.zeros_like(param)) for param in self.circuit.parameters()]\n",
    "        gradients.extend(circuit_gradients)\n",
    "\n",
    "        # Get gradients from policy parameters\n",
    "        policy_gradients = [torch.flatten(param.grad.clone().detach()) if param.grad is not None else torch.flatten(torch.zeros_like(param)) for param in self.policy.parameters()]\n",
    "        gradients.extend(policy_gradients)\n",
    "\n",
    "        return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceUpdate():\n",
    "\n",
    "    def __init__(self, pqc, optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name = None, rundate = None):\n",
    "        \n",
    "        self.pqc = pqc\n",
    "        self.optimizer = optimizer\n",
    "        self.env = gym.make(env_name)\n",
    "        self.env_name = env_name\n",
    "        self.n_episodes = n_episodes\n",
    "        self.max_t = max_t\n",
    "        self.gamma = gamma\n",
    "        self.scores_deque = deque(maxlen=print_every)\n",
    "        self.scores = []\n",
    "        self.print_every = print_every\n",
    "        self.verbose = verbose\n",
    "        self.file_name = file_name\n",
    "        self.rundate = rundate\n",
    "        self.running_reward = 10\n",
    "        self.done = False\n",
    "\n",
    "    def get_trajectory(self):\n",
    "        '''\n",
    "        Gets a trajectory based on the running policy until it runs out of bounds or solves the envinronment\n",
    "        '''\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        state = self.env.reset()\n",
    "        for t in range(self.max_t):\n",
    "            if t == 0:\n",
    "                state_tensor = torch.tensor(state[0]).float()\n",
    "            else:\n",
    "                state_tensor = torch.tensor(state).float()\n",
    "            action, log_prob, _, = self.pqc.sample(state_tensor)\n",
    "            state, reward, done, _, _ = self.env.step(action)\n",
    "            \n",
    "            self.saved_log_probs.append(log_prob)\n",
    "            self.rewards.append(reward)\n",
    "\n",
    "            if done:\n",
    "                self.scores_deque.append(sum(self.rewards))\n",
    "                self.scores.append(sum(self.rewards))\n",
    "                break\n",
    "\n",
    "    def update_policy(self):\n",
    "        '''\n",
    "        Computes the loss and gradients and updates the policy via gradient methods\n",
    "        '''\n",
    "\n",
    "        R = 0\n",
    "        policy_loss = []\n",
    "        returns = []\n",
    "        for r in self.rewards[::-1]:\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "\n",
    "        for log_prob, ret in zip(self.saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * ret)\n",
    "\n",
    "        policy_unsqueezed = [torch.unsqueeze(loss, 0) for loss in policy_loss]\n",
    "        self.loss = torch.cat(policy_unsqueezed).sum()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        self.loss.backward()\n",
    "        if self.done is False:\n",
    "            self.optimizer.step()\n",
    "        \n",
    "    def save_agent_data(self,main_path):\n",
    "        '''\n",
    "        Stores the model parameters into a json file\n",
    "\n",
    "        '''\n",
    "\n",
    "        agent_variables = {\n",
    "            \"Number of Qubits\": self.pqc.circuit.n_qubits,\n",
    "            \"Number of Layers\": self.pqc.circuit.n_layers,\n",
    "            \"Shots\": self.pqc.circuit.shots,\n",
    "            \"Input Scaling\": self.pqc.circuit.input_scaling,\n",
    "            \"Design\": self.pqc.circuit.design,\n",
    "            \"Differentiation Method\": self.pqc.circuit.diff_method,\n",
    "            \"Weight Initialization\": \"lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\",\n",
    "            \"Input Initialization\": get_function_representation(self.pqc.circuit.input_init),\n",
    "            \"Measure\": get_function_representation(self.pqc.circuit.measure),\n",
    "            \"Policy Type\": self.pqc.policy.post_processing,\n",
    "            \"Softmax scheduling (in case policy is softmax)\": str(self.pqc.policy.beta_scheduling) + (\". Starting beta: \" + str(self.pqc.policy.beta) + \". Increase rate: \" + str(self.pqc.policy.increase_rate)),\n",
    "            \"Softmax output scalling (in case policy is softmax)\" : str(self.pqc.policy.output_scaling) + \". Output Initialization: \" + get_function_representation(self.pqc.policy.output_init),\n",
    "            \"Optimizers\": str(self.optimizer),\n",
    "            \"Envinronment Name\": str(self.env_name),\n",
    "            \"Gamma (discounting factor)\": self.gamma,\n",
    "        }\n",
    "\n",
    "        with open(os.path.join(main_path, \"agent_characteristics.json\"), \"w\") as f:\n",
    "            json.dump(agent_variables, f, indent=4)\n",
    "\n",
    "    def save_data(self,main_path):\n",
    "        '''\n",
    "        Saves the data into a .npz file of each episode\n",
    "\n",
    "        '''\n",
    "\n",
    "        run= os.path.join(main_path,'run' + str(self.file_name) + '_data.npz')\n",
    "\n",
    "        if not os.path.exists(main_path):\n",
    "            os.makedirs(main_path)\n",
    "\n",
    "        if os.path.exists(run):\n",
    "            data = np.load(run, allow_pickle=True)\n",
    "            old_episode_reward = data['episode_reward'].tolist()\n",
    "            old_loss = data['loss'].tolist()\n",
    "            old_runtime = data['runtime'].tolist()\n",
    "            old_params_gradients = data['params_gradients'].tolist()\n",
    "            old_input_params_gradients = data['input_params_gradients'].tolist()\n",
    "        else:\n",
    "            old_episode_reward = []\n",
    "            old_loss = []\n",
    "            old_runtime = []\n",
    "            old_params_gradients = []\n",
    "            old_input_params_gradients = []\n",
    "\n",
    "\n",
    "        old_episode_reward.append(self.scores_deque[-1])\n",
    "        old_loss.append(self.loss.item())\n",
    "        old_runtime.append(self.runtime)\n",
    "        old_params_gradients.append(tensor_to_list(self.pqc.get_gradients()[0]))\n",
    "        old_input_params_gradients.append(tensor_to_list(self.pqc.get_gradients()[1]))\n",
    "\n",
    "        np.savez_compressed(run, episode_reward = np.array(old_episode_reward),\n",
    "                                 loss = np.array(old_loss),\n",
    "                                 runtime = np.array(old_runtime),\n",
    "                                 params_gradients = np.array(old_params_gradients), \n",
    "                                 input_params_gradients = np.array(old_input_params_gradients))\n",
    "        \n",
    "        del old_episode_reward[:]\n",
    "        del old_loss[:]\n",
    "        del old_runtime[:]\n",
    "        del old_params_gradients[:]\n",
    "        del old_input_params_gradients[:]\n",
    "\n",
    "    def writer_function(self, writer, iteration):\n",
    "        '''\n",
    "        Stores data into a tensorboard session\n",
    "\n",
    "        '''\n",
    "        writer.add_scalar(\"Episode Reward\", self.scores_deque[-1], global_step=iteration)\n",
    "        writer.add_scalar(\"Runtime\", self.runtime, global_step=iteration)\n",
    "        writer.add_scalar(\"Loss\", self.loss.item(), global_step=iteration)\n",
    "        writer.add_scalar(\"Beta\", self.pqc.policy.beta, global_step=iteration)\n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        logs_dir = \"../../data\"\n",
    "        os.makedirs(logs_dir, exist_ok=True)\n",
    "        envinronment_folder = os.path.join(logs_dir, self.env_name)\n",
    "        os.makedirs(envinronment_folder, exist_ok=True)\n",
    "        experiment_folder = f\"{self.pqc.policy.post_processing}_{self.pqc.circuit.n_layers}layer_{self.rundate}\"\n",
    "        experiment_path = os.path.join(envinronment_folder, experiment_folder)\n",
    "        os.makedirs(experiment_path, exist_ok=True)\n",
    "        run = os.path.join(experiment_path,str(self.file_name))\n",
    "        os.makedirs(run, exist_ok=True)\n",
    "        writer = SummaryWriter(log_dir=run)\n",
    "        self.save_agent_data(experiment_path)\n",
    "        \n",
    "        for i in range(1, self.n_episodes):\n",
    "            start_time = time.time()\n",
    "            self.get_trajectory()\n",
    "            \n",
    "            self.update_policy()\n",
    "            end_time = time.time()\n",
    "            self.runtime = end_time - start_time\n",
    "            self.writer_function(writer,i)\n",
    "            self.save_data(run)\n",
    "            if self.done is False:\n",
    "                self.pqc.policy.beta_schedule()\n",
    "            \n",
    "            if np.mean(self.scores_deque) > self.env.spec.reward_threshold:\n",
    "                self.done = True\n",
    "\n",
    "            if i % self.print_every == 0 and self.verbose == 1:\n",
    "                print('Episode {}\\tLast reward: {:.2f}\\tLast {} Episodes average reward: {:.2f}\\t '.format(i, self.scores_deque[-1], self.print_every, np.mean(self.scores_deque)))\n",
    "            elif self.verbose == 1:\n",
    "                print('Episode {}\\tLast reward: {:.2f}\\tRuntime: {:.2f}\\t '.format(i, self.scores_deque[-1], self.runtime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single agent runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 4\n",
    "n_layers = 4      #set to 1 if data_reuploading is off\n",
    "entanglement = \"all_to_all\"\n",
    "shots = None\n",
    "input_scaling = True\n",
    "design = 'jerbi_circuit' \n",
    "diff_method = 'backprop' \n",
    "weight_init = torch.nn.init.normal_\n",
    "#weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "input_init = torch.nn.init.ones_\n",
    "measure = measure_expval_pairs\n",
    "circuit = CircuitGenerator(n_qubits, \n",
    "                           n_layers,\n",
    "                           entanglement,\n",
    "                           shots,\n",
    "                           input_scaling,\n",
    "                           design,\n",
    "                           diff_method,\n",
    "                           weight_init,\n",
    "                           input_init,\n",
    "                           measure)\n",
    "\n",
    "\n",
    "n_actions = 2\n",
    "post_processing = 'softmax'\n",
    "beta_scheduling = True\n",
    "beta = 0.5\n",
    "increase_rate = 0.003\n",
    "output_scaling = False\n",
    "output_init = torch.nn.init.ones_\n",
    "policy_type = PolicyType(n_actions, \n",
    "                         post_processing, \n",
    "                         beta_scheduling, \n",
    "                         beta, increase_rate, \n",
    "                         output_scaling, \n",
    "                         output_init)\n",
    "\n",
    "pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "\n",
    "lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "circuit_params = list(circuit.parameters())\n",
    "policy_params = list(policy_type.parameters())\n",
    "params = circuit_params + policy_params\n",
    "optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "env_name = 'CartPole-v1'\n",
    "n_episodes = 1000\n",
    "max_t = 500\n",
    "gamma = 0.98\n",
    "print_every = 1\n",
    "verbose = 1\n",
    "reinforce_update = ReinforceUpdate(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose)\n",
    "reinforce_update.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple agent runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18.0, 33.0, 11.0, 28.0, 20.0, 14.0, 30.0, 11.0, 10.0, 12.0, 32.0, 25.0, 12.0, 16.0, 11.0, 34.0, 17.0, 42.0, 16.0, 17.0, 27.0, 13.0, 25.0, 30.0, 11.0, 26.0, 10.0, 20.0, 47.0, 20.0, 11.0, 13.0, 12.0, 43.0, 21.0, 27.0, 31.0, 35.0, 37.0, 37.0, 39.0, 28.0, 18.0, 19.0, 49.0, 41.0, 24.0, 23.0, 23.0, 35.0, 19.0, 41.0, 24.0, 34.0, 21.0, 19.0, 47.0, 21.0, 41.0, 35.0, 51.0, 32.0, 35.0, 34.0, 9.0, 43.0, 21.0, 26.0, 19.0, 48.0, 13.0, 28.0, 19.0, 35.0, 32.0, 34.0, 55.0, 55.0, 31.0, 32.0, 87.0, 78.0, 58.0, 46.0, 163.0, 70.0, 43.0, 36.0, 18.0, 76.0, 103.0, 76.0, 41.0, 118.0, 41.0, 59.0, 71.0, 102.0, 82.0, 38.0, 48.0, 76.0, 73.0, 51.0, 85.0, 44.0, 83.0, 66.0, 69.0, 97.0, 30.0, 19.0, 27.0, 44.0, 51.0, 17.0, 31.0, 38.0, 53.0, 47.0, 41.0, 35.0, 29.0, 30.0, 10.0, 31.0, 16.0, 38.0, 16.0, 50.0, 101.0, 75.0, 36.0, 85.0, 57.0, 37.0, 42.0, 26.0, 12.0, 55.0, 49.0, 68.0, 48.0, 31.0, 10.0, 41.0, 24.0, 40.0, 58.0, 35.0, 55.0, 30.0, 36.0, 62.0, 20.0, 76.0, 81.0, 65.0, 17.0, 41.0, 50.0, 27.0, 64.0, 104.0, 18.0, 43.0, 90.0, 75.0, 64.0, 78.0, 75.0, 41.0, 100.0, 42.0, 109.0, 131.0, 186.0, 49.0, 120.0, 198.0, 34.0, 110.0, 112.0, 51.0, 42.0, 140.0, 105.0, 63.0, 91.0, 25.0, 15.0, 54.0, 165.0, 78.0, 49.0, 140.0, 166.0, 92.0, 41.0, 254.0, 23.0, 99.0, 32.0, 66.0, 30.0, 194.0, 33.0, 52.0, 108.0, 86.0, 30.0, 14.0, 46.0, 65.0, 119.0, 37.0, 37.0, 89.0, 27.0, 28.0, 74.0, 60.0, 24.0, 94.0, 116.0, 47.0, 32.0, 114.0, 183.0, 65.0, 25.0, 80.0, 96.0, 105.0, 51.0, 76.0, 76.0, 143.0, 185.0, 83.0, 70.0, 133.0, 61.0, 116.0, 212.0, 164.0, 26.0, 73.0, 22.0, 190.0, 25.0, 79.0, 178.0, 139.0, 197.0, 51.0, 55.0, 88.0, 361.0, 156.0, 31.0, 76.0, 147.0, 119.0, 468.0, 172.0, 137.0, 140.0, 44.0, 265.0, 324.0, 61.0, 266.0, 87.0, 24.0, 339.0, 53.0, 212.0, 92.0, 150.0, 66.0, 126.0, 13.0, 144.0, 130.0, 149.0, 394.0, 150.0, 86.0, 219.0, 218.0, 87.0, 46.0, 208.0, 78.0, 125.0, 415.0, 38.0, 43.0, 16.0, 91.0, 55.0, 215.0, 84.0, 286.0, 86.0, 127.0, 57.0, 140.0, 208.0, 69.0, 16.0, 113.0, 72.0, 106.0, 81.0, 76.0, 87.0, 89.0, 145.0, 143.0, 71.0, 128.0, 136.0, 88.0, 197.0, 219.0, 173.0, 130.0, 112.0, 131.0, 83.0, 210.0, 184.0, 169.0, 138.0, 186.0, 164.0, 78.0, 28.0, 95.0, 191.0, 324.0, 82.0, 23.0, 344.0, 360.0, 174.0, 238.0, 452.0, 193.0, 239.0, 97.0, 171.0, 89.0, 93.0, 228.0, 76.0, 12.0, 71.0, 57.0, 63.0, 38.0, 138.0, 57.0, 133.0, 147.0, 48.0, 82.0, 137.0, 145.0, 49.0, 160.0, 163.0, 106.0, 173.0, 46.0, 87.0, 133.0, 217.0, 79.0, 193.0, 252.0, 85.0, 372.0, 98.0, 253.0, 118.0, 238.0, 218.0, 361.0, 397.0, 77.0, 170.0, 124.0, 216.0, 335.0, 118.0, 68.0, 225.0, 263.0, 74.0, 417.0, 259.0, 264.0, 368.0, 403.0, 243.0, 251.0, 229.0, 434.0, 251.0, 117.0, 149.0, 43.0, 177.0, 53.0, 397.0, 491.0, 318.0, 331.0, 414.0, 182.0, 319.0, 138.0, 311.0, 163.0, 172.0, 144.0, 181.0, 276.0, 240.0, 202.0, 235.0, 58.0, 28.0, 21.0, 81.0, 113.0, 63.0, 56.0, 13.0, 13.0, 23.0, 78.0, 11.0, 49.0, 18.0, 63.0, 11.0, 99.0, 65.0, 26.0, 25.0, 243.0, 17.0, 148.0, 94.0, 75.0, 91.0, 14.0, 91.0, 138.0, 172.0, 64.0, 192.0, 99.0, 113.0, 199.0, 128.0, 138.0, 117.0, 458.0, 136.0, 216.0, 138.0, 289.0, 246.0, 255.0, 139.0, 244.0, 364.0, 80.0, 167.0, 59.0, 231.0, 456.0, 383.0, 303.0, 434.0, 413.0, 248.0, 363.0, 389.0, 488.0, 429.0, 357.0, 173.0, 350.0, 366.0, 163.0, 180.0, 422.0, 152.0, 192.0, 302.0, 220.0, 268.0, 146.0, 356.0, 336.0, 470.0, 81.0, 304.0, 383.0, 388.0, 318.0, 295.0, 383.0, 460.0, 359.0, 475.0, 308.0, 299.0, 319.0, 450.0, 242.0, 310.0, 303.0, 312.0, 493.0, 382.0, 438.0, 383.0, 246.0, 368.0, 371.0, 332.0, 178.0, 222.0, 379.0, 81.0, 297.0, 322.0, 350.0, 366.0, 443.0, 350.0, 421.0, 248.0, 228.0, 152.0, 79.0, 170.0, 370.0, 316.0, 84.0, 338.0, 217.0, 422.0, 371.0, 246.0, 420.0, 465.0, 292.0, 277.0, 200.0, 231.0, 316.0, 157.0, 92.0, 262.0, 128.0, 194.0, 161.0, 93.0, 149.0, 169.0, 173.0, 122.0, 151.0, 234.0, 136.0, 181.0, 174.0, 153.0, 228.0, 133.0, 169.0, 172.0, 120.0, 107.0, 183.0, 132.0, 147.0, 215.0, 113.0, 222.0, 114.0, 122.0, 280.0, 456.0, 286.0, 119.0, 48.0, 118.0, 291.0, 185.0, 129.0, 138.0, 281.0, 308.0, 293.0, 346.0, 199.0, 250.0, 258.0, 47.0, 223.0, 313.0, 248.0, 150.0, 248.0, 101.0, 25.0, 264.0, 179.0, 301.0, 406.0, 247.0, 249.0, 381.0, 416.0, 313.0, 464.0, 396.0, 443.0, 402.0, 391.0, 248.0, 489.0, 310.0, 268.0, 264.0, 415.0, 288.0, 268.0, 254.0, 405.0, 106.0, 309.0, 270.0, 207.0, 251.0, 208.0, 353.0, 386.0, 243.0, 192.0, 268.0, 443.0, 439.0, 258.0, 364.0, 362.0, 377.0, 276.0, 470.0, 340.0, 427.0, 482.0, 315.0, 405.0, 166.0, 268.0, 487.0, 309.0, 261.0, 281.0, 453.0, 309.0, 180.0, 357.0, 234.0, 249.0, 301.0, 328.0, 130.0, 240.0, 222.0, 309.0, 126.0, 296.0, 280.0, 190.0, 137.0, 245.0, 154.0, 165.0, 177.0, 184.0, 308.0, 281.0, 162.0, 142.0, 232.0, 171.0, 221.0, 181.0, 367.0, 228.0, 133.0, 346.0, 182.0, 152.0, 278.0, 285.0, 161.0, 272.0, 468.0, 48.0, 93.0, 375.0, 344.0, 371.0, 341.0, 485.0, 360.0, 298.0, 316.0, 210.0, 277.0, 395.0, 210.0, 132.0, 446.0, 435.0, 437.0, 166.0, 395.0, 338.0, 305.0, 363.0, 175.0, 369.0, 481.0, 284.0, 151.0, 295.0, 472.0, 175.0, 353.0, 281.0, 205.0, 490.0, 299.0, 318.0, 434.0, 337.0, 144.0, 436.0, 187.0, 267.0, 122.0, 281.0, 230.0, 282.0, 398.0, 203.0, 254.0, 277.0, 437.0, 349.0, 283.0, 302.0, 451.0, 340.0, 158.0, 459.0, 362.0, 359.0, 232.0, 417.0, 475.0, 198.0, 241.0, 332.0, 355.0, 153.0, 421.0, 377.0, 245.0, 197.0, 180.0, 294.0, 193.0, 237.0, 214.0, 162.0, 324.0, 193.0, 161.0, 163.0, 157.0, 261.0, 188.0, 177.0, 139.0, 170.0, 154.0, 153.0, 190.0, 165.0, 159.0, 139.0, 199.0, 175.0, 135.0, 137.0, 137.0, 174.0, 134.0, 122.0, 156.0, 143.0, 127.0, 96.0, 250.0, 114.0, 100.0, 189.0, 133.0, 145.0, 137.0, 133.0, 136.0, 121.0, 143.0, 173.0, 134.0, 126.0, 117.0, 357.0, 135.0, 137.0, 171.0, 122.0, 160.0, 128.0, 139.0, 180.0, 207.0, 128.0, 155.0, 173.0, 166.0, 131.0, 141.0, 163.0, 241.0, 189.0, 126.0, 194.0, 156.0, 118.0, 256.0], [16.0, 14.0, 12.0, 10.0, 18.0, 31.0, 18.0, 18.0, 16.0, 11.0, 17.0, 59.0, 11.0, 16.0, 27.0, 23.0, 22.0, 13.0, 21.0, 28.0, 14.0, 20.0, 31.0, 23.0, 13.0, 10.0, 23.0, 18.0, 21.0, 16.0, 10.0, 29.0, 18.0, 20.0, 19.0, 13.0, 17.0, 25.0, 10.0, 34.0, 21.0, 29.0, 18.0, 39.0, 10.0, 20.0, 15.0, 40.0, 51.0, 49.0, 14.0, 54.0, 35.0, 101.0, 20.0, 45.0, 18.0, 36.0, 54.0, 14.0, 53.0, 13.0, 30.0, 33.0, 45.0, 26.0, 29.0, 38.0, 33.0, 25.0, 13.0, 22.0, 67.0, 72.0, 46.0, 21.0, 50.0, 31.0, 17.0, 26.0, 30.0, 63.0, 110.0, 23.0, 60.0, 25.0, 16.0, 16.0, 14.0, 40.0, 21.0, 14.0, 22.0, 38.0, 15.0, 79.0, 29.0, 52.0, 75.0, 32.0, 43.0, 55.0, 73.0, 45.0, 93.0, 74.0, 14.0, 25.0, 32.0, 33.0, 89.0, 48.0, 61.0, 62.0, 52.0, 26.0, 49.0, 61.0, 42.0, 55.0, 15.0, 28.0, 70.0, 36.0, 38.0, 154.0, 104.0, 33.0, 20.0, 31.0, 19.0, 25.0, 18.0, 84.0, 78.0, 34.0, 10.0, 18.0, 22.0, 64.0, 47.0, 19.0, 11.0, 27.0, 91.0, 29.0, 25.0, 17.0, 32.0, 39.0, 80.0, 119.0, 103.0, 12.0, 79.0, 65.0, 65.0, 42.0, 73.0, 50.0, 64.0, 21.0, 55.0, 34.0, 19.0, 49.0, 12.0, 84.0, 47.0, 30.0, 43.0, 95.0, 73.0, 36.0, 58.0, 23.0, 105.0, 38.0, 79.0, 37.0, 40.0, 54.0, 74.0, 79.0, 28.0, 49.0, 49.0, 123.0, 116.0, 15.0, 67.0, 250.0, 107.0, 88.0, 68.0, 171.0, 48.0, 64.0, 54.0, 50.0, 55.0, 57.0, 53.0, 34.0, 93.0, 43.0, 17.0, 66.0, 117.0, 62.0, 91.0, 64.0, 93.0, 65.0, 42.0, 68.0, 22.0, 73.0, 129.0, 67.0, 16.0, 193.0, 72.0, 124.0, 15.0, 103.0, 80.0, 121.0, 69.0, 60.0, 81.0, 34.0, 77.0, 48.0, 69.0, 59.0, 61.0, 70.0, 56.0, 157.0, 97.0, 75.0, 157.0, 116.0, 148.0, 150.0, 56.0, 81.0, 39.0, 84.0, 181.0, 43.0, 124.0, 101.0, 116.0, 101.0, 144.0, 125.0, 70.0, 115.0, 115.0, 63.0, 106.0, 121.0, 134.0, 112.0, 184.0, 108.0, 90.0, 99.0, 164.0, 107.0, 132.0, 47.0, 74.0, 81.0, 135.0, 84.0, 106.0, 92.0, 72.0, 106.0, 103.0, 107.0, 96.0, 107.0, 105.0, 34.0, 30.0, 131.0, 109.0, 24.0, 149.0, 93.0, 28.0, 119.0, 119.0, 101.0, 150.0, 115.0, 154.0, 114.0, 161.0, 142.0, 149.0, 274.0, 49.0, 181.0, 270.0, 294.0, 61.0, 236.0, 421.0, 197.0, 345.0, 226.0, 297.0, 173.0, 153.0, 152.0, 149.0, 130.0, 155.0, 189.0, 113.0, 41.0, 215.0, 97.0, 41.0, 211.0, 179.0, 184.0, 180.0, 72.0, 326.0, 283.0, 158.0, 427.0, 271.0, 315.0, 284.0, 203.0, 155.0, 380.0, 324.0, 312.0, 192.0, 190.0, 20.0, 168.0, 205.0, 129.0, 115.0, 97.0, 154.0, 115.0, 106.0, 133.0, 66.0, 92.0, 86.0, 104.0, 25.0, 179.0, 77.0, 147.0, 258.0, 180.0, 79.0, 149.0, 201.0, 92.0, 139.0, 36.0, 176.0, 110.0, 110.0, 32.0, 94.0, 123.0, 136.0, 133.0, 89.0, 154.0, 92.0, 128.0, 120.0, 161.0, 90.0, 58.0, 141.0, 49.0, 75.0, 74.0, 85.0, 185.0, 111.0, 109.0, 125.0, 131.0, 100.0, 82.0, 175.0, 143.0, 58.0, 164.0, 61.0, 268.0, 199.0, 21.0, 100.0, 119.0, 122.0, 158.0, 93.0, 179.0, 53.0, 115.0, 166.0, 89.0, 404.0, 212.0, 139.0, 96.0, 81.0, 56.0, 22.0, 103.0, 103.0, 82.0, 80.0, 129.0, 50.0, 71.0, 79.0, 99.0, 115.0, 66.0, 107.0, 147.0, 86.0, 61.0, 86.0, 19.0, 64.0, 38.0, 85.0, 76.0, 93.0, 57.0, 68.0, 47.0, 82.0, 126.0, 106.0, 88.0, 85.0, 129.0, 140.0, 119.0, 157.0, 106.0, 121.0, 167.0, 22.0, 35.0, 212.0, 177.0, 99.0, 131.0, 289.0, 219.0, 176.0, 41.0, 188.0, 99.0, 160.0, 495.0, 273.0, 196.0, 190.0, 361.0, 165.0, 151.0, 223.0, 208.0, 173.0, 179.0, 205.0, 142.0, 192.0, 127.0, 110.0, 146.0, 150.0, 113.0, 171.0, 148.0, 115.0, 89.0, 51.0, 118.0, 111.0, 88.0, 111.0, 126.0, 79.0, 105.0, 65.0, 87.0, 115.0, 119.0, 119.0, 88.0, 114.0, 119.0, 113.0, 97.0, 123.0, 145.0, 121.0, 131.0, 90.0, 120.0, 129.0, 131.0, 146.0, 167.0, 170.0, 132.0, 109.0, 167.0, 137.0, 123.0, 138.0, 108.0, 185.0, 124.0, 94.0, 99.0, 125.0, 100.0, 80.0, 82.0, 98.0, 83.0, 105.0, 115.0, 83.0, 76.0, 96.0, 112.0, 116.0, 72.0, 82.0, 90.0, 102.0, 112.0, 86.0, 111.0, 66.0, 86.0, 132.0, 82.0, 129.0, 88.0, 77.0, 101.0, 91.0, 74.0, 123.0, 196.0, 105.0, 215.0, 97.0, 52.0, 136.0, 147.0, 283.0, 89.0, 98.0, 156.0, 331.0, 153.0, 163.0, 100.0, 176.0, 107.0, 150.0, 143.0, 106.0, 65.0, 194.0, 306.0, 47.0, 365.0, 221.0, 396.0, 176.0, 165.0, 484.0, 488.0, 126.0, 368.0, 192.0, 128.0, 183.0, 435.0, 187.0, 472.0, 204.0, 185.0, 407.0, 191.0, 93.0, 425.0, 399.0, 244.0, 107.0, 302.0, 194.0, 262.0, 55.0, 141.0, 189.0, 256.0, 11.0, 219.0, 108.0, 157.0, 214.0, 95.0, 52.0, 78.0, 154.0, 130.0, 202.0, 103.0, 13.0, 187.0, 12.0, 206.0, 157.0, 314.0, 281.0, 193.0, 128.0, 138.0, 256.0, 184.0, 322.0, 313.0, 227.0, 180.0, 11.0, 106.0, 146.0, 155.0, 141.0, 280.0, 129.0, 175.0, 132.0, 180.0, 348.0, 127.0, 42.0, 371.0, 188.0, 388.0, 231.0, 423.0, 212.0, 18.0, 251.0, 449.0, 150.0, 487.0, 169.0, 465.0, 363.0, 327.0, 184.0, 129.0, 187.0, 224.0, 232.0, 20.0, 186.0, 200.0, 233.0, 224.0, 241.0, 135.0, 156.0, 33.0, 309.0, 300.0, 149.0, 219.0, 188.0, 195.0, 250.0, 274.0, 304.0, 247.0, 198.0, 383.0, 310.0, 455.0, 443.0, 367.0, 182.0, 65.0, 97.0, 274.0, 243.0, 266.0, 175.0, 280.0, 201.0, 178.0, 181.0, 156.0, 182.0, 203.0, 99.0, 117.0, 139.0, 181.0, 197.0, 154.0, 115.0, 125.0, 162.0, 161.0, 165.0, 133.0, 111.0, 182.0, 128.0, 250.0, 162.0, 23.0, 217.0, 238.0, 319.0, 144.0, 175.0, 155.0, 104.0, 71.0, 91.0, 154.0, 202.0, 139.0, 209.0, 182.0, 146.0, 183.0, 122.0, 80.0, 134.0, 16.0, 418.0, 173.0, 200.0, 122.0, 442.0, 259.0, 198.0, 275.0, 423.0, 363.0, 300.0, 396.0, 317.0, 411.0, 282.0, 409.0, 398.0, 451.0, 488.0, 470.0, 498.0, 473.0, 190.0, 314.0, 324.0, 327.0, 254.0, 365.0, 340.0, 298.0, 341.0, 190.0, 224.0, 134.0, 150.0, 167.0, 57.0, 147.0, 122.0, 121.0, 153.0, 130.0, 144.0, 124.0, 161.0, 125.0, 153.0, 117.0, 115.0, 130.0, 100.0, 101.0, 122.0, 135.0, 197.0, 85.0, 112.0, 138.0, 199.0, 152.0, 102.0, 98.0, 94.0, 118.0, 109.0, 206.0, 95.0, 118.0, 90.0, 240.0, 154.0, 173.0, 195.0, 204.0, 177.0, 116.0, 151.0, 143.0, 110.0, 111.0, 111.0, 147.0, 103.0, 128.0, 59.0, 128.0, 82.0, 74.0, 21.0, 100.0, 78.0, 108.0, 83.0, 85.0, 101.0, 101.0, 97.0, 80.0, 98.0, 85.0, 115.0, 45.0, 86.0, 108.0, 118.0, 122.0, 130.0, 158.0, 117.0, 100.0, 93.0, 158.0, 88.0, 55.0, 167.0, 89.0, 172.0, 81.0, 67.0, 85.0, 383.0, 120.0, 83.0, 405.0, 105.0, 350.0, 177.0, 304.0, 82.0, 205.0, 105.0, 71.0, 383.0, 399.0, 214.0, 273.0, 273.0, 101.0, 433.0, 178.0, 226.0, 420.0, 60.0, 160.0, 48.0, 409.0, 294.0, 384.0, 65.0, 288.0, 145.0, 39.0, 25.0], [22.0, 13.0, 12.0, 11.0, 14.0, 24.0, 11.0, 19.0, 17.0, 14.0, 14.0, 28.0, 21.0, 50.0, 9.0, 10.0, 14.0, 31.0, 21.0, 15.0, 11.0, 55.0, 14.0, 37.0, 12.0, 19.0, 13.0, 29.0, 13.0, 24.0, 17.0, 57.0, 24.0, 10.0, 12.0, 24.0, 28.0, 14.0, 28.0, 26.0, 12.0, 37.0, 50.0, 25.0, 34.0, 61.0, 61.0, 18.0, 72.0, 23.0, 53.0, 31.0, 17.0, 16.0, 49.0, 15.0, 32.0, 29.0, 15.0, 20.0, 46.0, 21.0, 38.0, 17.0, 36.0, 67.0, 23.0, 19.0, 29.0, 14.0, 64.0, 26.0, 37.0, 11.0, 56.0, 36.0, 46.0, 43.0, 42.0, 20.0, 55.0, 99.0, 20.0, 26.0, 41.0, 40.0, 117.0, 40.0, 22.0, 89.0, 54.0, 42.0, 32.0, 51.0, 107.0, 70.0, 100.0, 50.0, 31.0, 16.0, 128.0, 149.0, 25.0, 46.0, 12.0, 66.0, 94.0, 49.0, 12.0, 24.0, 40.0, 41.0, 52.0, 17.0, 37.0, 73.0, 53.0, 68.0, 133.0, 56.0, 63.0, 13.0, 58.0, 33.0, 66.0, 37.0, 15.0, 34.0, 22.0, 77.0, 20.0, 77.0, 88.0, 101.0, 9.0, 93.0, 28.0, 48.0, 68.0, 56.0, 34.0, 87.0, 40.0, 25.0, 186.0, 52.0, 24.0, 91.0, 26.0, 57.0, 37.0, 40.0, 22.0, 37.0, 69.0, 179.0, 27.0, 14.0, 21.0, 70.0, 110.0, 55.0, 95.0, 65.0, 16.0, 17.0, 85.0, 48.0, 134.0, 51.0, 32.0, 13.0, 26.0, 21.0, 26.0, 20.0, 59.0, 59.0, 24.0, 131.0, 50.0, 104.0, 47.0, 77.0, 67.0, 204.0, 122.0, 49.0, 79.0, 84.0, 109.0, 59.0, 59.0, 49.0, 57.0, 83.0, 121.0, 49.0, 56.0, 62.0, 140.0, 49.0, 46.0, 92.0, 17.0, 36.0, 20.0, 45.0, 17.0, 82.0, 73.0, 12.0, 61.0, 117.0, 103.0, 21.0, 13.0, 91.0, 22.0, 34.0, 46.0, 15.0, 55.0, 31.0, 53.0, 27.0, 32.0, 17.0, 51.0, 30.0, 86.0, 72.0, 32.0, 35.0, 51.0, 31.0, 151.0, 62.0, 57.0, 27.0, 37.0, 36.0, 82.0, 48.0, 63.0, 71.0, 98.0, 23.0, 87.0, 25.0, 102.0, 63.0, 40.0, 55.0, 138.0, 113.0, 16.0, 20.0, 26.0, 46.0, 220.0, 42.0, 131.0, 113.0, 35.0, 128.0, 134.0, 139.0, 21.0, 55.0, 88.0, 244.0, 92.0, 107.0, 53.0, 88.0, 116.0, 151.0, 40.0, 59.0, 36.0, 123.0, 120.0, 76.0, 158.0, 112.0, 155.0, 214.0, 92.0, 95.0, 242.0, 214.0, 130.0, 133.0, 194.0, 181.0, 120.0, 48.0, 90.0, 137.0, 172.0, 252.0, 45.0, 127.0, 172.0, 68.0, 44.0, 45.0, 158.0, 78.0, 84.0, 25.0, 280.0, 143.0, 95.0, 89.0, 113.0, 105.0, 175.0, 91.0, 67.0, 32.0, 155.0, 182.0, 153.0, 156.0, 205.0, 98.0, 117.0, 122.0, 72.0, 84.0, 51.0, 137.0, 30.0, 111.0, 41.0, 53.0, 84.0, 52.0, 95.0, 104.0, 476.0, 170.0, 178.0, 120.0, 42.0, 102.0, 47.0, 152.0, 74.0, 142.0, 69.0, 171.0, 117.0, 200.0, 139.0, 218.0, 273.0, 89.0, 76.0, 134.0, 80.0, 191.0, 144.0, 134.0, 64.0, 141.0, 60.0, 21.0, 137.0, 164.0, 40.0, 89.0, 23.0, 104.0, 115.0, 36.0, 90.0, 134.0, 81.0, 89.0, 96.0, 129.0, 330.0, 112.0, 293.0, 144.0, 63.0, 166.0, 82.0, 112.0, 88.0, 126.0, 99.0, 111.0, 125.0, 259.0, 168.0, 126.0, 186.0, 94.0, 138.0, 145.0, 121.0, 125.0, 124.0, 150.0, 130.0, 174.0, 96.0, 59.0, 111.0, 108.0, 110.0, 94.0, 63.0, 106.0, 190.0, 181.0, 131.0, 178.0, 149.0, 220.0, 138.0, 215.0, 31.0, 147.0, 197.0, 254.0, 247.0, 242.0, 360.0, 266.0, 399.0, 324.0, 409.0, 287.0, 488.0, 135.0, 419.0, 392.0, 337.0, 472.0, 140.0, 334.0, 431.0, 436.0, 454.0, 424.0, 380.0, 284.0, 446.0, 490.0, 363.0, 278.0, 109.0, 279.0, 225.0, 315.0, 300.0, 281.0, 350.0, 97.0, 141.0, 421.0, 295.0, 302.0, 331.0, 277.0, 157.0, 297.0, 76.0, 375.0, 370.0, 498.0, 457.0, 397.0, 238.0, 229.0, 303.0, 301.0, 281.0, 239.0, 261.0, 345.0, 224.0, 290.0, 147.0, 207.0, 236.0, 432.0, 382.0, 163.0, 229.0, 207.0, 260.0, 224.0, 68.0, 191.0, 293.0, 199.0, 246.0, 181.0, 217.0, 210.0, 295.0, 35.0, 272.0, 344.0, 315.0, 247.0, 159.0, 259.0, 438.0, 458.0, 468.0, 421.0, 277.0, 350.0, 425.0, 404.0, 422.0, 323.0, 122.0, 236.0, 168.0, 357.0, 212.0, 300.0, 176.0, 396.0, 221.0, 225.0, 249.0, 200.0, 352.0, 274.0, 493.0, 348.0, 384.0, 280.0, 264.0, 280.0, 328.0, 464.0, 269.0, 232.0, 236.0, 320.0, 254.0, 272.0, 280.0, 235.0, 228.0, 160.0, 188.0, 186.0, 179.0, 224.0, 183.0, 171.0, 174.0, 184.0, 210.0, 195.0, 252.0, 163.0, 190.0, 153.0, 134.0, 196.0, 172.0, 282.0, 173.0, 116.0, 169.0, 134.0, 173.0, 202.0, 108.0, 114.0, 153.0, 194.0, 148.0, 192.0, 115.0, 152.0, 139.0, 130.0, 131.0, 176.0, 124.0, 124.0, 129.0, 99.0, 129.0, 277.0, 108.0, 230.0, 114.0, 178.0, 102.0, 77.0, 95.0, 204.0, 112.0, 122.0, 86.0, 244.0, 159.0, 283.0, 167.0, 113.0, 197.0, 324.0, 164.0, 113.0, 240.0, 35.0, 155.0, 257.0, 166.0, 74.0, 179.0, 136.0, 86.0, 121.0, 11.0, 242.0, 195.0, 165.0, 191.0, 156.0, 150.0, 153.0, 247.0, 104.0, 125.0, 184.0, 170.0, 89.0, 166.0, 135.0, 168.0, 127.0, 194.0, 147.0, 89.0, 103.0, 75.0, 62.0, 156.0, 100.0, 109.0, 98.0, 61.0, 90.0, 62.0, 100.0, 76.0, 104.0, 48.0, 53.0, 113.0, 141.0, 76.0, 96.0, 110.0, 138.0, 74.0, 105.0, 59.0, 134.0, 86.0, 111.0, 59.0, 88.0, 52.0, 89.0, 108.0, 101.0, 183.0, 90.0, 32.0, 74.0, 21.0, 25.0, 70.0, 130.0, 92.0, 84.0, 79.0, 86.0, 64.0, 112.0, 66.0, 104.0, 108.0, 94.0, 80.0, 33.0, 36.0, 105.0, 76.0, 81.0, 108.0, 82.0, 62.0, 99.0, 28.0, 122.0, 86.0, 83.0, 116.0, 98.0, 124.0, 82.0, 122.0, 91.0, 99.0, 124.0, 102.0, 78.0, 77.0, 164.0, 54.0, 129.0, 100.0, 93.0, 94.0, 109.0, 104.0, 82.0, 72.0, 135.0, 79.0, 113.0, 113.0, 79.0, 140.0, 82.0, 83.0, 49.0, 103.0, 114.0, 106.0, 92.0, 106.0, 107.0, 97.0, 81.0, 68.0, 84.0, 95.0, 87.0, 86.0, 116.0, 136.0, 108.0, 94.0, 106.0, 126.0, 108.0, 75.0, 102.0, 102.0, 100.0, 103.0, 85.0, 136.0, 91.0, 127.0, 127.0, 97.0, 103.0, 119.0, 100.0, 77.0, 144.0, 117.0, 124.0, 129.0, 94.0, 104.0, 118.0, 43.0, 113.0, 150.0, 118.0, 95.0, 82.0, 92.0, 101.0, 128.0, 110.0, 105.0, 100.0, 109.0, 158.0, 114.0, 89.0, 132.0, 123.0, 126.0, 140.0, 172.0, 167.0, 116.0, 123.0, 141.0, 120.0, 161.0, 156.0, 184.0, 128.0, 149.0, 142.0, 183.0, 143.0, 123.0, 156.0, 176.0, 175.0, 146.0, 158.0, 18.0, 141.0, 127.0, 162.0, 208.0, 133.0, 187.0, 124.0, 166.0, 159.0, 133.0, 132.0, 97.0, 169.0, 145.0, 164.0, 156.0, 181.0, 195.0, 154.0, 166.0, 175.0, 173.0, 175.0, 135.0, 158.0, 177.0, 150.0, 173.0, 124.0, 159.0, 137.0, 162.0, 224.0, 187.0, 43.0, 168.0, 142.0, 142.0, 122.0, 141.0, 141.0, 208.0, 153.0, 170.0, 150.0, 173.0, 145.0, 170.0, 145.0, 180.0, 197.0, 178.0, 113.0, 178.0, 122.0, 139.0, 184.0, 155.0, 305.0, 151.0, 93.0, 165.0, 167.0, 174.0, 151.0, 289.0, 169.0, 139.0, 183.0, 117.0, 138.0, 191.0, 201.0, 12.0, 307.0, 175.0, 124.0, 31.0, 139.0, 187.0, 209.0, 62.0, 203.0, 135.0, 181.0, 225.0, 221.0, 183.0, 185.0, 234.0, 42.0, 156.0, 189.0, 217.0, 161.0, 102.0], [25.0, 12.0, 13.0, 12.0, 12.0, 27.0, 12.0, 16.0, 28.0, 12.0, 10.0, 30.0, 18.0, 13.0, 19.0, 21.0, 16.0, 13.0, 13.0, 9.0, 10.0, 20.0, 10.0, 20.0, 17.0, 14.0, 17.0, 14.0, 21.0, 21.0, 29.0, 30.0, 37.0, 57.0, 31.0, 26.0, 77.0, 51.0, 11.0, 46.0, 15.0, 39.0, 33.0, 48.0, 21.0, 13.0, 13.0, 14.0, 15.0, 22.0, 32.0, 40.0, 29.0, 23.0, 12.0, 36.0, 48.0, 33.0, 46.0, 67.0, 87.0, 40.0, 30.0, 20.0, 25.0, 17.0, 21.0, 150.0, 84.0, 49.0, 53.0, 53.0, 93.0, 21.0, 15.0, 30.0, 15.0, 150.0, 77.0, 46.0, 11.0, 55.0, 47.0, 40.0, 43.0, 158.0, 62.0, 48.0, 37.0, 24.0, 52.0, 36.0, 34.0, 98.0, 13.0, 30.0, 49.0, 15.0, 60.0, 14.0, 79.0, 70.0, 38.0, 50.0, 49.0, 24.0, 100.0, 43.0, 19.0, 50.0, 45.0, 66.0, 101.0, 28.0, 52.0, 102.0, 125.0, 11.0, 35.0, 60.0, 16.0, 38.0, 28.0, 21.0, 10.0, 47.0, 33.0, 21.0, 58.0, 55.0, 71.0, 49.0, 35.0, 99.0, 37.0, 22.0, 148.0, 53.0, 50.0, 118.0, 157.0, 54.0, 135.0, 75.0, 70.0, 75.0, 53.0, 20.0, 52.0, 71.0, 46.0, 116.0, 52.0, 91.0, 28.0, 39.0, 46.0, 48.0, 72.0, 46.0, 45.0, 47.0, 58.0, 73.0, 83.0, 75.0, 69.0, 93.0, 76.0, 113.0, 112.0, 66.0, 45.0, 44.0, 52.0, 98.0, 94.0, 84.0, 62.0, 69.0, 110.0, 125.0, 64.0, 39.0, 67.0, 58.0, 31.0, 93.0, 99.0, 31.0, 18.0, 28.0, 99.0, 104.0, 50.0, 63.0, 70.0, 100.0, 68.0, 96.0, 61.0, 38.0, 69.0, 131.0, 41.0, 80.0, 54.0, 85.0, 65.0, 11.0, 91.0, 47.0, 76.0, 102.0, 67.0, 34.0, 69.0, 137.0, 31.0, 39.0, 42.0, 117.0, 79.0, 122.0, 21.0, 115.0, 100.0, 118.0, 179.0, 39.0, 88.0, 98.0, 56.0, 99.0, 59.0, 189.0, 57.0, 86.0, 99.0, 97.0, 86.0, 174.0, 83.0, 300.0, 164.0, 117.0, 101.0, 89.0, 90.0, 175.0, 38.0, 261.0, 26.0, 77.0, 79.0, 103.0, 54.0, 28.0, 119.0, 8.0, 37.0, 177.0, 176.0, 174.0, 61.0, 114.0, 212.0, 50.0, 274.0, 197.0, 52.0, 48.0, 131.0, 56.0, 198.0, 43.0, 48.0, 122.0, 75.0, 65.0, 47.0, 20.0, 28.0, 154.0, 163.0, 92.0, 93.0, 222.0, 52.0, 106.0, 118.0, 82.0, 126.0, 120.0, 111.0, 87.0, 97.0, 76.0, 79.0, 106.0, 128.0, 132.0, 163.0, 92.0, 141.0, 123.0, 101.0, 132.0, 171.0, 112.0, 102.0, 161.0, 170.0, 174.0, 130.0, 148.0, 176.0, 143.0, 298.0, 287.0, 184.0, 120.0, 224.0, 73.0, 172.0, 221.0, 25.0, 121.0, 187.0, 117.0, 53.0, 54.0, 99.0, 10.0, 75.0, 196.0, 133.0, 73.0, 135.0, 195.0, 23.0, 170.0, 87.0, 85.0, 130.0, 104.0, 96.0, 115.0, 114.0, 109.0, 85.0, 19.0, 115.0, 95.0, 163.0, 19.0, 118.0, 43.0, 74.0, 30.0, 32.0, 38.0, 48.0, 98.0, 111.0, 212.0, 21.0, 174.0, 18.0, 309.0, 173.0, 196.0, 88.0, 168.0, 82.0, 95.0, 221.0, 74.0, 51.0, 97.0, 30.0, 121.0, 85.0, 152.0, 119.0, 35.0, 75.0, 174.0, 114.0, 64.0, 179.0, 60.0, 124.0, 97.0, 79.0, 82.0, 162.0, 120.0, 362.0, 177.0, 250.0, 89.0, 102.0, 132.0, 241.0, 155.0, 139.0, 86.0, 224.0, 315.0, 106.0, 115.0, 249.0, 218.0, 194.0, 498.0, 201.0, 148.0, 183.0, 126.0, 306.0, 424.0, 296.0, 202.0, 128.0, 379.0, 372.0, 410.0, 245.0, 102.0, 191.0, 360.0, 110.0, 438.0, 307.0, 275.0, 186.0, 183.0, 294.0, 74.0, 309.0, 217.0, 431.0, 203.0, 122.0, 222.0, 15.0, 154.0, 58.0, 337.0, 117.0, 123.0, 43.0, 209.0, 192.0, 106.0, 139.0, 101.0, 154.0, 140.0, 103.0, 498.0, 151.0, 162.0, 61.0, 473.0, 129.0, 151.0, 166.0, 193.0, 215.0, 117.0, 197.0, 266.0, 211.0, 300.0, 106.0, 96.0, 42.0, 117.0, 349.0, 233.0, 358.0, 388.0, 301.0, 352.0, 241.0, 114.0, 259.0, 51.0, 368.0, 365.0, 237.0, 221.0, 385.0, 306.0, 301.0, 245.0, 126.0, 96.0, 98.0, 322.0, 269.0, 219.0, 63.0, 90.0, 130.0, 149.0, 219.0, 195.0, 148.0, 139.0, 114.0, 144.0, 82.0, 160.0, 132.0, 124.0, 93.0, 96.0, 111.0, 118.0, 140.0, 68.0, 100.0, 130.0, 88.0, 130.0, 58.0, 135.0, 137.0, 85.0, 129.0, 130.0, 99.0, 110.0, 87.0, 68.0, 108.0, 139.0, 93.0, 94.0, 80.0, 86.0, 139.0, 131.0, 153.0, 123.0, 60.0, 117.0, 85.0, 118.0, 108.0, 154.0, 97.0, 184.0, 133.0, 78.0, 110.0, 151.0, 202.0, 114.0, 100.0, 79.0, 83.0, 101.0, 56.0, 68.0, 69.0, 62.0, 93.0, 15.0, 107.0, 71.0, 97.0, 57.0, 60.0, 62.0, 49.0, 85.0, 61.0, 100.0, 87.0, 90.0, 102.0, 70.0, 127.0, 77.0, 161.0, 94.0, 73.0, 66.0, 87.0, 68.0, 46.0, 87.0, 39.0, 93.0, 76.0, 87.0, 66.0, 126.0, 111.0, 115.0, 128.0, 105.0, 57.0, 93.0, 62.0, 125.0, 87.0, 108.0, 67.0, 120.0, 79.0, 173.0, 83.0, 112.0, 98.0, 118.0, 140.0, 93.0, 119.0, 101.0, 104.0, 126.0, 77.0, 94.0, 69.0, 97.0, 85.0, 87.0, 95.0, 57.0, 63.0, 36.0, 73.0, 305.0, 65.0, 100.0, 99.0, 193.0, 26.0, 55.0, 54.0, 92.0, 52.0, 58.0, 69.0, 108.0, 106.0, 76.0, 137.0, 31.0, 101.0, 46.0, 96.0, 262.0, 38.0, 78.0, 141.0, 161.0, 119.0, 132.0, 176.0, 60.0, 208.0, 169.0, 23.0, 112.0, 86.0, 127.0, 34.0, 54.0, 266.0, 150.0, 161.0, 156.0, 125.0, 155.0, 89.0, 265.0, 124.0, 87.0, 89.0, 118.0, 124.0, 35.0, 13.0, 46.0, 82.0, 111.0, 84.0, 86.0, 469.0, 74.0, 13.0, 104.0, 42.0, 84.0, 57.0, 90.0, 122.0, 122.0, 96.0, 89.0, 55.0, 133.0, 53.0, 56.0, 169.0, 213.0, 79.0, 60.0, 67.0, 137.0, 105.0, 98.0, 134.0, 79.0, 25.0, 108.0, 57.0, 114.0, 39.0, 66.0, 148.0, 161.0, 72.0, 31.0, 15.0, 47.0, 73.0, 51.0, 18.0, 176.0, 111.0, 87.0, 83.0, 146.0, 142.0, 64.0, 94.0, 120.0, 208.0, 123.0, 333.0, 146.0, 51.0, 122.0, 77.0, 87.0, 84.0, 92.0, 207.0, 158.0, 131.0, 78.0, 84.0, 92.0, 166.0, 234.0, 232.0, 111.0, 212.0, 156.0, 193.0, 61.0, 153.0, 76.0, 238.0, 91.0, 196.0, 141.0, 94.0, 82.0, 81.0, 102.0, 357.0, 128.0, 60.0, 45.0, 101.0, 89.0, 166.0, 125.0, 114.0, 228.0, 140.0, 114.0, 10.0, 141.0, 127.0, 71.0, 181.0, 175.0, 34.0, 58.0, 125.0, 100.0, 89.0, 104.0, 81.0, 139.0, 146.0, 135.0, 187.0, 25.0, 89.0, 272.0, 166.0, 127.0, 89.0, 9.0, 175.0, 40.0, 146.0, 102.0, 141.0, 281.0, 252.0, 75.0, 230.0, 236.0, 191.0, 197.0, 119.0, 133.0, 315.0, 173.0, 315.0, 215.0, 144.0, 18.0, 367.0, 141.0, 150.0, 125.0, 159.0, 344.0, 379.0, 335.0, 340.0, 320.0, 313.0, 192.0, 128.0, 256.0, 293.0, 166.0, 122.0, 285.0, 186.0, 165.0, 188.0, 372.0, 282.0, 149.0, 369.0, 89.0, 265.0, 237.0, 258.0, 294.0, 85.0, 286.0, 400.0, 278.0, 99.0, 119.0, 143.0, 98.0, 164.0, 119.0, 87.0, 113.0, 24.0, 57.0, 59.0, 13.0, 31.0, 74.0, 51.0, 85.0, 104.0, 94.0, 70.0, 56.0, 51.0, 23.0, 78.0, 49.0, 67.0, 169.0, 134.0, 86.0, 159.0, 57.0, 54.0, 130.0, 120.0, 60.0, 152.0, 79.0, 130.0, 466.0, 128.0, 380.0, 37.0, 81.0, 342.0, 96.0, 328.0, 291.0, 276.0, 148.0, 260.0, 213.0, 216.0, 285.0, 232.0, 50.0, 145.0, 200.0, 401.0, 149.0, 55.0, 441.0, 414.0, 266.0, 140.0, 226.0, 201.0, 95.0, 262.0, 171.0, 97.0, 212.0, 119.0, 138.0, 142.0, 86.0, 200.0, 150.0, 219.0, 156.0, 174.0, 190.0, 186.0, 160.0, 146.0, 62.0, 174.0, 111.0, 127.0, 118.0, 125.0, 153.0, 104.0, 120.0, 189.0, 162.0, 127.0, 106.0, 122.0, 77.0, 101.0, 126.0, 61.0, 96.0, 123.0, 257.0, 91.0, 85.0, 131.0, 110.0, 265.0, 129.0, 131.0, 133.0, 65.0, 147.0, 206.0, 246.0, 142.0, 182.0], [15.0, 21.0, 39.0, 10.0, 15.0, 15.0, 14.0, 14.0, 33.0, 13.0, 18.0, 25.0, 17.0, 35.0, 19.0, 18.0, 14.0, 22.0, 23.0, 26.0, 70.0, 19.0, 23.0, 22.0, 31.0, 39.0, 50.0, 28.0, 25.0, 30.0, 15.0, 20.0, 67.0, 32.0, 17.0, 21.0, 15.0, 12.0, 31.0, 48.0, 27.0, 18.0, 17.0, 23.0, 30.0, 13.0, 39.0, 25.0, 42.0, 17.0, 39.0, 31.0, 21.0, 50.0, 28.0, 16.0, 53.0, 82.0, 35.0, 63.0, 44.0, 51.0, 19.0, 33.0, 18.0, 19.0, 15.0, 54.0, 14.0, 38.0, 45.0, 34.0, 33.0, 71.0, 78.0, 34.0, 38.0, 13.0, 47.0, 30.0, 30.0, 88.0, 43.0, 20.0, 24.0, 55.0, 64.0, 85.0, 69.0, 45.0, 94.0, 55.0, 71.0, 29.0, 171.0, 29.0, 55.0, 66.0, 70.0, 63.0, 55.0, 26.0, 36.0, 112.0, 63.0, 65.0, 73.0, 28.0, 22.0, 72.0, 107.0, 67.0, 29.0, 106.0, 35.0, 47.0, 97.0, 105.0, 134.0, 156.0, 58.0, 36.0, 100.0, 36.0, 76.0, 54.0, 153.0, 110.0, 74.0, 76.0, 252.0, 38.0, 84.0, 74.0, 75.0, 48.0, 239.0, 177.0, 239.0, 95.0, 40.0, 104.0, 157.0, 53.0, 42.0, 75.0, 41.0, 96.0, 58.0, 71.0, 95.0, 34.0, 175.0, 29.0, 77.0, 75.0, 63.0, 103.0, 21.0, 68.0, 34.0, 69.0, 33.0, 21.0, 76.0, 112.0, 68.0, 44.0, 89.0, 225.0, 118.0, 46.0, 76.0, 35.0, 110.0, 68.0, 27.0, 89.0, 68.0, 83.0, 41.0, 17.0, 42.0, 18.0, 49.0, 51.0, 87.0, 33.0, 17.0, 10.0, 28.0, 57.0, 100.0, 28.0, 60.0, 24.0, 94.0, 183.0, 86.0, 65.0, 56.0, 46.0, 200.0, 15.0, 92.0, 32.0, 40.0, 163.0, 84.0, 30.0, 154.0, 119.0, 117.0, 144.0, 95.0, 79.0, 62.0, 126.0, 59.0, 99.0, 113.0, 133.0, 112.0, 187.0, 152.0, 14.0, 25.0, 337.0, 25.0, 106.0, 56.0, 202.0, 121.0, 122.0, 212.0, 164.0, 86.0, 130.0, 280.0, 295.0, 64.0, 140.0, 71.0, 150.0, 127.0, 143.0, 132.0, 149.0, 41.0, 139.0, 16.0, 104.0, 127.0, 83.0, 99.0, 68.0, 98.0, 66.0, 94.0, 117.0, 91.0, 59.0, 45.0, 95.0, 107.0, 123.0, 80.0, 115.0, 82.0, 116.0, 108.0, 139.0, 262.0, 174.0, 120.0, 97.0, 120.0, 57.0, 63.0, 146.0, 88.0, 75.0, 79.0, 91.0, 88.0, 147.0, 95.0, 67.0, 36.0, 128.0, 93.0, 47.0, 108.0, 137.0, 54.0, 166.0, 103.0, 93.0, 91.0, 98.0, 115.0, 100.0, 61.0, 126.0, 51.0, 81.0, 164.0, 133.0, 88.0, 92.0, 34.0, 268.0, 161.0, 104.0, 193.0, 49.0, 202.0, 220.0, 96.0, 158.0, 142.0, 139.0, 289.0, 201.0, 171.0, 172.0, 38.0, 207.0, 328.0, 48.0, 161.0, 256.0, 256.0, 182.0, 466.0, 80.0, 173.0, 123.0, 166.0, 149.0, 176.0, 161.0, 138.0, 213.0, 162.0, 126.0, 138.0, 158.0, 149.0, 69.0, 150.0, 162.0, 75.0, 84.0, 55.0, 211.0, 254.0, 169.0, 125.0, 23.0, 233.0, 411.0, 221.0, 269.0, 244.0, 163.0, 270.0, 484.0, 101.0, 391.0, 300.0, 396.0, 229.0, 123.0, 127.0, 204.0, 206.0, 207.0, 149.0, 406.0, 149.0, 173.0, 315.0, 203.0, 226.0, 217.0, 180.0, 170.0, 166.0, 184.0, 159.0, 104.0, 207.0, 163.0, 201.0, 134.0, 127.0, 120.0, 135.0, 144.0, 141.0, 145.0, 193.0, 99.0, 98.0, 104.0, 108.0, 131.0, 117.0, 129.0, 133.0, 81.0, 90.0, 63.0, 148.0, 96.0, 123.0, 49.0, 153.0, 141.0, 374.0, 115.0, 122.0, 378.0, 138.0, 88.0, 131.0, 191.0, 106.0, 108.0, 138.0, 108.0, 153.0, 156.0, 154.0, 174.0, 131.0, 131.0, 87.0, 63.0, 87.0, 213.0, 99.0, 125.0, 112.0, 65.0, 130.0, 118.0, 99.0, 73.0, 114.0, 14.0, 126.0, 153.0, 120.0, 105.0, 155.0, 100.0, 80.0, 125.0, 107.0, 98.0, 106.0, 83.0, 103.0, 128.0, 75.0, 116.0, 84.0, 79.0, 117.0, 49.0, 264.0, 85.0, 78.0, 91.0, 144.0, 137.0, 121.0, 79.0, 89.0, 68.0, 117.0, 99.0, 116.0, 65.0, 93.0, 125.0, 96.0, 122.0, 126.0, 103.0, 60.0, 148.0, 136.0, 106.0, 114.0, 162.0, 148.0, 125.0, 143.0, 191.0, 149.0, 109.0, 91.0, 166.0, 103.0, 120.0, 163.0, 149.0, 147.0, 181.0, 129.0, 156.0, 145.0, 151.0, 102.0, 110.0, 145.0, 118.0, 159.0, 154.0, 189.0, 99.0, 259.0, 165.0, 162.0, 68.0, 154.0, 287.0, 130.0, 82.0, 110.0, 33.0, 43.0, 92.0, 156.0, 159.0, 37.0, 159.0, 91.0, 268.0, 130.0, 206.0, 163.0, 208.0, 151.0, 185.0, 106.0, 175.0, 143.0, 154.0, 169.0, 142.0, 250.0, 155.0, 219.0, 253.0, 220.0, 171.0, 177.0, 276.0, 303.0, 176.0, 287.0, 290.0, 187.0, 353.0, 173.0, 475.0, 422.0, 349.0, 207.0, 375.0, 252.0, 122.0, 312.0, 344.0, 236.0, 457.0, 444.0, 440.0, 290.0, 375.0, 474.0, 408.0, 418.0, 246.0, 391.0, 195.0, 140.0, 381.0, 240.0, 280.0, 433.0, 175.0, 121.0, 162.0, 387.0, 162.0, 191.0, 195.0, 487.0, 315.0, 300.0, 124.0, 284.0, 414.0, 332.0, 300.0, 448.0, 238.0, 148.0, 272.0, 417.0, 57.0, 210.0, 22.0, 185.0, 23.0, 53.0, 80.0, 295.0, 174.0, 142.0, 142.0, 99.0, 230.0, 141.0, 351.0, 323.0, 145.0, 239.0, 199.0, 169.0, 182.0, 92.0, 308.0, 129.0, 191.0, 180.0, 243.0, 321.0, 152.0, 51.0, 91.0, 239.0, 469.0, 404.0, 103.0, 467.0, 120.0, 202.0, 181.0, 266.0, 334.0, 267.0, 142.0, 65.0, 131.0, 247.0, 284.0, 224.0, 184.0, 234.0, 257.0, 194.0, 261.0, 290.0, 230.0, 193.0, 242.0, 268.0, 163.0, 263.0, 286.0, 179.0, 170.0, 210.0, 280.0, 185.0, 216.0, 189.0, 194.0, 79.0, 128.0, 130.0, 134.0, 185.0, 189.0, 205.0, 194.0, 236.0, 156.0, 161.0, 221.0, 141.0, 192.0, 41.0, 201.0, 164.0, 162.0, 163.0, 155.0, 140.0, 141.0, 148.0, 185.0, 204.0, 89.0, 114.0, 135.0, 252.0, 194.0, 154.0, 142.0, 199.0, 170.0, 191.0, 166.0, 191.0, 153.0, 130.0, 187.0, 138.0, 138.0, 128.0, 175.0, 119.0, 141.0, 178.0, 187.0, 133.0, 136.0, 160.0, 143.0, 174.0, 164.0, 180.0, 265.0, 191.0, 135.0, 160.0, 182.0, 156.0, 369.0, 170.0, 260.0, 192.0, 299.0, 328.0, 189.0, 411.0, 349.0, 292.0, 114.0, 129.0, 425.0, 292.0, 423.0, 146.0, 144.0, 269.0, 221.0, 127.0, 128.0, 80.0, 121.0, 155.0, 174.0, 198.0, 170.0, 106.0, 66.0, 74.0, 123.0, 109.0, 97.0, 84.0, 160.0, 124.0, 85.0, 63.0, 55.0, 111.0, 91.0, 63.0, 137.0, 40.0, 120.0, 92.0, 137.0, 114.0, 115.0, 135.0, 135.0, 70.0, 73.0, 57.0, 59.0, 90.0, 134.0, 101.0, 111.0, 77.0, 100.0, 46.0, 61.0, 67.0, 74.0, 132.0, 84.0, 64.0, 99.0, 105.0, 37.0, 30.0, 126.0, 117.0, 104.0, 102.0, 64.0, 56.0, 71.0, 89.0, 85.0, 116.0, 98.0, 23.0, 80.0, 77.0, 117.0, 77.0, 77.0, 98.0, 74.0, 92.0, 84.0, 75.0, 76.0, 83.0, 73.0, 64.0, 96.0, 95.0, 74.0, 125.0, 79.0, 111.0, 79.0, 68.0, 59.0, 103.0, 82.0, 62.0, 94.0, 84.0, 89.0, 104.0, 65.0, 84.0, 121.0, 94.0, 88.0, 102.0, 98.0, 90.0, 86.0, 93.0, 72.0, 76.0, 77.0, 131.0, 82.0, 72.0, 42.0, 76.0, 98.0, 117.0, 77.0, 84.0, 87.0, 105.0, 112.0, 87.0, 79.0, 90.0, 91.0, 99.0, 113.0, 91.0, 127.0, 127.0, 122.0, 128.0, 94.0, 143.0, 148.0, 111.0, 99.0, 149.0, 96.0, 109.0, 77.0, 128.0, 121.0, 101.0, 101.0, 124.0, 103.0, 95.0, 120.0, 84.0, 15.0, 116.0, 145.0, 149.0, 126.0, 164.0, 175.0, 179.0, 156.0, 158.0, 172.0, 163.0, 158.0, 149.0, 154.0, 161.0, 135.0, 178.0, 160.0, 168.0, 103.0, 197.0, 137.0, 19.0, 177.0, 202.0, 242.0, 200.0, 193.0, 236.0, 234.0, 255.0, 153.0, 199.0, 222.0, 291.0, 162.0, 251.0, 227.0, 118.0, 222.0, 230.0, 153.0, 97.0, 96.0, 129.0, 183.0, 38.0, 99.0], [15.0, 10.0, 12.0, 15.0, 16.0, 14.0, 13.0, 14.0, 20.0, 19.0, 12.0, 17.0, 12.0, 12.0, 17.0, 23.0, 16.0, 14.0, 16.0, 27.0, 15.0, 11.0, 14.0, 22.0, 36.0, 40.0, 21.0, 19.0, 24.0, 18.0, 16.0, 16.0, 14.0, 19.0, 53.0, 14.0, 23.0, 11.0, 14.0, 12.0, 24.0, 18.0, 31.0, 16.0, 15.0, 57.0, 16.0, 11.0, 11.0, 44.0, 16.0, 35.0, 73.0, 23.0, 28.0, 75.0, 87.0, 56.0, 26.0, 33.0, 41.0, 45.0, 32.0, 23.0, 29.0, 20.0, 68.0, 34.0, 49.0, 36.0, 23.0, 11.0, 14.0, 21.0, 22.0, 34.0, 22.0, 23.0, 24.0, 35.0, 77.0, 53.0, 65.0, 51.0, 19.0, 25.0, 19.0, 15.0, 39.0, 51.0, 37.0, 21.0, 46.0, 64.0, 62.0, 37.0, 35.0, 27.0, 21.0, 73.0, 69.0, 36.0, 23.0, 20.0, 30.0, 30.0, 19.0, 72.0, 35.0, 10.0, 65.0, 105.0, 55.0, 93.0, 21.0, 28.0, 46.0, 67.0, 79.0, 73.0, 41.0, 17.0, 63.0, 63.0, 61.0, 18.0, 53.0, 12.0, 14.0, 53.0, 36.0, 99.0, 52.0, 41.0, 63.0, 70.0, 35.0, 87.0, 38.0, 46.0, 35.0, 71.0, 53.0, 58.0, 68.0, 39.0, 36.0, 51.0, 25.0, 52.0, 72.0, 19.0, 17.0, 64.0, 82.0, 42.0, 10.0, 41.0, 71.0, 97.0, 21.0, 71.0, 59.0, 68.0, 83.0, 16.0, 112.0, 57.0, 43.0, 37.0, 44.0, 49.0, 87.0, 35.0, 113.0, 66.0, 44.0, 13.0, 54.0, 33.0, 33.0, 33.0, 29.0, 61.0, 15.0, 98.0, 42.0, 21.0, 73.0, 57.0, 51.0, 30.0, 75.0, 47.0, 79.0, 14.0, 34.0, 44.0, 23.0, 60.0, 51.0, 72.0, 80.0, 59.0, 95.0, 54.0, 66.0, 81.0, 56.0, 129.0, 69.0, 84.0, 87.0, 65.0, 124.0, 44.0, 119.0, 69.0, 55.0, 59.0, 57.0, 98.0, 37.0, 76.0, 68.0, 44.0, 101.0, 116.0, 176.0, 27.0, 137.0, 27.0, 45.0, 54.0, 85.0, 66.0, 32.0, 37.0, 54.0, 91.0, 62.0, 150.0, 75.0, 79.0, 39.0, 98.0, 70.0, 202.0, 38.0, 86.0, 25.0, 28.0, 37.0, 65.0, 42.0, 170.0, 29.0, 48.0, 110.0, 47.0, 48.0, 128.0, 45.0, 72.0, 56.0, 71.0, 93.0, 43.0, 103.0, 171.0, 47.0, 110.0, 71.0, 108.0, 250.0, 89.0, 98.0, 56.0, 55.0, 105.0, 60.0, 167.0, 159.0, 132.0, 32.0, 48.0, 43.0, 87.0, 110.0, 64.0, 236.0, 40.0, 132.0, 224.0, 78.0, 244.0, 77.0, 60.0, 210.0, 57.0, 78.0, 196.0, 91.0, 148.0, 47.0, 77.0, 55.0, 97.0, 52.0, 68.0, 31.0, 78.0, 151.0, 205.0, 219.0, 218.0, 255.0, 120.0, 87.0, 101.0, 66.0, 27.0, 45.0, 210.0, 104.0, 29.0, 113.0, 178.0, 146.0, 77.0, 154.0, 88.0, 35.0, 231.0, 143.0, 52.0, 238.0, 107.0, 183.0, 14.0, 70.0, 61.0, 75.0, 83.0, 106.0, 197.0, 22.0, 111.0, 132.0, 270.0, 78.0, 72.0, 238.0, 136.0, 134.0, 83.0, 154.0, 129.0, 222.0, 429.0, 201.0, 147.0, 427.0, 293.0, 424.0, 291.0, 200.0, 130.0, 156.0, 147.0, 135.0, 72.0, 116.0, 160.0, 396.0, 386.0, 158.0, 224.0, 323.0, 186.0, 71.0, 114.0, 77.0, 103.0, 109.0, 256.0, 132.0, 75.0, 87.0, 136.0, 71.0, 120.0, 12.0, 76.0, 104.0, 84.0, 96.0, 53.0, 97.0, 95.0, 59.0, 81.0, 79.0, 89.0, 155.0, 73.0, 53.0, 25.0, 117.0, 43.0, 46.0, 26.0, 26.0, 62.0, 62.0, 26.0, 152.0, 44.0, 42.0, 40.0, 44.0, 71.0, 59.0, 18.0, 48.0, 156.0, 55.0, 23.0, 15.0, 19.0, 73.0, 74.0, 95.0, 74.0, 118.0, 86.0, 118.0, 101.0, 55.0, 33.0, 130.0, 48.0, 32.0, 110.0, 64.0, 35.0, 54.0, 64.0, 73.0, 89.0, 134.0, 35.0, 61.0, 34.0, 90.0, 80.0, 100.0, 94.0, 105.0, 68.0, 95.0, 67.0, 114.0, 83.0, 106.0, 158.0, 145.0, 119.0, 51.0, 142.0, 213.0, 74.0, 105.0, 112.0, 106.0, 117.0, 143.0, 91.0, 175.0, 232.0, 38.0, 121.0, 21.0, 108.0, 135.0, 111.0, 85.0, 84.0, 124.0, 84.0, 52.0, 93.0, 76.0, 79.0, 67.0, 107.0, 39.0, 91.0, 156.0, 121.0, 18.0, 104.0, 139.0, 126.0, 134.0, 136.0, 29.0, 192.0, 157.0, 110.0, 92.0, 79.0, 170.0, 170.0, 197.0, 302.0, 278.0, 59.0, 128.0, 28.0, 27.0, 252.0, 57.0, 24.0, 283.0, 169.0, 195.0, 250.0, 246.0, 105.0, 131.0, 204.0, 218.0, 218.0, 147.0, 203.0, 118.0, 129.0, 32.0, 190.0, 152.0, 164.0, 167.0, 155.0, 194.0, 193.0, 153.0, 256.0, 149.0, 184.0, 148.0, 143.0, 214.0, 171.0, 184.0, 72.0, 461.0, 226.0, 284.0, 165.0, 210.0, 352.0, 185.0, 55.0, 86.0, 25.0, 476.0, 51.0, 144.0, 21.0, 78.0, 269.0, 16.0, 132.0, 25.0, 292.0, 471.0, 389.0, 375.0, 260.0, 306.0, 228.0, 271.0, 187.0, 305.0, 306.0, 134.0, 338.0, 246.0, 124.0, 216.0, 416.0, 127.0, 139.0, 289.0, 431.0, 287.0, 115.0, 214.0, 258.0, 119.0, 346.0, 119.0, 69.0, 251.0, 254.0, 266.0, 247.0, 79.0, 202.0, 307.0, 277.0, 244.0, 266.0, 320.0, 354.0, 131.0, 174.0, 317.0, 229.0, 184.0, 191.0, 397.0, 331.0, 181.0, 145.0, 254.0, 297.0, 361.0, 66.0, 386.0, 223.0, 330.0, 271.0, 208.0, 184.0, 225.0, 189.0, 236.0, 171.0, 187.0, 159.0, 200.0, 216.0, 45.0, 208.0, 214.0, 164.0, 142.0, 179.0, 156.0, 190.0, 185.0, 211.0, 187.0, 176.0, 213.0, 118.0, 168.0, 36.0, 85.0, 40.0, 103.0, 150.0, 62.0, 45.0, 96.0, 97.0, 147.0, 180.0, 146.0, 280.0, 22.0, 208.0, 125.0, 68.0, 27.0, 77.0, 208.0, 21.0, 155.0, 83.0, 19.0, 180.0, 112.0, 156.0, 213.0, 272.0, 105.0, 168.0, 214.0, 140.0, 125.0, 155.0, 289.0, 180.0, 243.0, 226.0, 48.0, 286.0, 44.0, 165.0, 251.0, 239.0, 230.0, 266.0, 177.0, 261.0, 262.0, 223.0, 225.0, 184.0, 203.0, 226.0, 185.0, 298.0, 172.0, 186.0, 150.0, 177.0, 257.0, 14.0, 193.0, 160.0, 203.0, 177.0, 187.0, 221.0, 131.0, 166.0, 194.0, 220.0, 185.0, 208.0, 279.0, 230.0, 207.0, 244.0, 318.0, 209.0, 336.0, 188.0, 255.0, 221.0, 288.0, 98.0, 254.0, 220.0, 219.0, 157.0, 161.0, 167.0, 162.0, 201.0, 189.0, 206.0, 176.0, 177.0, 202.0, 205.0, 108.0, 225.0, 53.0, 198.0, 189.0, 186.0, 200.0, 190.0, 178.0, 240.0, 204.0, 166.0, 219.0, 180.0, 200.0, 200.0, 19.0, 191.0, 182.0, 217.0, 167.0, 210.0, 28.0, 184.0, 283.0, 166.0, 110.0, 171.0, 87.0, 108.0, 110.0, 132.0, 152.0, 114.0, 128.0, 119.0, 227.0, 132.0, 136.0, 137.0, 132.0, 55.0, 40.0, 68.0, 79.0, 160.0, 107.0, 101.0, 109.0, 137.0, 114.0, 112.0, 173.0, 62.0, 13.0, 84.0, 74.0, 208.0, 12.0, 131.0, 124.0, 95.0, 96.0, 91.0, 96.0, 154.0, 87.0, 108.0, 85.0, 106.0, 125.0, 85.0, 103.0, 129.0, 127.0, 120.0, 106.0, 130.0, 132.0, 180.0, 126.0, 125.0, 202.0, 138.0, 129.0, 162.0, 104.0, 170.0, 155.0, 154.0, 139.0, 151.0, 41.0, 154.0, 200.0, 114.0, 190.0, 32.0, 167.0, 165.0, 100.0, 166.0, 126.0, 186.0, 172.0, 168.0, 155.0, 164.0, 181.0, 74.0, 140.0, 109.0, 134.0, 156.0, 154.0, 142.0, 142.0, 150.0, 21.0, 165.0, 138.0, 26.0, 38.0, 124.0, 129.0, 136.0, 160.0, 79.0, 18.0, 119.0, 15.0, 34.0, 41.0, 144.0, 148.0, 89.0, 35.0, 79.0, 46.0, 34.0, 28.0, 133.0, 151.0, 143.0, 132.0, 124.0, 139.0, 104.0, 92.0, 145.0, 132.0, 111.0, 48.0, 140.0, 134.0, 135.0, 90.0, 32.0, 61.0, 88.0, 123.0, 57.0, 157.0, 128.0, 16.0, 131.0, 134.0, 133.0, 133.0, 145.0, 132.0, 49.0, 146.0, 143.0, 152.0, 131.0, 148.0, 128.0, 164.0, 168.0, 158.0, 160.0, 145.0, 163.0, 142.0, 143.0, 128.0, 116.0, 80.0, 117.0, 101.0, 105.0, 118.0, 99.0, 112.0, 71.0, 97.0, 78.0, 186.0, 141.0, 172.0, 125.0, 142.0, 18.0, 13.0, 120.0, 141.0, 146.0, 102.0, 118.0, 18.0, 174.0, 116.0, 137.0, 16.0, 50.0, 106.0, 149.0, 117.0, 135.0, 118.0, 123.0, 125.0, 14.0, 82.0, 80.0, 113.0], [27.0, 18.0, 20.0, 32.0, 18.0, 25.0, 18.0, 13.0, 11.0, 49.0, 45.0, 24.0, 27.0, 14.0, 20.0, 61.0, 29.0, 15.0, 25.0, 13.0, 31.0, 25.0, 11.0, 32.0, 45.0, 37.0, 35.0, 21.0, 34.0, 40.0, 27.0, 33.0, 54.0, 50.0, 66.0, 78.0, 26.0, 11.0, 47.0, 43.0, 29.0, 30.0, 29.0, 50.0, 16.0, 18.0, 9.0, 8.0, 12.0, 21.0, 19.0, 16.0, 34.0, 14.0, 24.0, 12.0, 18.0, 20.0, 32.0, 24.0, 14.0, 14.0, 13.0, 17.0, 14.0, 17.0, 20.0, 14.0, 15.0, 37.0, 25.0, 37.0, 12.0, 39.0, 13.0, 10.0, 47.0, 32.0, 23.0, 35.0, 23.0, 26.0, 36.0, 18.0, 15.0, 27.0, 16.0, 22.0, 21.0, 12.0, 12.0, 18.0, 26.0, 30.0, 30.0, 19.0, 44.0, 62.0, 35.0, 50.0, 44.0, 42.0, 39.0, 43.0, 11.0, 13.0, 48.0, 46.0, 29.0, 37.0, 39.0, 33.0, 13.0, 143.0, 26.0, 36.0, 68.0, 43.0, 18.0, 36.0, 51.0, 31.0, 35.0, 68.0, 47.0, 28.0, 63.0, 8.0, 54.0, 41.0, 70.0, 19.0, 10.0, 20.0, 33.0, 21.0, 61.0, 36.0, 24.0, 23.0, 44.0, 22.0, 52.0, 15.0, 34.0, 28.0, 43.0, 111.0, 54.0, 44.0, 74.0, 17.0, 32.0, 13.0, 18.0, 21.0, 50.0, 40.0, 28.0, 19.0, 14.0, 24.0, 13.0, 43.0, 16.0, 42.0, 51.0, 115.0, 66.0, 27.0, 71.0, 105.0, 42.0, 80.0, 25.0, 108.0, 15.0, 22.0, 56.0, 13.0, 26.0, 21.0, 14.0, 64.0, 13.0, 105.0, 28.0, 56.0, 41.0, 77.0, 141.0, 76.0, 42.0, 101.0, 11.0, 84.0, 94.0, 39.0, 45.0, 72.0, 55.0, 42.0, 13.0, 53.0, 79.0, 76.0, 52.0, 31.0, 12.0, 53.0, 18.0, 104.0, 55.0, 13.0, 69.0, 52.0, 15.0, 15.0, 90.0, 101.0, 51.0, 70.0, 86.0, 21.0, 98.0, 46.0, 72.0, 67.0, 13.0, 51.0, 74.0, 87.0, 19.0, 79.0, 92.0, 23.0, 46.0, 29.0, 15.0, 87.0, 10.0, 13.0, 20.0, 61.0, 49.0, 14.0, 39.0, 25.0, 21.0, 63.0, 114.0, 73.0, 103.0, 71.0, 78.0, 57.0, 51.0, 47.0, 185.0, 111.0, 91.0, 93.0, 63.0, 96.0, 147.0, 128.0, 29.0, 50.0, 89.0, 86.0, 139.0, 111.0, 98.0, 17.0, 101.0, 145.0, 69.0, 75.0, 84.0, 84.0, 74.0, 93.0, 72.0, 118.0, 94.0, 56.0, 111.0, 114.0, 90.0, 87.0, 76.0, 187.0, 66.0, 72.0, 67.0, 92.0, 76.0, 90.0, 112.0, 94.0, 134.0, 108.0, 119.0, 150.0, 121.0, 176.0, 142.0, 165.0, 319.0, 308.0, 306.0, 54.0, 183.0, 58.0, 12.0, 78.0, 147.0, 83.0, 184.0, 230.0, 53.0, 105.0, 131.0, 267.0, 199.0, 93.0, 228.0, 152.0, 414.0, 138.0, 62.0, 50.0, 262.0, 170.0, 49.0, 219.0, 278.0, 337.0, 309.0, 108.0, 330.0, 414.0, 134.0, 149.0, 303.0, 224.0, 168.0, 339.0, 185.0, 58.0, 256.0, 190.0, 189.0, 207.0, 160.0, 97.0, 191.0, 96.0, 170.0, 45.0, 126.0, 46.0, 172.0, 64.0, 38.0, 216.0, 65.0, 113.0, 137.0, 107.0, 53.0, 84.0, 18.0, 100.0, 43.0, 57.0, 327.0, 76.0, 188.0, 115.0, 151.0, 81.0, 114.0, 48.0, 224.0, 194.0, 128.0, 31.0, 66.0, 41.0, 115.0, 189.0, 136.0, 141.0, 96.0, 125.0, 74.0, 103.0, 128.0, 99.0, 161.0, 95.0, 112.0, 152.0, 174.0, 111.0, 88.0, 120.0, 86.0, 15.0, 70.0, 98.0, 148.0, 89.0, 118.0, 125.0, 77.0, 128.0, 54.0, 92.0, 89.0, 59.0, 92.0, 162.0, 116.0, 74.0, 113.0, 95.0, 164.0, 121.0, 149.0, 60.0, 25.0, 129.0, 150.0, 187.0, 103.0, 153.0, 119.0, 93.0, 185.0, 208.0, 133.0, 106.0, 151.0, 101.0, 96.0, 132.0, 186.0, 195.0, 94.0, 85.0, 222.0, 206.0, 121.0, 121.0, 41.0, 203.0, 27.0, 204.0, 141.0, 88.0, 303.0, 160.0, 135.0, 79.0, 249.0, 71.0, 376.0, 476.0, 159.0, 307.0, 59.0, 129.0, 212.0, 152.0, 162.0, 341.0, 55.0, 480.0, 211.0, 238.0, 380.0, 187.0, 180.0, 82.0, 147.0, 88.0, 189.0, 173.0, 190.0, 196.0, 123.0, 154.0, 196.0, 143.0, 145.0, 183.0, 114.0, 51.0, 174.0, 164.0, 182.0, 136.0, 116.0, 252.0, 133.0, 179.0, 91.0, 66.0, 89.0, 95.0, 86.0, 45.0, 155.0, 176.0, 151.0, 80.0, 103.0, 81.0, 133.0, 104.0, 209.0, 144.0, 15.0, 112.0, 138.0, 29.0, 78.0, 17.0, 53.0, 90.0, 131.0, 89.0, 114.0, 109.0, 145.0, 101.0, 88.0, 111.0, 76.0, 67.0, 43.0, 65.0, 121.0, 151.0, 42.0, 53.0, 80.0, 82.0, 25.0, 60.0, 92.0, 36.0, 137.0, 154.0, 73.0, 93.0, 42.0, 18.0, 56.0, 32.0, 10.0, 45.0, 96.0, 102.0, 14.0, 58.0, 34.0, 35.0, 92.0, 58.0, 31.0, 34.0, 50.0, 94.0, 35.0, 114.0, 11.0, 41.0, 42.0, 51.0, 113.0, 186.0, 136.0, 43.0, 27.0, 44.0, 147.0, 117.0, 24.0, 252.0, 81.0, 131.0, 239.0, 141.0, 99.0, 164.0, 130.0, 184.0, 112.0, 77.0, 105.0, 193.0, 68.0, 87.0, 129.0, 89.0, 174.0, 226.0, 282.0, 343.0, 143.0, 64.0, 69.0, 166.0, 201.0, 101.0, 250.0, 305.0, 99.0, 152.0, 199.0, 50.0, 219.0, 402.0, 73.0, 317.0, 375.0, 107.0, 352.0, 159.0, 241.0, 136.0, 154.0, 128.0, 141.0, 226.0, 237.0, 232.0, 127.0, 125.0, 232.0, 259.0, 129.0, 165.0, 78.0, 258.0, 313.0, 345.0, 288.0, 58.0, 60.0, 426.0, 362.0, 75.0, 70.0, 52.0, 57.0, 158.0, 72.0, 54.0, 72.0, 104.0, 61.0, 14.0, 166.0, 74.0, 86.0, 112.0, 17.0, 63.0, 36.0, 127.0, 61.0, 100.0, 41.0, 171.0, 156.0, 180.0, 161.0, 34.0, 178.0, 159.0, 10.0, 14.0, 158.0, 118.0, 17.0, 163.0, 8.0, 128.0, 11.0, 92.0, 127.0, 15.0, 104.0, 11.0, 88.0, 20.0, 73.0, 69.0, 19.0, 89.0, 81.0, 60.0, 93.0, 116.0, 17.0, 14.0, 22.0, 13.0, 49.0, 106.0, 160.0, 20.0, 112.0, 15.0, 51.0, 168.0, 33.0, 16.0, 27.0, 141.0, 24.0, 39.0, 43.0, 12.0, 40.0, 177.0, 130.0, 146.0, 151.0, 235.0, 62.0, 188.0, 194.0, 196.0, 293.0, 272.0, 284.0, 271.0, 16.0, 370.0, 273.0, 334.0, 324.0, 41.0, 14.0, 17.0, 55.0, 12.0, 12.0, 86.0, 16.0, 9.0, 17.0, 28.0, 15.0, 63.0, 17.0, 15.0, 12.0, 17.0, 13.0, 44.0, 89.0, 47.0, 160.0, 57.0, 54.0, 70.0, 49.0, 49.0, 47.0, 184.0, 55.0, 140.0, 60.0, 74.0, 119.0, 161.0, 58.0, 54.0, 70.0, 107.0, 50.0, 41.0, 74.0, 63.0, 50.0, 24.0, 53.0, 71.0, 61.0, 176.0, 108.0, 180.0, 82.0, 98.0, 40.0, 105.0, 74.0, 133.0, 148.0, 396.0, 317.0, 295.0, 449.0, 60.0, 78.0, 181.0, 329.0, 390.0, 103.0, 469.0, 178.0, 253.0, 117.0, 81.0, 135.0, 97.0, 91.0, 174.0, 70.0, 97.0, 92.0, 126.0, 94.0, 77.0, 44.0, 116.0, 87.0, 16.0, 85.0, 101.0, 70.0, 75.0, 142.0, 92.0, 112.0, 38.0, 82.0, 118.0, 156.0, 162.0, 178.0, 155.0, 172.0, 125.0, 24.0, 300.0, 235.0, 217.0, 119.0, 152.0, 268.0, 406.0, 353.0, 141.0, 357.0, 342.0, 315.0, 37.0, 189.0, 310.0, 487.0, 201.0, 288.0, 156.0, 252.0, 245.0, 265.0, 388.0, 471.0, 330.0, 371.0, 147.0, 178.0, 317.0, 315.0, 186.0, 96.0, 150.0, 86.0, 464.0, 168.0, 87.0, 95.0, 81.0, 95.0, 123.0, 125.0, 161.0, 148.0, 148.0, 117.0, 250.0, 106.0, 133.0, 122.0, 119.0, 85.0, 124.0, 11.0, 15.0, 171.0, 166.0, 12.0, 67.0, 178.0, 178.0, 71.0, 126.0, 144.0, 168.0, 130.0, 187.0, 11.0, 149.0, 139.0, 265.0, 99.0, 126.0, 245.0, 124.0], [32.0, 15.0, 13.0, 19.0, 13.0, 28.0, 21.0, 12.0, 16.0, 34.0, 26.0, 34.0, 21.0, 65.0, 24.0, 43.0, 28.0, 26.0, 38.0, 36.0, 13.0, 45.0, 21.0, 15.0, 60.0, 36.0, 13.0, 30.0, 24.0, 43.0, 19.0, 61.0, 44.0, 85.0, 27.0, 32.0, 29.0, 17.0, 59.0, 34.0, 11.0, 27.0, 57.0, 18.0, 55.0, 45.0, 12.0, 68.0, 59.0, 40.0, 22.0, 23.0, 79.0, 39.0, 44.0, 32.0, 69.0, 46.0, 21.0, 40.0, 55.0, 32.0, 44.0, 53.0, 25.0, 23.0, 41.0, 62.0, 13.0, 56.0, 34.0, 50.0, 41.0, 38.0, 30.0, 10.0, 61.0, 44.0, 27.0, 32.0, 39.0, 76.0, 31.0, 35.0, 73.0, 51.0, 39.0, 71.0, 62.0, 16.0, 68.0, 46.0, 33.0, 68.0, 67.0, 60.0, 47.0, 14.0, 33.0, 52.0, 49.0, 29.0, 25.0, 45.0, 54.0, 67.0, 48.0, 56.0, 28.0, 54.0, 22.0, 13.0, 45.0, 27.0, 77.0, 55.0, 38.0, 15.0, 47.0, 233.0, 42.0, 36.0, 45.0, 41.0, 13.0, 54.0, 69.0, 34.0, 39.0, 32.0, 85.0, 49.0, 63.0, 63.0, 40.0, 74.0, 36.0, 80.0, 11.0, 50.0, 19.0, 61.0, 90.0, 34.0, 67.0, 92.0, 94.0, 25.0, 79.0, 45.0, 111.0, 27.0, 50.0, 45.0, 75.0, 72.0, 54.0, 23.0, 54.0, 34.0, 28.0, 150.0, 14.0, 31.0, 30.0, 35.0, 77.0, 53.0, 89.0, 43.0, 94.0, 71.0, 60.0, 60.0, 66.0, 42.0, 97.0, 60.0, 57.0, 31.0, 83.0, 80.0, 56.0, 30.0, 54.0, 58.0, 66.0, 100.0, 86.0, 111.0, 15.0, 32.0, 100.0, 74.0, 31.0, 34.0, 142.0, 144.0, 60.0, 19.0, 30.0, 74.0, 54.0, 40.0, 78.0, 29.0, 54.0, 71.0, 15.0, 70.0, 97.0, 71.0, 54.0, 14.0, 51.0, 49.0, 125.0, 14.0, 24.0, 108.0, 36.0, 15.0, 67.0, 117.0, 22.0, 44.0, 204.0, 11.0, 127.0, 88.0, 62.0, 55.0, 186.0, 148.0, 70.0, 138.0, 73.0, 102.0, 125.0, 65.0, 154.0, 80.0, 161.0, 70.0, 32.0, 93.0, 103.0, 89.0, 91.0, 109.0, 87.0, 96.0, 83.0, 56.0, 34.0, 77.0, 36.0, 68.0, 20.0, 99.0, 116.0, 11.0, 62.0, 46.0, 20.0, 78.0, 55.0, 43.0, 43.0, 70.0, 57.0, 90.0, 58.0, 27.0, 31.0, 131.0, 158.0, 59.0, 35.0, 101.0, 80.0, 47.0, 47.0, 81.0, 114.0, 90.0, 85.0, 152.0, 161.0, 215.0, 33.0, 74.0, 40.0, 92.0, 216.0, 50.0, 78.0, 194.0, 87.0, 145.0, 60.0, 254.0, 30.0, 129.0, 70.0, 161.0, 181.0, 194.0, 80.0, 54.0, 164.0, 185.0, 251.0, 124.0, 77.0, 198.0, 101.0, 55.0, 23.0, 272.0, 360.0, 234.0, 257.0, 103.0, 297.0, 191.0, 39.0, 21.0, 101.0, 68.0, 183.0, 114.0, 85.0, 48.0, 172.0, 298.0, 143.0, 312.0, 24.0, 227.0, 51.0, 43.0, 108.0, 48.0, 203.0, 26.0, 43.0, 117.0, 38.0, 27.0, 61.0, 74.0, 59.0, 90.0, 43.0, 129.0, 162.0, 188.0, 152.0, 41.0, 45.0, 126.0, 92.0, 42.0, 114.0, 277.0, 223.0, 25.0, 142.0, 204.0, 79.0, 59.0, 25.0, 174.0, 107.0, 68.0, 20.0, 92.0, 185.0, 17.0, 89.0, 32.0, 202.0, 30.0, 87.0, 256.0, 19.0, 214.0, 89.0, 97.0, 70.0, 123.0, 157.0, 147.0, 109.0, 34.0, 24.0, 139.0, 219.0, 235.0, 162.0, 183.0, 70.0, 65.0, 87.0, 157.0, 95.0, 79.0, 83.0, 60.0, 148.0, 105.0, 184.0, 118.0, 112.0, 192.0, 106.0, 240.0, 99.0, 49.0, 16.0, 71.0, 79.0, 71.0, 105.0, 38.0, 103.0, 130.0, 179.0, 180.0, 50.0, 180.0, 101.0, 151.0, 71.0, 247.0, 158.0, 112.0, 117.0, 76.0, 57.0, 74.0, 83.0, 240.0, 122.0, 125.0, 92.0, 89.0, 154.0, 146.0, 17.0, 153.0, 166.0, 58.0, 74.0, 83.0, 62.0, 103.0, 65.0, 46.0, 67.0, 81.0, 57.0, 288.0, 132.0, 89.0, 137.0, 236.0, 338.0, 189.0, 222.0, 303.0, 293.0, 175.0, 194.0, 107.0, 175.0, 296.0, 105.0, 65.0, 83.0, 86.0, 72.0, 196.0, 79.0, 13.0, 291.0, 226.0, 85.0, 22.0, 42.0, 137.0, 174.0, 37.0, 33.0, 100.0, 102.0, 133.0, 149.0, 140.0, 248.0, 233.0, 262.0, 342.0, 311.0, 317.0, 265.0, 125.0, 166.0, 286.0, 39.0, 132.0, 193.0, 107.0, 303.0, 142.0, 115.0, 35.0, 161.0, 78.0, 99.0, 63.0, 113.0, 459.0, 97.0, 49.0, 101.0, 91.0, 163.0, 148.0, 102.0, 69.0, 213.0, 197.0, 363.0, 119.0, 99.0, 163.0, 262.0, 34.0, 150.0, 15.0, 67.0, 141.0, 74.0, 122.0, 216.0, 117.0, 101.0, 267.0, 125.0, 273.0, 106.0, 183.0, 155.0, 176.0, 17.0, 434.0, 93.0, 91.0, 111.0, 440.0, 419.0, 167.0, 188.0, 126.0, 329.0, 228.0, 179.0, 68.0, 132.0, 169.0, 52.0, 92.0, 68.0, 106.0, 101.0, 105.0, 68.0, 117.0, 65.0, 49.0, 80.0, 67.0, 110.0, 105.0, 55.0, 86.0, 80.0, 102.0, 83.0, 102.0, 49.0, 78.0, 92.0, 83.0, 97.0, 96.0, 85.0, 75.0, 96.0, 63.0, 65.0, 89.0, 73.0, 135.0, 87.0, 25.0, 62.0, 58.0, 110.0, 198.0, 141.0, 78.0, 60.0, 73.0, 98.0, 95.0, 48.0, 70.0, 166.0, 95.0, 67.0, 71.0, 85.0, 90.0, 76.0, 161.0, 45.0, 90.0, 78.0, 59.0, 11.0, 81.0, 62.0, 93.0, 111.0, 95.0, 97.0, 54.0, 87.0, 121.0, 77.0, 65.0, 44.0, 88.0, 74.0, 105.0, 75.0, 99.0, 98.0, 67.0, 114.0, 106.0, 99.0, 76.0, 24.0, 66.0, 159.0, 107.0, 91.0, 104.0, 123.0, 100.0, 160.0, 116.0, 140.0, 174.0, 199.0, 166.0, 153.0, 168.0, 119.0, 101.0, 222.0, 159.0, 136.0, 209.0, 168.0, 235.0, 157.0, 162.0, 193.0, 321.0, 318.0, 97.0, 184.0, 243.0, 148.0, 213.0, 324.0, 240.0, 251.0, 178.0, 163.0, 250.0, 252.0, 233.0, 167.0, 227.0, 213.0, 211.0, 211.0, 214.0, 194.0, 254.0, 240.0, 230.0, 229.0, 257.0, 147.0, 209.0, 219.0, 216.0, 297.0, 225.0, 197.0, 184.0, 217.0, 201.0, 204.0, 224.0, 235.0, 284.0, 252.0, 284.0, 286.0, 379.0, 403.0, 371.0, 136.0, 426.0, 497.0, 475.0, 447.0, 98.0, 127.0, 317.0, 212.0, 286.0, 202.0, 284.0, 217.0, 233.0, 301.0, 240.0, 139.0, 114.0, 193.0, 125.0, 107.0, 53.0, 83.0, 71.0, 93.0, 27.0, 73.0, 235.0, 416.0, 254.0, 200.0, 178.0, 242.0, 177.0, 218.0, 223.0, 197.0, 217.0, 321.0, 350.0, 304.0, 328.0, 366.0, 293.0, 227.0, 228.0, 259.0, 324.0, 328.0, 281.0, 241.0, 236.0, 283.0, 286.0, 282.0, 294.0, 306.0, 375.0, 297.0, 422.0, 425.0, 207.0, 155.0, 156.0, 392.0, 496.0, 161.0, 101.0, 416.0, 433.0, 334.0, 485.0, 308.0, 206.0, 450.0, 420.0, 480.0, 11.0, 197.0, 468.0, 409.0, 427.0, 463.0, 425.0, 429.0, 14.0, 428.0, 404.0, 436.0, 98.0, 413.0, 388.0, 336.0, 296.0, 354.0, 340.0, 336.0, 346.0, 343.0, 119.0, 70.0, 332.0, 332.0, 184.0, 311.0, 299.0, 359.0, 288.0, 211.0, 247.0, 322.0, 286.0, 248.0, 246.0, 252.0, 97.0, 258.0, 345.0, 317.0, 201.0, 293.0, 392.0, 454.0, 369.0, 429.0, 474.0, 369.0, 293.0, 206.0, 401.0, 351.0, 378.0, 312.0, 84.0, 338.0, 306.0, 378.0, 491.0, 288.0, 331.0, 217.0, 271.0, 249.0, 221.0, 159.0, 189.0, 124.0, 184.0, 119.0, 174.0, 161.0, 136.0, 173.0, 126.0, 157.0, 95.0, 147.0, 169.0, 90.0, 130.0, 77.0, 135.0, 156.0, 182.0, 182.0, 22.0, 70.0, 120.0, 137.0, 169.0, 165.0, 149.0, 192.0, 99.0, 112.0, 158.0, 140.0, 165.0, 105.0, 94.0, 165.0, 154.0, 173.0, 169.0, 170.0, 179.0, 143.0, 148.0, 182.0, 178.0, 167.0, 172.0, 151.0, 138.0, 143.0, 157.0, 137.0, 144.0, 134.0, 159.0, 188.0, 175.0, 159.0, 178.0, 161.0, 222.0, 151.0, 128.0, 180.0, 174.0, 200.0, 143.0, 167.0, 217.0, 157.0, 154.0], [23.0, 28.0, 35.0, 11.0, 14.0, 17.0, 15.0, 15.0, 25.0, 23.0, 22.0, 21.0, 88.0, 15.0, 47.0, 14.0, 21.0, 34.0, 16.0, 21.0, 35.0, 12.0, 17.0, 16.0, 27.0, 44.0, 67.0, 16.0, 21.0, 27.0, 33.0, 10.0, 16.0, 33.0, 16.0, 13.0, 13.0, 15.0, 14.0, 24.0, 56.0, 65.0, 77.0, 22.0, 35.0, 22.0, 65.0, 30.0, 30.0, 45.0, 25.0, 41.0, 32.0, 17.0, 61.0, 28.0, 35.0, 20.0, 30.0, 37.0, 10.0, 43.0, 41.0, 44.0, 40.0, 26.0, 31.0, 22.0, 29.0, 32.0, 49.0, 66.0, 26.0, 79.0, 47.0, 28.0, 49.0, 57.0, 48.0, 10.0, 35.0, 28.0, 94.0, 48.0, 59.0, 73.0, 51.0, 61.0, 36.0, 61.0, 18.0, 99.0, 60.0, 61.0, 37.0, 53.0, 19.0, 38.0, 90.0, 47.0, 67.0, 53.0, 83.0, 61.0, 63.0, 39.0, 32.0, 78.0, 78.0, 70.0, 98.0, 13.0, 24.0, 21.0, 46.0, 167.0, 59.0, 14.0, 120.0, 103.0, 206.0, 51.0, 86.0, 54.0, 49.0, 40.0, 121.0, 155.0, 21.0, 42.0, 110.0, 43.0, 123.0, 123.0, 70.0, 106.0, 124.0, 188.0, 108.0, 148.0, 146.0, 61.0, 66.0, 97.0, 61.0, 36.0, 144.0, 105.0, 113.0, 108.0, 61.0, 120.0, 108.0, 114.0, 311.0, 150.0, 39.0, 29.0, 281.0, 23.0, 39.0, 170.0, 73.0, 71.0, 61.0, 44.0, 52.0, 18.0, 62.0, 184.0, 172.0, 18.0, 30.0, 41.0, 345.0, 54.0, 73.0, 136.0, 68.0, 433.0, 100.0, 38.0, 185.0, 177.0, 103.0, 157.0, 56.0, 187.0, 50.0, 100.0, 174.0, 210.0, 74.0, 86.0, 147.0, 204.0, 170.0, 112.0, 197.0, 221.0, 61.0, 123.0, 75.0, 189.0, 29.0, 120.0, 220.0, 112.0, 120.0, 47.0, 146.0, 114.0, 101.0, 57.0, 71.0, 281.0, 126.0, 80.0, 170.0, 103.0, 136.0, 136.0, 315.0, 206.0, 80.0, 220.0, 197.0, 236.0, 57.0, 229.0, 107.0, 90.0, 185.0, 35.0, 319.0, 225.0, 154.0, 176.0, 116.0, 111.0, 194.0, 93.0, 96.0, 140.0, 182.0, 305.0, 166.0, 210.0, 151.0, 124.0, 86.0, 208.0, 73.0, 293.0, 81.0, 177.0, 123.0, 231.0, 282.0, 147.0, 162.0, 219.0, 181.0, 198.0, 105.0, 144.0, 306.0, 129.0, 155.0, 134.0, 75.0, 386.0, 193.0, 339.0, 272.0, 333.0, 290.0, 354.0, 122.0, 328.0, 242.0, 222.0, 190.0, 429.0, 294.0, 66.0, 438.0, 472.0, 178.0, 419.0, 242.0, 359.0, 401.0, 466.0, 177.0, 137.0, 478.0, 433.0, 266.0, 455.0, 134.0, 402.0, 454.0, 257.0, 198.0, 92.0, 378.0, 195.0, 38.0, 162.0, 268.0, 26.0, 262.0, 90.0, 36.0, 23.0, 165.0, 430.0, 421.0, 414.0, 313.0, 301.0, 266.0, 376.0, 379.0, 360.0, 187.0, 341.0, 139.0, 249.0, 126.0, 126.0, 111.0, 205.0, 242.0, 288.0, 195.0, 199.0, 16.0, 164.0, 187.0, 326.0, 183.0, 176.0, 119.0, 178.0, 205.0, 63.0, 310.0, 297.0, 243.0, 354.0, 166.0, 179.0, 485.0, 79.0, 207.0, 393.0, 217.0, 273.0, 173.0, 152.0, 246.0, 209.0, 168.0, 163.0, 127.0, 149.0, 155.0, 149.0, 168.0, 131.0, 373.0, 130.0, 159.0, 123.0, 86.0, 100.0, 87.0, 53.0, 71.0, 85.0, 95.0, 98.0, 63.0, 97.0, 96.0, 109.0, 107.0, 121.0, 99.0, 123.0, 128.0, 104.0, 95.0, 35.0, 114.0, 99.0, 70.0, 114.0, 94.0, 114.0, 125.0, 87.0, 122.0, 95.0, 86.0, 108.0, 115.0, 120.0, 111.0, 67.0, 129.0, 122.0, 91.0, 93.0, 82.0, 133.0, 118.0, 127.0, 114.0, 131.0, 125.0, 100.0, 112.0, 79.0, 93.0, 130.0, 72.0, 106.0, 92.0, 115.0, 108.0, 72.0, 64.0, 90.0, 125.0, 108.0, 82.0, 137.0, 127.0, 111.0, 102.0, 117.0, 115.0, 107.0, 131.0, 109.0, 83.0, 108.0, 87.0, 102.0, 91.0, 81.0, 86.0, 110.0, 86.0, 90.0, 97.0, 78.0, 95.0, 82.0, 122.0, 88.0, 103.0, 72.0, 106.0, 107.0, 141.0, 95.0, 102.0, 45.0, 84.0, 74.0, 89.0, 87.0, 52.0, 109.0, 89.0, 155.0, 96.0, 116.0, 166.0, 107.0, 192.0, 119.0, 141.0, 220.0, 55.0, 114.0, 62.0, 101.0, 160.0, 51.0, 100.0, 87.0, 118.0, 271.0, 102.0, 144.0, 179.0, 97.0, 206.0, 168.0, 229.0, 163.0, 248.0, 133.0, 162.0, 93.0, 190.0, 154.0, 181.0, 171.0, 140.0, 64.0, 185.0, 167.0, 318.0, 227.0, 221.0, 124.0, 201.0, 87.0, 314.0, 143.0, 25.0, 142.0, 26.0, 162.0, 70.0, 70.0, 106.0, 220.0, 38.0, 127.0, 174.0, 221.0, 113.0, 229.0, 221.0, 125.0, 210.0, 251.0, 239.0, 246.0, 288.0, 270.0, 204.0, 177.0, 256.0, 282.0, 350.0, 419.0, 313.0, 368.0, 361.0, 302.0, 328.0, 341.0, 327.0, 323.0, 325.0, 377.0, 405.0, 303.0, 380.0, 317.0, 317.0, 321.0, 328.0, 323.0, 321.0, 292.0, 381.0, 344.0, 248.0, 302.0, 348.0, 208.0, 397.0, 396.0, 383.0, 453.0, 474.0, 125.0, 448.0, 480.0, 473.0, 461.0, 174.0, 259.0, 312.0, 395.0, 211.0, 75.0, 337.0, 437.0, 349.0, 320.0, 429.0, 345.0, 251.0, 370.0, 419.0, 250.0, 292.0, 270.0, 373.0, 406.0, 329.0, 190.0, 401.0, 294.0, 170.0, 463.0, 406.0, 266.0, 332.0, 118.0, 254.0, 230.0, 166.0, 187.0, 125.0, 210.0, 68.0, 267.0, 131.0, 77.0, 142.0, 210.0, 346.0, 93.0, 177.0, 108.0, 76.0, 102.0, 291.0, 55.0, 48.0, 55.0, 179.0, 167.0, 154.0, 188.0, 146.0, 179.0, 185.0, 245.0, 351.0, 297.0, 259.0, 234.0, 231.0, 199.0, 289.0, 221.0, 178.0, 235.0, 196.0, 165.0, 425.0, 226.0, 236.0, 151.0, 154.0, 172.0, 210.0, 201.0, 142.0, 9.0, 179.0, 436.0, 121.0, 451.0, 161.0, 120.0, 206.0, 352.0, 254.0, 211.0, 297.0, 204.0, 241.0, 28.0, 136.0, 334.0, 415.0, 334.0, 365.0, 41.0, 175.0, 151.0, 319.0, 179.0, 203.0, 214.0, 301.0, 342.0, 291.0, 302.0, 392.0, 286.0, 308.0, 161.0, 192.0, 67.0, 344.0, 289.0, 259.0, 291.0, 342.0, 448.0, 232.0, 273.0, 279.0, 248.0, 239.0, 291.0, 179.0, 184.0, 166.0, 221.0, 197.0, 91.0, 230.0, 243.0, 360.0, 180.0, 377.0, 119.0, 189.0, 138.0, 221.0, 173.0, 187.0, 332.0, 137.0, 223.0, 355.0, 201.0, 281.0, 81.0, 332.0, 173.0, 164.0, 139.0, 384.0, 115.0, 289.0, 340.0, 205.0, 168.0, 263.0, 445.0, 168.0, 141.0, 216.0, 133.0, 241.0, 176.0, 129.0, 151.0, 339.0, 221.0, 121.0, 168.0, 191.0, 184.0, 180.0, 104.0, 221.0, 215.0, 87.0, 222.0, 168.0, 60.0, 230.0, 144.0, 314.0, 147.0, 156.0, 54.0, 183.0, 136.0, 262.0, 160.0, 131.0, 149.0, 189.0, 136.0, 132.0, 158.0, 165.0, 144.0, 189.0, 44.0, 138.0, 217.0, 205.0, 186.0, 220.0, 208.0, 172.0, 236.0, 262.0, 82.0, 115.0, 93.0, 257.0, 313.0, 278.0, 223.0, 123.0, 241.0, 346.0, 292.0, 233.0, 230.0, 186.0, 238.0, 239.0, 301.0, 263.0, 254.0, 378.0, 466.0, 315.0, 336.0, 262.0, 370.0, 475.0, 337.0, 283.0, 244.0, 304.0, 307.0, 300.0, 127.0, 273.0, 255.0, 473.0, 138.0, 354.0, 361.0, 399.0, 151.0, 379.0, 224.0, 313.0, 266.0, 283.0, 369.0, 385.0, 318.0, 264.0, 259.0, 281.0, 245.0, 344.0, 288.0, 305.0, 353.0, 348.0, 269.0, 413.0, 281.0, 396.0, 138.0, 278.0, 227.0, 325.0, 190.0, 353.0, 358.0, 237.0, 268.0, 258.0, 393.0, 341.0, 250.0, 108.0, 399.0, 432.0, 204.0, 290.0, 455.0, 119.0, 455.0], [26.0, 14.0, 24.0, 14.0, 12.0, 16.0, 18.0, 14.0, 29.0, 15.0, 20.0, 39.0, 31.0, 29.0, 24.0, 11.0, 45.0, 14.0, 22.0, 33.0, 14.0, 36.0, 64.0, 19.0, 81.0, 18.0, 19.0, 15.0, 31.0, 26.0, 51.0, 67.0, 58.0, 101.0, 37.0, 25.0, 44.0, 17.0, 24.0, 40.0, 23.0, 39.0, 29.0, 31.0, 31.0, 23.0, 30.0, 23.0, 79.0, 70.0, 23.0, 24.0, 12.0, 49.0, 27.0, 32.0, 38.0, 15.0, 15.0, 12.0, 59.0, 57.0, 39.0, 60.0, 29.0, 18.0, 54.0, 64.0, 29.0, 16.0, 86.0, 70.0, 48.0, 35.0, 74.0, 68.0, 36.0, 44.0, 59.0, 51.0, 30.0, 47.0, 29.0, 53.0, 101.0, 37.0, 29.0, 12.0, 52.0, 35.0, 24.0, 61.0, 78.0, 59.0, 53.0, 13.0, 40.0, 72.0, 70.0, 49.0, 20.0, 38.0, 23.0, 56.0, 57.0, 64.0, 105.0, 98.0, 31.0, 65.0, 84.0, 15.0, 10.0, 82.0, 24.0, 48.0, 84.0, 87.0, 140.0, 52.0, 51.0, 57.0, 66.0, 160.0, 107.0, 69.0, 95.0, 69.0, 97.0, 87.0, 11.0, 33.0, 26.0, 71.0, 121.0, 125.0, 115.0, 143.0, 65.0, 58.0, 120.0, 147.0, 34.0, 139.0, 42.0, 68.0, 91.0, 25.0, 95.0, 118.0, 68.0, 102.0, 69.0, 63.0, 11.0, 162.0, 110.0, 86.0, 109.0, 108.0, 14.0, 121.0, 140.0, 97.0, 139.0, 68.0, 316.0, 175.0, 270.0, 259.0, 143.0, 134.0, 90.0, 242.0, 78.0, 254.0, 105.0, 88.0, 88.0, 94.0, 97.0, 108.0, 62.0, 98.0, 75.0, 110.0, 41.0, 253.0, 137.0, 114.0, 72.0, 82.0, 31.0, 143.0, 130.0, 118.0, 114.0, 64.0, 134.0, 154.0, 116.0, 228.0, 117.0, 163.0, 152.0, 197.0, 339.0, 197.0, 254.0, 198.0, 224.0, 162.0, 306.0, 271.0, 244.0, 76.0, 310.0, 350.0, 125.0, 315.0, 166.0, 54.0, 452.0, 110.0, 26.0, 35.0, 47.0, 76.0, 20.0, 34.0, 54.0, 174.0, 241.0, 70.0, 120.0, 80.0, 13.0, 374.0, 436.0, 164.0, 117.0, 266.0, 208.0, 107.0, 380.0, 339.0, 228.0, 75.0, 413.0, 275.0, 176.0, 376.0, 204.0, 13.0, 377.0, 167.0, 64.0, 41.0, 127.0, 152.0, 82.0, 37.0, 101.0, 102.0, 153.0, 43.0, 170.0, 45.0, 185.0, 118.0, 136.0, 79.0, 117.0, 86.0, 159.0, 116.0, 89.0, 121.0, 101.0, 99.0, 103.0, 140.0, 127.0, 122.0, 101.0, 70.0, 107.0, 139.0, 37.0, 95.0, 103.0, 118.0, 114.0, 60.0, 134.0, 82.0, 64.0, 82.0, 102.0, 104.0, 132.0, 184.0, 137.0, 134.0, 99.0, 205.0, 80.0, 190.0, 41.0, 169.0, 134.0, 102.0, 152.0, 164.0, 178.0, 108.0, 78.0, 53.0, 184.0, 174.0, 200.0, 180.0, 163.0, 177.0, 164.0, 173.0, 117.0, 184.0, 142.0, 50.0, 145.0, 115.0, 135.0, 174.0, 218.0, 145.0, 188.0, 163.0, 26.0, 142.0, 134.0, 120.0, 247.0, 189.0, 139.0, 15.0, 227.0, 71.0, 219.0, 152.0, 21.0, 281.0, 285.0, 97.0, 96.0, 398.0, 58.0, 169.0, 385.0, 442.0, 161.0, 281.0, 357.0, 344.0, 283.0, 357.0, 303.0, 198.0, 441.0, 432.0, 297.0, 231.0, 232.0, 288.0, 307.0, 342.0, 188.0, 497.0, 224.0, 385.0, 212.0, 319.0, 261.0, 161.0, 102.0, 236.0, 161.0, 188.0, 52.0, 472.0, 165.0, 292.0, 293.0, 324.0, 219.0, 403.0, 93.0, 311.0, 354.0, 423.0, 157.0, 181.0, 304.0, 437.0, 100.0, 392.0, 293.0, 330.0, 431.0, 488.0, 323.0, 317.0, 164.0, 485.0, 218.0, 338.0, 181.0, 306.0, 185.0, 157.0, 106.0, 170.0, 143.0, 140.0, 126.0, 191.0, 168.0, 181.0, 204.0, 145.0, 101.0, 176.0, 235.0, 95.0, 123.0, 164.0, 136.0, 213.0, 120.0, 183.0, 157.0, 132.0, 203.0, 137.0, 154.0, 150.0, 325.0, 118.0, 274.0, 171.0, 158.0, 77.0, 251.0, 188.0, 138.0, 343.0, 156.0, 45.0, 219.0, 247.0, 111.0, 72.0, 124.0, 153.0, 170.0, 223.0, 198.0, 289.0, 423.0, 270.0, 318.0, 467.0, 456.0, 389.0, 430.0, 176.0, 390.0, 428.0, 132.0, 252.0, 329.0, 423.0, 411.0, 112.0, 348.0, 320.0, 356.0, 340.0, 432.0, 149.0, 417.0, 121.0, 346.0, 336.0, 485.0, 353.0, 235.0, 430.0, 443.0, 445.0, 379.0, 487.0, 329.0, 240.0, 202.0, 168.0, 454.0, 57.0, 366.0, 268.0, 282.0, 333.0, 159.0, 322.0, 214.0, 339.0, 224.0, 339.0, 214.0, 281.0, 224.0, 338.0, 94.0, 423.0, 313.0, 286.0, 228.0, 265.0, 468.0, 317.0, 123.0, 209.0, 163.0, 77.0, 76.0, 153.0, 104.0, 135.0, 72.0, 66.0, 50.0, 18.0, 126.0, 91.0, 106.0, 100.0, 76.0, 132.0, 96.0, 10.0, 42.0, 171.0, 162.0, 103.0, 115.0, 96.0, 127.0, 45.0, 93.0, 113.0, 96.0, 14.0, 102.0, 164.0, 103.0, 149.0, 141.0, 58.0, 94.0, 141.0, 73.0, 74.0, 20.0, 129.0, 63.0, 32.0, 97.0, 233.0, 31.0, 90.0, 130.0, 123.0, 147.0, 112.0, 47.0, 93.0, 41.0, 67.0, 127.0, 116.0, 90.0, 115.0, 56.0, 190.0, 77.0, 175.0, 134.0, 117.0, 95.0, 159.0, 264.0, 106.0, 67.0, 158.0, 269.0, 123.0, 88.0, 66.0, 93.0, 55.0, 159.0, 321.0, 226.0, 120.0, 269.0, 164.0, 118.0, 115.0, 141.0, 179.0, 324.0, 167.0, 93.0, 199.0, 171.0, 172.0, 160.0, 184.0, 196.0, 113.0, 167.0, 194.0, 90.0, 180.0, 150.0, 126.0, 117.0, 215.0, 102.0, 171.0, 148.0, 164.0, 199.0, 191.0, 138.0, 187.0, 322.0, 258.0, 125.0, 167.0, 206.0, 121.0, 256.0, 252.0, 116.0, 309.0, 189.0, 121.0, 46.0, 176.0, 312.0, 50.0, 222.0, 157.0, 175.0, 291.0, 456.0, 200.0, 42.0, 385.0, 152.0, 20.0, 140.0, 66.0, 235.0, 314.0, 317.0, 137.0, 50.0, 253.0, 231.0, 314.0, 80.0, 179.0, 237.0, 149.0, 152.0, 136.0, 144.0, 256.0, 176.0, 134.0, 164.0, 127.0, 132.0, 38.0, 97.0, 102.0, 114.0, 188.0, 62.0, 16.0, 101.0, 17.0, 89.0, 28.0, 98.0, 32.0, 45.0, 25.0, 107.0, 93.0, 31.0, 91.0, 26.0, 115.0, 118.0, 97.0, 98.0, 99.0, 76.0, 82.0, 45.0, 102.0, 50.0, 145.0, 55.0, 52.0, 23.0, 35.0, 44.0, 39.0, 40.0, 39.0, 23.0, 40.0, 49.0, 113.0, 139.0, 63.0, 132.0, 23.0, 80.0, 258.0, 86.0, 104.0, 117.0, 152.0, 78.0, 124.0, 94.0, 139.0, 109.0, 121.0, 127.0, 134.0, 82.0, 144.0, 190.0, 141.0, 134.0, 145.0, 174.0, 141.0, 80.0, 90.0, 77.0, 83.0, 157.0, 60.0, 78.0, 64.0, 71.0, 84.0, 87.0, 63.0, 154.0, 119.0, 271.0, 275.0, 136.0, 134.0, 26.0, 221.0, 89.0, 46.0, 105.0, 53.0, 56.0, 134.0, 35.0, 44.0, 67.0, 202.0, 75.0, 93.0, 115.0, 65.0, 46.0, 136.0, 47.0, 67.0, 118.0, 94.0, 65.0, 89.0, 64.0, 63.0, 103.0, 163.0, 99.0, 35.0, 38.0, 80.0, 78.0, 36.0, 79.0, 64.0, 30.0, 113.0, 68.0, 78.0, 119.0, 82.0, 43.0, 138.0, 62.0, 72.0, 135.0, 92.0, 109.0, 71.0, 32.0, 118.0, 91.0, 40.0, 99.0, 119.0, 94.0, 106.0, 113.0, 65.0, 134.0, 67.0, 87.0, 115.0, 28.0, 123.0, 131.0, 100.0, 159.0, 119.0, 164.0, 74.0, 74.0, 31.0, 46.0, 84.0, 35.0, 152.0, 124.0, 165.0, 14.0, 129.0, 116.0, 16.0, 24.0, 16.0, 23.0, 59.0, 69.0, 112.0, 95.0, 82.0, 138.0, 87.0, 63.0, 93.0, 40.0, 48.0, 10.0, 74.0, 108.0, 74.0, 104.0, 119.0, 94.0, 108.0, 80.0, 84.0, 96.0, 97.0, 105.0, 14.0, 115.0, 129.0, 127.0, 81.0, 91.0, 74.0, 74.0, 154.0, 181.0, 142.0, 67.0, 105.0, 80.0, 109.0, 104.0, 102.0, 137.0, 123.0, 46.0, 173.0, 139.0, 121.0, 52.0, 55.0, 139.0, 36.0, 28.0, 46.0, 36.0, 126.0, 117.0, 125.0, 80.0, 211.0, 75.0, 237.0, 199.0, 128.0, 180.0, 152.0, 217.0, 205.0, 126.0, 188.0, 116.0, 156.0, 242.0, 296.0, 59.0, 182.0, 114.0, 12.0, 238.0, 241.0, 325.0, 97.0, 125.0, 27.0, 76.0, 138.0, 286.0, 102.0, 134.0, 155.0, 268.0, 338.0, 80.0, 217.0, 217.0, 193.0, 67.0, 111.0, 199.0, 365.0, 141.0]]\n"
     ]
    }
   ],
   "source": [
    "# Parallel runs\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return reinforce_update.scores\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 4      #set to 1 if data_reuploading is off\n",
    "    entanglement = \"all_to_all\"\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = torch.nn.init.normal_\n",
    "    #weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.ones_\n",
    "    measure = measure_expval_pairs\n",
    "    circuit = CircuitGenerator(n_qubits, \n",
    "                            n_layers,\n",
    "                            entanglement,\n",
    "                            shots,\n",
    "                            input_scaling,\n",
    "                            design,\n",
    "                            diff_method,\n",
    "                            weight_init,\n",
    "                            input_init,\n",
    "                            measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling, beta, increase_rate = True, 0.5, 0.003\n",
    "    output_scaling, output_init = False, torch.nn.init.ones_\n",
    "    policy_type = PolicyType(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, \n",
    "                            increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "\n",
    "    lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    circuit_params = list(circuit.parameters())\n",
    "    policy_params = list(policy_type.parameters())\n",
    "    params = circuit_params + policy_params\n",
    "    optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, i, rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel runs\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return reinforce_update.scores\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 4      #set to 1 if data_reuploading is off\n",
    "    entanglement = \"all_to_all\"\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = torch.nn.init.normal_\n",
    "    #weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.ones_\n",
    "    measure = measure_expval_pairs\n",
    "    circuit = CircuitGenerator(n_qubits, \n",
    "                            n_layers,\n",
    "                            entanglement,\n",
    "                            shots,\n",
    "                            input_scaling,\n",
    "                            design,\n",
    "                            diff_method,\n",
    "                            weight_init,\n",
    "                            input_init,\n",
    "                            measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling, beta, increase_rate = True, 1, 0.003\n",
    "    output_scaling, output_init = False, torch.nn.init.ones_\n",
    "    policy_type = PolicyType(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, \n",
    "                            increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "\n",
    "    lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    circuit_params = list(circuit.parameters())\n",
    "    policy_params = list(policy_type.parameters())\n",
    "    params = circuit_params + policy_params\n",
    "    optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, i, rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel runs\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return reinforce_update.scores\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 4      #set to 1 if data_reuploading is off\n",
    "    entanglement = \"all_to_all\"\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = torch.nn.init.normal_\n",
    "    #weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.ones_\n",
    "    measure = measure_expval_pairs\n",
    "    circuit = CircuitGenerator(n_qubits, \n",
    "                            n_layers,\n",
    "                            entanglement,\n",
    "                            shots,\n",
    "                            input_scaling,\n",
    "                            design,\n",
    "                            diff_method,\n",
    "                            weight_init,\n",
    "                            input_init,\n",
    "                            measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling, beta, increase_rate = True, 3, 0.003\n",
    "    output_scaling, output_init = False, torch.nn.init.ones_\n",
    "    policy_type = PolicyType(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, \n",
    "                            increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "\n",
    "    lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    circuit_params = list(circuit.parameters())\n",
    "    policy_params = list(policy_type.parameters())\n",
    "    params = circuit_params + policy_params\n",
    "    optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 1\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, i, rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel runs\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return reinforce_update.scores\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 4      #set to 1 if data_reuploading is off\n",
    "    entanglement = \"all_to_all\"\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = torch.nn.init.normal_\n",
    "    #weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.ones_\n",
    "    measure = measure_expval_pairs\n",
    "    circuit = CircuitGenerator(n_qubits, \n",
    "                            n_layers,\n",
    "                            entanglement,\n",
    "                            shots,\n",
    "                            input_scaling,\n",
    "                            design,\n",
    "                            diff_method,\n",
    "                            weight_init,\n",
    "                            input_init,\n",
    "                            measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling, beta, increase_rate = True, 5, 0.003\n",
    "    output_scaling, output_init = False, torch.nn.init.ones_\n",
    "    policy_type = PolicyType(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, \n",
    "                            increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "\n",
    "    lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    circuit_params = list(circuit.parameters())\n",
    "    policy_params = list(policy_type.parameters())\n",
    "    params = circuit_params + policy_params\n",
    "    optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 1\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, i, rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel runs\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return reinforce_update.scores\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 4      #set to 1 if data_reuploading is off\n",
    "    entanglement = \"all_to_all\"\n",
    "    shots = None\n",
    "    input_scaling = False\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = torch.nn.init.normal_\n",
    "    #weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.ones_\n",
    "    measure = measure_expval_pairs\n",
    "    circuit = CircuitGenerator(n_qubits, \n",
    "                            n_layers,\n",
    "                            entanglement,\n",
    "                            shots,\n",
    "                            input_scaling,\n",
    "                            design,\n",
    "                            diff_method,\n",
    "                            weight_init,\n",
    "                            input_init,\n",
    "                            measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling, beta, increase_rate = False, 1, 0.003\n",
    "    output_scaling, output_init = True, torch.nn.init.ones_\n",
    "    policy_type = PolicyType(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, \n",
    "                            increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "\n",
    "    lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    circuit_params = list(circuit.parameters())\n",
    "    policy_params = list(policy_type.parameters())\n",
    "    params = circuit_params + policy_params\n",
    "    optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 1\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, i, rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel runs\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return reinforce_update.scores\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 4      #set to 1 if data_reuploading is off\n",
    "    entanglement = \"all_to_all\"\n",
    "    shots = None\n",
    "    input_scaling = False\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = torch.nn.init.normal_\n",
    "    #weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.ones_\n",
    "    measure = measure_expval_pairs\n",
    "    circuit = CircuitGenerator(n_qubits, \n",
    "                            n_layers,\n",
    "                            entanglement,\n",
    "                            shots,\n",
    "                            input_scaling,\n",
    "                            design,\n",
    "                            diff_method,\n",
    "                            weight_init,\n",
    "                            input_init,\n",
    "                            measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling, beta, increase_rate = False, 1, 0.003\n",
    "    output_scaling, output_init = False, torch.nn.init.ones_\n",
    "    policy_type = PolicyType(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, \n",
    "                            increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "\n",
    "    lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    circuit_params = list(circuit.parameters())\n",
    "    policy_params = list(policy_type.parameters())\n",
    "    params = circuit_params + policy_params\n",
    "    optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 1\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, i, rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel runs\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return reinforce_update.scores\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 4      #set to 1 if data_reuploading is off\n",
    "    entanglement = \"all_to_all\"\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = torch.nn.init.normal_\n",
    "    #weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.ones_\n",
    "    measure = measure_expval_pairs\n",
    "    circuit = CircuitGenerator(n_qubits, \n",
    "                            n_layers,\n",
    "                            entanglement,\n",
    "                            shots,\n",
    "                            input_scaling,\n",
    "                            design,\n",
    "                            diff_method,\n",
    "                            weight_init,\n",
    "                            input_init,\n",
    "                            measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling, beta, increase_rate = False, 1, 0.003\n",
    "    output_scaling, output_init = False, torch.nn.init.ones_\n",
    "    policy_type = PolicyType(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, \n",
    "                            increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "\n",
    "    lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    circuit_params = list(circuit.parameters())\n",
    "    policy_params = list(policy_type.parameters())\n",
    "    params = circuit_params + policy_params\n",
    "    optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 1\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, i, rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel runs\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return reinforce_update.scores\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 4      #set to 1 if data_reuploading is off\n",
    "    entanglement = \"all_to_all\"\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = torch.nn.init.normal_\n",
    "    #weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.ones_\n",
    "    measure = measure_expval_pairs\n",
    "    circuit = CircuitGenerator(n_qubits, \n",
    "                            n_layers,\n",
    "                            entanglement,\n",
    "                            shots,\n",
    "                            input_scaling,\n",
    "                            design,\n",
    "                            diff_method,\n",
    "                            weight_init,\n",
    "                            input_init,\n",
    "                            measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling, beta, increase_rate = False, 1, 0.003\n",
    "    output_scaling, output_init = True, torch.nn.init.ones_\n",
    "    policy_type = PolicyType(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, \n",
    "                            increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "\n",
    "    lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    circuit_params = list(circuit.parameters())\n",
    "    policy_params = list(policy_type.parameters())\n",
    "    params = circuit_params + policy_params\n",
    "    optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 1\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, i, rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel runs\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return reinforce_update.scores\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 1      #set to 1 if data_reuploading is off\n",
    "    entanglement = \"all_to_all\"\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = torch.nn.init.normal_\n",
    "    #weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.ones_\n",
    "    measure = measure_expval_pairs\n",
    "    circuit = CircuitGenerator(n_qubits, \n",
    "                            n_layers,\n",
    "                            entanglement,\n",
    "                            shots,\n",
    "                            input_scaling,\n",
    "                            design,\n",
    "                            diff_method,\n",
    "                            weight_init,\n",
    "                            input_init,\n",
    "                            measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'raw_parity'\n",
    "    beta_scheduling, beta, increase_rate = False, 0.5, 0.003\n",
    "    output_scaling, output_init = False, torch.nn.init.ones_\n",
    "    policy_type = PolicyType(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, \n",
    "                            increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "\n",
    "    lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    circuit_params = list(circuit.parameters())\n",
    "    policy_params = list(policy_type.parameters())\n",
    "    params = circuit_params + policy_params\n",
    "    optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, i, rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel runs\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return reinforce_update.scores\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 2      #set to 1 if data_reuploading is off\n",
    "    entanglement = \"all_to_all\"\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = torch.nn.init.normal_\n",
    "    #weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.ones_\n",
    "    measure = measure_expval_pairs\n",
    "    circuit = CircuitGenerator(n_qubits, \n",
    "                            n_layers,\n",
    "                            entanglement,\n",
    "                            shots,\n",
    "                            input_scaling,\n",
    "                            design,\n",
    "                            diff_method,\n",
    "                            weight_init,\n",
    "                            input_init,\n",
    "                            measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'raw_parity'\n",
    "    beta_scheduling, beta, increase_rate = False, 0.5, 0.003\n",
    "    output_scaling, output_init = False, torch.nn.init.ones_\n",
    "    policy_type = PolicyType(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, \n",
    "                            increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "\n",
    "    lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    circuit_params = list(circuit.parameters())\n",
    "    policy_params = list(policy_type.parameters())\n",
    "    params = circuit_params + policy_params\n",
    "    optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, i, rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel runs\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return reinforce_update.scores\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 3      #set to 1 if data_reuploading is off\n",
    "    entanglement = \"all_to_all\"\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = torch.nn.init.normal_\n",
    "    #weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.ones_\n",
    "    measure = measure_expval_pairs\n",
    "    circuit = CircuitGenerator(n_qubits, \n",
    "                            n_layers,\n",
    "                            entanglement,\n",
    "                            shots,\n",
    "                            input_scaling,\n",
    "                            design,\n",
    "                            diff_method,\n",
    "                            weight_init,\n",
    "                            input_init,\n",
    "                            measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'raw_parity'\n",
    "    beta_scheduling, beta, increase_rate = False, 0.5, 0.003\n",
    "    output_scaling, output_init = False, torch.nn.init.ones_\n",
    "    policy_type = PolicyType(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, \n",
    "                            increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "\n",
    "    lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    circuit_params = list(circuit.parameters())\n",
    "    policy_params = list(policy_type.parameters())\n",
    "    params = circuit_params + policy_params\n",
    "    optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, i, rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel runs\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return reinforce_update.scores\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 4      #set to 1 if data_reuploading is off\n",
    "    entanglement = \"all_to_all\"\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = torch.nn.init.normal_\n",
    "    #weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.ones_\n",
    "    measure = measure_expval_pairs\n",
    "    circuit = CircuitGenerator(n_qubits, \n",
    "                            n_layers,\n",
    "                            entanglement,\n",
    "                            shots,\n",
    "                            input_scaling,\n",
    "                            design,\n",
    "                            diff_method,\n",
    "                            weight_init,\n",
    "                            input_init,\n",
    "                            measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'raw_parity'\n",
    "    beta_scheduling, beta, increase_rate = False, 0.5, 0.003\n",
    "    output_scaling, output_init = False, torch.nn.init.ones_\n",
    "    policy_type = PolicyType(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, \n",
    "                            increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "\n",
    "    lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    circuit_params = list(circuit.parameters())\n",
    "    policy_params = list(policy_type.parameters())\n",
    "    params = circuit_params + policy_params\n",
    "    optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, i, rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel runs\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return reinforce_update.scores\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 5      #set to 1 if data_reuploading is off\n",
    "    entanglement = \"all_to_all\"\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = torch.nn.init.normal_\n",
    "    #weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.ones_\n",
    "    measure = measure_expval_pairs\n",
    "    circuit = CircuitGenerator(n_qubits, \n",
    "                            n_layers,\n",
    "                            entanglement,\n",
    "                            shots,\n",
    "                            input_scaling,\n",
    "                            design,\n",
    "                            diff_method,\n",
    "                            weight_init,\n",
    "                            input_init,\n",
    "                            measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'raw_parity'\n",
    "    beta_scheduling, beta, increase_rate = False, 0.5, 0.003\n",
    "    output_scaling, output_init = False, torch.nn.init.ones_\n",
    "    policy_type = PolicyType(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, \n",
    "                            increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "\n",
    "    lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    circuit_params = list(circuit.parameters())\n",
    "    policy_params = list(policy_type.parameters())\n",
    "    params = circuit_params + policy_params\n",
    "    optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, i, rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contiguous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel runs\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return reinforce_update.scores\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 1      #set to 1 if data_reuploading is off\n",
    "    entanglement = \"all_to_all\"\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = torch.nn.init.normal_\n",
    "    #weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.ones_\n",
    "    measure = measure_expval_pairs\n",
    "    circuit = CircuitGenerator(n_qubits, \n",
    "                            n_layers,\n",
    "                            entanglement,\n",
    "                            shots,\n",
    "                            input_scaling,\n",
    "                            design,\n",
    "                            diff_method,\n",
    "                            weight_init,\n",
    "                            input_init,\n",
    "                            measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'raw_contiguous'\n",
    "    beta_scheduling, beta, increase_rate = False, 0.5, 0.003\n",
    "    output_scaling, output_init = False, torch.nn.init.ones_\n",
    "    policy_type = PolicyType(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, \n",
    "                            increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "\n",
    "    lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    circuit_params = list(circuit.parameters())\n",
    "    policy_params = list(policy_type.parameters())\n",
    "    params = circuit_params + policy_params\n",
    "    optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, i, rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel runs\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return reinforce_update.scores\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 2      #set to 1 if data_reuploading is off\n",
    "    entanglement = \"all_to_all\"\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = torch.nn.init.normal_\n",
    "    #weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.ones_\n",
    "    measure = measure_expval_pairs\n",
    "    circuit = CircuitGenerator(n_qubits, \n",
    "                            n_layers,\n",
    "                            entanglement,\n",
    "                            shots,\n",
    "                            input_scaling,\n",
    "                            design,\n",
    "                            diff_method,\n",
    "                            weight_init,\n",
    "                            input_init,\n",
    "                            measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'raw_contiguous'\n",
    "    beta_scheduling, beta, increase_rate = False, 0.5, 0.003\n",
    "    output_scaling, output_init = False, torch.nn.init.ones_\n",
    "    policy_type = PolicyType(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, \n",
    "                            increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "\n",
    "    lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    circuit_params = list(circuit.parameters())\n",
    "    policy_params = list(policy_type.parameters())\n",
    "    params = circuit_params + policy_params\n",
    "    optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, i, rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel runs\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return reinforce_update.scores\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 3      #set to 1 if data_reuploading is off\n",
    "    entanglement = \"all_to_all\"\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = torch.nn.init.normal_\n",
    "    #weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.ones_\n",
    "    measure = measure_expval_pairs\n",
    "    circuit = CircuitGenerator(n_qubits, \n",
    "                            n_layers,\n",
    "                            entanglement,\n",
    "                            shots,\n",
    "                            input_scaling,\n",
    "                            design,\n",
    "                            diff_method,\n",
    "                            weight_init,\n",
    "                            input_init,\n",
    "                            measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'raw_contiguous'\n",
    "    beta_scheduling, beta, increase_rate = False, 0.5, 0.003\n",
    "    output_scaling, output_init = False, torch.nn.init.ones_\n",
    "    policy_type = PolicyType(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, \n",
    "                            increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "\n",
    "    lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    circuit_params = list(circuit.parameters())\n",
    "    policy_params = list(policy_type.parameters())\n",
    "    params = circuit_params + policy_params\n",
    "    optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, i, rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel runs\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return reinforce_update.scores\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 4      #set to 1 if data_reuploading is off\n",
    "    entanglement = \"all_to_all\"\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = torch.nn.init.normal_\n",
    "    #weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.ones_\n",
    "    measure = measure_expval_pairs\n",
    "    circuit = CircuitGenerator(n_qubits, \n",
    "                            n_layers,\n",
    "                            entanglement,\n",
    "                            shots,\n",
    "                            input_scaling,\n",
    "                            design,\n",
    "                            diff_method,\n",
    "                            weight_init,\n",
    "                            input_init,\n",
    "                            measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'raw_contiguous'\n",
    "    beta_scheduling, beta, increase_rate = False, 0.5, 0.003\n",
    "    output_scaling, output_init = False, torch.nn.init.ones_\n",
    "    policy_type = PolicyType(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, \n",
    "                            increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "\n",
    "    lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    circuit_params = list(circuit.parameters())\n",
    "    policy_params = list(policy_type.parameters())\n",
    "    params = circuit_params + policy_params\n",
    "    optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, i, rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel runs\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return reinforce_update.scores\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 5      #set to 1 if data_reuploading is off\n",
    "    entanglement = \"all_to_all\"\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = torch.nn.init.normal_\n",
    "    #weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.ones_\n",
    "    measure = measure_expval_pairs\n",
    "    circuit = CircuitGenerator(n_qubits, \n",
    "                            n_layers,\n",
    "                            entanglement,\n",
    "                            shots,\n",
    "                            input_scaling,\n",
    "                            design,\n",
    "                            diff_method,\n",
    "                            weight_init,\n",
    "                            input_init,\n",
    "                            measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'raw_contiguous'\n",
    "    beta_scheduling, beta, increase_rate = False, 0.5, 0.003\n",
    "    output_scaling, output_init = False, torch.nn.init.ones_\n",
    "    policy_type = PolicyType(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, \n",
    "                            increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "\n",
    "    lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    circuit_params = list(circuit.parameters())\n",
    "    policy_params = list(policy_type.parameters())\n",
    "    params = circuit_params + policy_params\n",
    "    optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, i, rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel runs\n",
    "import optuna\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return reinforce_update.scores\n",
    "\n",
    "def objective_function(results):\n",
    "\n",
    "    results_mean = np.mean(results, axis=0)\n",
    "    area = np.abs(np.trapz(results_mean))\n",
    "    maximum_performance_area = float(len(results[0]) * 200)\n",
    "\n",
    "    # Create a metric called performance area and normalize it between 0 and 1\n",
    "    performance_area = area / maximum_performance_area\n",
    "    return performance_area\n",
    "\n",
    "'''\n",
    "def sum_and_average(results):\n",
    "\n",
    "    averages = np.mean(results, axis=1)\n",
    "    return np.mean(averages)\n",
    "'''\n",
    "\n",
    "def objective(trial):    \n",
    "\n",
    "    n_qubits = 4\n",
    "    n_layers = 4      #set to 1 if data_reuploading is off\n",
    "    entanglement = \"all_to_all\"\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = torch.nn.init.normal_\n",
    "    #weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.ones_\n",
    "    measure = measure_expval_pairs\n",
    "    circuit = CircuitGenerator(n_qubits, \n",
    "                            n_layers,\n",
    "                            entanglement,\n",
    "                            shots,\n",
    "                            input_scaling,\n",
    "                            design,\n",
    "                            diff_method,\n",
    "                            weight_init,\n",
    "                            input_init,\n",
    "                            measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling, beta, increase_rate = True, 1, 0.005\n",
    "    output_scaling, output_init = False, torch.nn.init.ones_\n",
    "    policy_type = PolicyType(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, \n",
    "                            increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "\n",
    "    lr1 = trial.suggest_float(\"lr1\", 1e-5, 1e-1, log=True)\n",
    "    #lr2 = trial.suggest_float(\"lr2\", 1e-5, 1e-1, log=True)\n",
    "    #lr3 = trial.suggest_float(\"lr3\", 1e-5, 1e-1, log=True)\n",
    "    lr2, lr3 = 0.1, 0.1\n",
    "    lr_list= [lr1, lr2, lr3]  # [weights, input_weights, output_weights]\n",
    "    circuit_params = list(circuit.parameters())\n",
    "    policy_params = list(policy_type.parameters())\n",
    "    params = circuit_params + policy_params\n",
    "    optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 10\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 1\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 5\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, i, trial.number) for i in range(num_agents))\n",
    "    performance_metric = objective_function(results)\n",
    "    #performance_metric = sum_and_average(results)\n",
    "    return performance_metric\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    print(f\"Best trial: {study.best_trial.value}\")\n",
    "    print(f\"Best parameters: {study.best_trial.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0832, 0.9168])\n"
     ]
    }
   ],
   "source": [
    "softmax_output = F.softmax(torch.Tensor([0.1,0.9]) * 3, dim=0)\n",
    "print(softmax_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
