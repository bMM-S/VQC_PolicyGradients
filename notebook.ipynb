{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pennylane as qml \n",
    "import torch\n",
    "\n",
    "def create_circuit(n_qubits,n_layers=None,circ = \"simplified_two_design\",fim=False, shots=None):\n",
    "\n",
    "    dev = qml.device(\"default.qubit.torch\", wires=n_qubits, shots=shots)\n",
    "\n",
    "    def RZRY(params):\n",
    "        #qml.SpecialUnitary(params, wires=range(n_qubits))\n",
    "        #qml.SimplifiedTwoDesign(initial_layer_weights=init_params, weights=params, wires=range(n_qubits))\n",
    "        #qml.AngleEmbedding(params,wires=range(n_qubits))\n",
    "        for q in range(n_qubits):\n",
    "            qml.Hadamard(wires=q)\n",
    "\n",
    "        for w in range(n_layers): \n",
    "            for q in range(n_qubits):\n",
    "                index = w * (2*n_qubits) + q * 2\n",
    "                qml.RZ(params[index],wires=q)\n",
    "                qml.RY(params[index + 1],wires=q)\n",
    "        \n",
    "        qml.broadcast(qml.CNOT , wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "        \n",
    "        return qml.probs(wires=range(n_qubits))\n",
    "\n",
    "    def S2D(init_params,params,measurement_qubits=0,prod_approx=False):\n",
    "        #qml.SpecialUnitary(params, wires=range(n_qubits))\n",
    "        qml.SimplifiedTwoDesign(initial_layer_weights=init_params, weights=params, wires=range(n_qubits))\n",
    "        \n",
    "        #qml.broadcast(qml.CNOT , wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "        if not prod_approx:\n",
    "            return qml.probs(wires=list(range(measurement_qubits)))\n",
    "        else:\n",
    "            return [qml.probs(i) for i in range(measurement_qubits)]\n",
    "\n",
    "    def SU(params):\n",
    "        qml.SpecialUnitary(params, wires=range(n_qubits))\n",
    "        \n",
    "        ZZ = qml.operation.Tensor(qml.PauliZ(0), qml.PauliZ(1))\n",
    "        for i in range(2,n_qubits):\n",
    "            ZZ = qml.operation.Tensor(ZZ, qml.PauliZ(i))\n",
    "\n",
    "        return qml.expval(ZZ)\n",
    "    \n",
    "    def simmpleRZRY(params,cnots=True):\n",
    "        qml.broadcast(qml.Hadamard, wires=range(n_qubits), pattern=\"single\")\n",
    "        qml.broadcast(qml.RZ, wires=range(n_qubits), pattern=\"single\", parameters=params[0])\n",
    "        qml.broadcast(qml.RY, wires=range(n_qubits), pattern=\"single\", parameters=params[1])\n",
    "        if cnots:\n",
    "            qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"chain\")\n",
    "\n",
    "            return qml.expval(qml.PauliZ(n_qubits-1))\n",
    "        else:\n",
    "            ZZ = qml.operation.Tensor(qml.PauliZ(0), qml.PauliZ(1))\n",
    "            for i in range(2,n_qubits):\n",
    "                ZZ = qml.operation.Tensor(ZZ, qml.PauliZ(i))\n",
    "\n",
    "            return qml.expval(ZZ)\n",
    "        \n",
    "    def RY(params,y=True,probs=False,prod=False, entanglement=None):\n",
    "        #qml.broadcast(qml.Hadamard, wires=range(n_qubits), pattern=\"single\")\n",
    "        qml.broadcast(qml.RY, wires=range(n_qubits), pattern=\"single\", parameters=params)\n",
    "        #qml.broadcast(qml.CZ, wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "\n",
    "        if entanglement==\"all_to_all\":\n",
    "            qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "        \n",
    "        if y==True:\n",
    "            #YY = qml.operation.Tensor(qml.PauliY(0), qml.PauliY(1))\n",
    "            YY = [qml.PauliZ(0), qml.PauliZ(1)]\n",
    "            for i in range(2,n_qubits):\n",
    "                #YY = qml.operation.Tensor(YY, qml.PauliY(i))\n",
    "                YY.append(qml.PauliZ(i))\n",
    "            \n",
    "            #return [qml.expval(i) for i in YY]\n",
    "            return qml.expval(YY)\n",
    "\n",
    "        elif probs==False:\n",
    "\n",
    "            ZZ = qml.operation.Tensor(qml.PauliZ(0), qml.PauliZ(1))\n",
    "            #ZZ = [qml.PauliZ(0), qml.PauliZ(1)]\n",
    "            for i in range(2,n_qubits):\n",
    "                ZZ = qml.operation.Tensor(ZZ, qml.PauliZ(i))        \n",
    "                #ZZ.append(qml.PauliZ(i))        \n",
    "\n",
    "            #return [qml.expval(i) for i in ZZ]\n",
    "            return qml.expval(ZZ)\n",
    "\n",
    "        else:\n",
    "            if prod:\n",
    "                return [qml.probs(i) for i in range(n_qubits)]\n",
    "            else:\n",
    "                return qml.probs(wires=range(n_qubits))\n",
    "            \n",
    "        \n",
    "        \n",
    "    def GHZ(params,measurement_qubits=0):\n",
    "        qml.RY(params,wires=0)\n",
    "        qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"chain\")\n",
    "\n",
    "        return qml.probs(wires=range(measurement_qubits))\n",
    "\n",
    "    def random_product_state(params,gate_sequence=None):\n",
    "                \n",
    "        for i in range(n_qubits):\n",
    "            qml.RY(np.pi / 4, wires=i)\n",
    "\n",
    "        for ll in range(len(params)):\n",
    "\n",
    "            for i in range(n_qubits):\n",
    "                gate_sequence[\"{}{}\".format(ll,i)](params[ll][i], wires=i)\n",
    "\n",
    "            #for i in range(n_qubits - 1):\n",
    "                #qml.CZ(wires=[i, i + 1])\n",
    "    def SEL(params, measurement_qubits=0):\n",
    "        qml.StronglyEntanglingLayers(params, wires=range(n_qubits))\n",
    "        return qml.probs(wires=range(measurement_qubits))\n",
    "    \n",
    "    def RL(params, measurement_qubits=0):\n",
    "        qml.RandomLayers(params, ratio_imprim=0.8 ,imprimitive=qml.CZ, wires=range(n_qubits))\n",
    "        return qml.probs(wires=range(measurement_qubits))\n",
    "    \n",
    "    if circ == \"rzry\":\n",
    "        qcircuit = RZRY\n",
    "    elif circ == \"simplified_two_design\":\n",
    "        qcircuit = S2D\n",
    "    elif circ == \"special_unitary\":\n",
    "        qcircuit = SU\n",
    "    elif circ == \"simpleRZRY\":\n",
    "        qcircuit = simmpleRZRY\n",
    "    elif circ == \"RY\":\n",
    "        qcircuit = RY\n",
    "    elif circ == \"ghz\":\n",
    "        qcircuit = GHZ\n",
    "    elif circ == \"random_product_state\":\n",
    "        qcircuit = random_product_state\n",
    "    elif circ == \"SEL\":\n",
    "        qcircuit = SEL\n",
    "    elif circ == \"RL\":\n",
    "        qcircuit = RL\n",
    "    if not fim:\n",
    "        circuit = qml.QNode(qcircuit, dev,interface=\"torch\", diff_method=\"backprop\")\n",
    "    else:\n",
    "        circuit = qml.QNode(qcircuit, dev)\n",
    "\n",
    "    return circuit\n",
    "\n",
    "def compute_gradient(log_prob, w):\n",
    "    \"\"\"Compute gradient of the log probability with respect to weights.\n",
    "    \n",
    "    Args:\n",
    "    - log_prob (torch.Tensor): The log probability tensor.\n",
    "    - w (torch.Tensor): The weights tensor, with requires_grad=True.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The gradient of log_prob with respect to w, flattened.\n",
    "    \"\"\"\n",
    "    if w.grad is not None:\n",
    "        w.grad.zero_()\n",
    "    log_prob.backward(retain_graph=True)\n",
    "    \n",
    "    if w.grad is None:\n",
    "        raise ValueError(\"The gradient for the given log_prob with respect to w is None.\")\n",
    "    \n",
    "    return w.grad.view(-1).detach().numpy()\n",
    "\n",
    "def policy(probs, policy_type=\"contiguous-like\", n_actions=2, n_qubits=1):\n",
    "\n",
    "    if policy_type == \"contiguous-like\":\n",
    "        return probs\n",
    "    elif policy_type == \"parity-like\":\n",
    "        policy = torch.zeros(n_actions)\n",
    "        for i in range(len(probs)):\n",
    "            a=[]\n",
    "            for m in range(int(np.log2(n_actions))):\n",
    "                if m==0:    \n",
    "                    bitstring = np.binary_repr(i,width=n_qubits)\n",
    "                else:\n",
    "                    bitstring = np.binary_repr(i,width=n_qubits)[:-m]\n",
    "                \n",
    "                a.append(bitstring.count(\"1\") % 2)\n",
    "            policy[int(\"\".join(str(x) for x in a),2)] += probs[i]\n",
    "\n",
    "        return policy    \n",
    "    \n",
    "def compute_policy_and_gradient(args):\n",
    "    n_qubits, shapes, type , n_actions, policy_type, clamp = args\n",
    "\n",
    "    if policy_type == \"parity-like\":\n",
    "        measure_qubits = n_qubits\n",
    "    else:\n",
    "        measure_qubits = int(np.log2(n_actions))\n",
    "\n",
    "    qc = create_circuit(n_qubits, circ=type, fim=False, shots=None)\n",
    "\n",
    "    if type == \"simplified_two_design\":\n",
    "        weights = [np.random.uniform(-np.pi,np.pi,size=shape) for shape in shapes]    \n",
    "        weights_tensor_init = torch.tensor(weights[0], requires_grad=False)\n",
    "        weights_tensor_params = torch.tensor(weights[1], requires_grad=True)\n",
    "        \n",
    "        probs = qc(weights_tensor_init,weights_tensor_params, measurement_qubits=measure_qubits)\n",
    "\n",
    "    else:\n",
    "        weights = [np.random.uniform(-np.pi,np.pi,size=shape) for shape in shapes]    \n",
    "        weights_tensor_params = torch.tensor(weights, requires_grad=True)\n",
    "\n",
    "        probs = qc(weights_tensor_params, measurement_qubits=measure_qubits)\n",
    "\n",
    "    pi = policy(probs, policy_type=policy_type, n_actions=n_actions, n_qubits=n_qubits)\n",
    "    if clamp is not None:\n",
    "        pi = torch.clamp(pi, clamp, 1)\n",
    "\n",
    "    dist = torch.distributions.Categorical(probs=pi)\n",
    "    \n",
    "    action = dist.sample()\n",
    "    log_prob = dist.log_prob(action)\n",
    "\n",
    "    gradient_no_clamp = np.linalg.norm(compute_gradient(log_prob, weights_tensor_params), 2)\n",
    "    return gradient_no_clamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(policy, optimizer, env, n_episodes=1000, max_t=1000, gamma=1.0, print_every=5):\n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "    scores = []\n",
    "    average_scores = []\n",
    "    runtime_sum = 0\n",
    "    for e in range(1, n_episodes):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        # Collect trajectory\n",
    "        for t in range(max_t):\n",
    "            # Sample the action from current policy\n",
    "            if t==0:\n",
    "                state_tensor = torch.tensor(state[0]).float()\n",
    "            else:\n",
    "                state_tensor = torch.tensor(state).float()\n",
    "            action, log_prob, _, = policy.sample(state_tensor)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    # Total expected reward\n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "    #standardized returns\n",
    "        R=0\n",
    "        policy_loss = []\n",
    "        returns = []\n",
    "        for r in rewards[::-1]:\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0,R)\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + np.finfo(np.float32).eps)\n",
    "\n",
    "        for log_prob, R in zip(saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "\n",
    "        policy_unsqueezed = [torch.unsqueeze(loss, 0) for loss in policy_loss]\n",
    "        policy_sum = torch.cat(policy_unsqueezed).sum()\n",
    "\n",
    "    # Backpropagation\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        policy_sum.backward()\n",
    "        optimizer.step()\n",
    "        end_time = time.time()\n",
    "        runtime = end_time-start_time\n",
    "        \n",
    "        runtime_sum += runtime\n",
    "        if e % print_every == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tLast {}\\tEpisodes average reward: {:.2f}\\tRuntime: {:.2f}'.format(e, scores_deque[-1], print_every, np.mean(scores_deque), runtime_sum))\n",
    "            runtime_sum = 0\n",
    "        if np.mean(scores_deque) == 500:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(e, np.mean(scores_deque)))\n",
    "            break\n",
    "    return scores, policy.gradient_list, average_scores  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def save_training_data(self):\n",
    "        ''' \n",
    "        Saves training data into json files\n",
    "        '''\n",
    "        current_directory = os.path.dirname(__file__)\n",
    "        folder_name = f\"{str(self.env_name)}_{self.pqc.policy.post_processing}_{self.pqc.circuit.n_layers}\"\n",
    "        folder_path = os.path.join(current_directory, folder_name)\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "        episode_data = [self.scores_deque, \n",
    "                        self.runtime, \n",
    "                        self.loss.item(), \n",
    "                        tensor_to_list(self.pqc.get_gradients()[0]), \n",
    "                        tensor_to_list(self.pqc.get_gradients()[1])]\n",
    "\n",
    "        if folder_path is not None:\n",
    "            file_path = os.path.join(self.folder_path, f\"{self.file_name}.json\")\n",
    "            if os.path.exists(file_path):\n",
    "                with open(file_path, 'r') as f:\n",
    "                    existing_data = json.load(f)\n",
    "                existing_data.append(episode_data)\n",
    "                with open(file_path, 'w') as f:\n",
    "                    json.dump(existing_data, f, indent=4)\n",
    "            else:\n",
    "                with open(file_path, 'w') as f:\n",
    "                    json.dump([episode_data], f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import gym\n",
    "from collections import deque\n",
    "\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir ../../data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "\n",
    "def tensor_to_list(tensor):\n",
    "    \"\"\"\n",
    "    Convert a tensor or numpy array to a nested list.\n",
    "    \"\"\"\n",
    "    if isinstance(tensor, list):\n",
    "        return [tensor_to_list(t) for t in tensor]\n",
    "    elif isinstance(tensor, dict):\n",
    "        return {key: tensor_to_list(value) for key, value in tensor.items()}\n",
    "    elif isinstance(tensor, np.ndarray):\n",
    "        return tensor.tolist()\n",
    "    elif isinstance(tensor, torch.Tensor):\n",
    "        return tensor.tolist()\n",
    "    else:\n",
    "        return tensor\n",
    "\n",
    "def measure_probs(qubits):\n",
    "    return qml.probs(wires=range(qubits)) \n",
    "\n",
    "def measure_expval_pairs(qubits):\n",
    "    expvals = []\n",
    "    for i in range(qubits // 2):\n",
    "        expvals.append(qml.expval(qml.PauliZ(2*i) @ qml.PauliZ(2*i + 1)))\n",
    "    return expvals\n",
    "    \n",
    "def create_optimizer_with_lr(params, lr_list, use_amsgrad=False):\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': p, 'lr': lr} for p, lr in zip(params, lr_list)\n",
    "    ], amsgrad=use_amsgrad)\n",
    "    return optimizer\n",
    "\n",
    "def get_function_representation(func):\n",
    "    if callable(func):\n",
    "        # Check if the function is a lambda\n",
    "        if func.__name__ == \"<lambda>\":\n",
    "            # Optionally, check if the function has a custom description attribute\n",
    "            return f\"{func.__module__}.<lambda>\" + (getattr(func, 'description', ''))\n",
    "        else:\n",
    "            return f\"{func.__module__}.{func.__name__}\"\n",
    "    return \"Unknown Function Type\"\n",
    "\n",
    "def jerbi_circuit(n_qubits, n_layers, shots, input_scaling, diff_method, weight_init, input_init, measure, measure_qubits):\n",
    "\n",
    "    if shots is None:\n",
    "        dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "    else:\n",
    "        dev = qml.device(\"default.qubit\", wires=n_qubits, shots=shots)\n",
    "    \n",
    "    if n_layers < 1:\n",
    "        raise ValueError(\"Number of layers can't take values below 1\")\n",
    "    \n",
    "    weight_shapes = {\"params\": (n_layers + 1, n_qubits, 2),\n",
    "                    \"input_params\": (n_layers, n_qubits, 2)}\n",
    "    init_method   = {\"params\": weight_init,\n",
    "                    \"input_params\": input_init}\n",
    "    \n",
    "    @qml.qnode(dev, interface='torch', diff_method=diff_method)\n",
    "    def qnode(inputs, params, input_params):\n",
    "    #in case n_qubits != input length\n",
    "        if n_qubits > len(inputs) and n_qubits % len(inputs) == 0:\n",
    "            multiplier = n_qubits // len(inputs)\n",
    "            inputs = torch.cat([inputs] * multiplier)\n",
    "        elif n_qubits != len(inputs) and n_qubits % len(inputs) != 0:\n",
    "            raise ValueError('Number of qubits cannot be divided by input lenght')\n",
    "\n",
    "    #hadamard\n",
    "        qml.broadcast(qml.Hadamard, wires=range(n_qubits), pattern=\"single\")\n",
    "        \n",
    "        for layer in range(n_layers):\n",
    "            for wire in range(n_qubits):\n",
    "                qml.RZ(params[layer][wire][0], wires=wire)\n",
    "                qml.RY(params[layer][wire][1], wires=wire)\n",
    "\n",
    "            qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"chain\")\n",
    "\n",
    "            if input_scaling:\n",
    "                for wire in range(n_qubits):\n",
    "                    qml.RY(input_params[layer][wire][0] * inputs[wire], wires=wire)\n",
    "                    qml.RZ(input_params[layer][wire][1] * inputs[wire], wires=wire)\n",
    "            else:\n",
    "                for wire in range(n_qubits):\n",
    "                    qml.RY(inputs[wire], wires=wire)\n",
    "                    qml.RZ(inputs[wire], wires=wire)\n",
    "\n",
    "        for wire in range(n_qubits):\n",
    "            qml.RZ(params[-1][wire][0], wires=wire)\n",
    "            qml.RY(params[-1][wire][1], wires=wire)\n",
    "            \n",
    "        return measure(measure_qubits)\n",
    "\n",
    "    model = qml.qnn.TorchLayer(qnode, weight_shapes=weight_shapes, init_method=init_method)  \n",
    "    \n",
    "    return model\n",
    "    \n",
    "def S2D(n_qubits, n_layers, shots, input_scaling, diff_method, weight_init, input_init, measure_type, observables, measure_qubits):\n",
    "\n",
    "    dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "    observables = observables if observables is not None else None\n",
    "    \n",
    "    shapes = qml.SimplifiedTwoDesign.shape(n_layers=n_layers, n_wires=n_qubits)\n",
    "\n",
    "    weight_shapes = {\"params\": shapes[1],\n",
    "                     \"input_params\": shapes[0]}\n",
    "    \n",
    "    init_method   = {\"params\": weight_init,\n",
    "                     \"input_params\": input_init}\n",
    "\n",
    "    @qml.qnode(dev, interface='torch', diff_method='parameter-shift')\n",
    "    def qnode(inputs, params, input_params):\n",
    "\n",
    "        return measure_selection(measure_type, observables, measure_qubits)\n",
    "\n",
    "    model = qml.qnn.TorchLayer(qnode, weight_shapes=weight_shapes, init_method=init_method)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircuitGenerator(nn.Module):\n",
    "\n",
    "    def __init__(self, n_qubits, n_layers,  shots = None, input_scaling = True,\n",
    "                design = 'jerbi_circuit', diff_method = 'backprop', weight_init = torch.nn.init.uniform_, \n",
    "                input_init = torch.nn.init.ones_, measure = None, measure_qubits = None):\n",
    "        super(CircuitGenerator, self).__init__()\n",
    "        '''\n",
    "\n",
    "        Creates a parameterized quantum circuit based on the arguments:\n",
    "\n",
    "            n_qubits(int) = Number of qubits\n",
    "            n_layers(int) = Number of layers (0 if no data re-uploading)\n",
    "            shots(int) = Number of times the circuit gets executed\n",
    "            input_scaling(bool) = Input parameters are used if True (input*input_params)\n",
    "            design(str) = The PQC ansatz design ('jerbi_circuit')\n",
    "            diff_method(str) = Differentiation method ('best', 'backprop', 'parameter-shift', ...)\n",
    "            weight_init (function) = How PQC weights are initialized (.uniform_, .ones_, ...)\n",
    "            input_init (function) = How input weights are initialized (.uniform_, .ones_, ...)\n",
    "            measure (function) = Measure function (measure_probs, measure_expval_pairs)\n",
    "            measure_qubits (int) = Number of qubits to be measured (in some cases might be equal to the number of qubits)\n",
    "            \n",
    "        '''\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.shots = shots\n",
    "        self.input_scaling = input_scaling\n",
    "        self.design = design\n",
    "        self.diff_method = diff_method\n",
    "        self.weight_init = weight_init\n",
    "        self.input_init = input_init\n",
    "        if measure is None:\n",
    "            self.measure = measure_probs\n",
    "        else:\n",
    "            self.measure = measure\n",
    "\n",
    "        if measure_qubits is None:\n",
    "            self.measure_qubits = n_qubits\n",
    "        else:\n",
    "            self.measure_qubits = measure_qubits\n",
    "\n",
    "        if self.design == 'jerbi_circuit':\n",
    "            self.circuit = jerbi_circuit(n_qubits = self.n_qubits,\n",
    "                                        n_layers = self.n_layers,\n",
    "                                        shots = self.shots,\n",
    "                                        input_scaling = self.input_scaling,\n",
    "                                        diff_method = self.diff_method,\n",
    "                                        weight_init = self.weight_init,\n",
    "                                        input_init = self.input_init,\n",
    "                                        measure = self.measure,\n",
    "                                        measure_qubits = self.measure_qubits)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported circuit type\")\n",
    "\n",
    "    def input(self,inputs):\n",
    "\n",
    "        outputs = self.circuit(inputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyType(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_actions, post_processing = 'raw_contiguous', \n",
    "                 beta_scheduling = False, beta = 1, increase_rate = 0.003, \n",
    "                 output_scaling = False, output_init = torch.nn.init.ones_):\n",
    "        super(PolicyType, self).__init__()\n",
    "\n",
    "        '''\n",
    "        Determines the type of policy used based on the arguments:\n",
    "\n",
    "            n_actions(int) = Number of actions\n",
    "            post_processing(str) = Type of policy ('raw_contiguous', 'raw_parity', 'softmax')\n",
    "            beta_scheduling(bool) = Inverse temperature parameter used in softmax (used if set to True)\n",
    "            beta(float) = Beta parameter or inverse temperature (used only for softmax)\n",
    "            increase_rate(float) = Amount added to beta at the end of each episode (used only for softmax)\n",
    "            output_scaling(bool) = Output parameters are used if True\n",
    "            output_init(function) = How the output parameters are initialized\n",
    "            \n",
    "        '''\n",
    "        self.n_actions = n_actions\n",
    "        self.post_processing = post_processing\n",
    "        self.beta_scheduling = beta_scheduling\n",
    "        self.beta = beta\n",
    "        self.increase_rate = increase_rate\n",
    "        self.output_scaling = output_scaling\n",
    "        self.output_init = output_init\n",
    "\n",
    "        if self.output_scaling == True:\n",
    "            self.output_params = nn.parameter.Parameter(torch.Tensor(self.n_actions), requires_grad=True)\n",
    "            self.output_init(self.output_params)\n",
    "        else:\n",
    "            self.register_parameter('w_input', None)\n",
    "\n",
    "\n",
    "    def input(self,probs):\n",
    "        if self.post_processing == 'raw_contiguous':\n",
    "            policy = self.raw_contiguous(probs)\n",
    "        elif self.post_processing == 'raw_parity':\n",
    "            policy = self.raw_parity(probs)\n",
    "        elif self.post_processing == 'softmax':\n",
    "            policy = self.softmax(probs)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid post-processing method specified.\")\n",
    "        return policy\n",
    "\n",
    "    def raw_contiguous(self,probs):\n",
    "        \n",
    "        probs_flatten = probs.flatten()\n",
    "        chunk_size = len(probs_flatten) // self.n_actions\n",
    "        remainder = len(probs_flatten) % self.n_actions\n",
    "\n",
    "        policy = []\n",
    "\n",
    "        for i in range(self.n_actions):\n",
    "            start = i * chunk_size\n",
    "            end = (i + 1) * chunk_size\n",
    "\n",
    "            if i < remainder:\n",
    "                end += 1\n",
    "\n",
    "            # Update the original policy list instead of creating a new one\n",
    "            policy.append(sum(probs_flatten[start:end]))\n",
    "\n",
    "        policy_tensor = torch.stack(policy)\n",
    "        return policy_tensor\n",
    "        \n",
    "    def raw_parity(self,probs):\n",
    "\n",
    "        if self.n_actions % 2 != 0:\n",
    "            raise ValueError('For parity-like policy, n_actions must be an even number')\n",
    "        \n",
    "        probs_flatten = probs.flatten()\n",
    "        policy = torch.zeros(self.n_actions)\n",
    "        counter = 0\n",
    "        for prob in probs_flatten:\n",
    "            policy[counter] += prob\n",
    "            counter += 1\n",
    "            if counter == self.n_actions:\n",
    "                counter = 0\n",
    "        \n",
    "        return policy\n",
    "    \n",
    "    def softmax(self, probs):\n",
    "        \n",
    "        if self.output_scaling == True:\n",
    "            probs *= self.output_params\n",
    "\n",
    "        if len(probs) == self.n_actions:\n",
    "            scaled_output = probs * self.beta\n",
    "            softmax_output = F.softmax(scaled_output, dim=0)\n",
    "            return softmax_output\n",
    "        else:\n",
    "            probs_flatten = probs.flatten()\n",
    "            chunk_size = len(probs_flatten) // self.n_actions\n",
    "            remainder = len(probs_flatten) % self.n_actions\n",
    "\n",
    "            policy = []\n",
    "\n",
    "            for i in range(self.n_actions):\n",
    "                start = i * chunk_size\n",
    "                end = (i + 1) * chunk_size\n",
    "\n",
    "                if i < remainder:\n",
    "                    end += 1\n",
    "\n",
    "                # Update the original policy list instead of creating a new one\n",
    "                policy.append(sum(probs_flatten[start:end]))\n",
    "            policy_tensor = torch.stack(policy)\n",
    "            softmax_output = F.softmax(policy_tensor * self.beta, dim=0)\n",
    "            return softmax_output\n",
    "        \n",
    "    def beta_schedule(self):\n",
    "        if self.beta_scheduling == True:\n",
    "            if self.post_processing == 'softmax':\n",
    "                self.beta += self.increase_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumPolicyModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, circuit, policy):\n",
    "        super(QuantumPolicyModel, self).__init__()\n",
    "        self.circuit = circuit\n",
    "        self.policy = policy\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        Input state is fed to the circuit - its output is then fed to the post processing \n",
    "        '''\n",
    "        probs = self.circuit.input(inputs)\n",
    "        probs_processed = self.policy.input(probs)\n",
    "        return probs_processed\n",
    "    \n",
    "    def sample(self, inputs):\n",
    "        '''\n",
    "        Samples an action from the action probability distribution aka policy\n",
    "        '''\n",
    "        policy = self.forward(inputs)\n",
    "        dist = torch.distributions.Categorical(policy)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action), policy\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        '''\n",
    "        Returns the values of each set of parameters\n",
    "        '''\n",
    "        parameter_values = [param.clone().detach().numpy().flatten() for param in self.circuit.parameters()]\n",
    "        return parameter_values\n",
    "    \n",
    "    def get_gradients(self):\n",
    "        '''\n",
    "        Returns the gradient values of each set of parameters from both circuit and policy\n",
    "        '''\n",
    "        gradients = []\n",
    "\n",
    "        # Get gradients from circuit parameters\n",
    "        circuit_gradients = [torch.flatten(param.grad.clone().detach()) if param.grad is not None else torch.flatten(torch.zeros_like(param)) for param in self.circuit.parameters()]\n",
    "        gradients.extend(circuit_gradients)\n",
    "\n",
    "        # Get gradients from policy parameters\n",
    "        policy_gradients = [torch.flatten(param.grad.clone().detach()) if param.grad is not None else torch.flatten(torch.zeros_like(param)) for param in self.policy.parameters()]\n",
    "        gradients.extend(policy_gradients)\n",
    "\n",
    "        return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceUpdate():\n",
    "\n",
    "    def __init__(self, pqc, optimizer, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name = None, rundate = None):\n",
    "        \n",
    "        self.pqc = pqc\n",
    "        self.optimizer = optimizer\n",
    "        self.env = env\n",
    "        self.env_name = env_name\n",
    "        self.n_episodes = n_episodes\n",
    "        self.max_t = max_t\n",
    "        self.gamma = gamma\n",
    "        self.scores_deque = deque(maxlen=10)\n",
    "        self.print_every = print_every\n",
    "        self.verbose = verbose\n",
    "        self.file_name = file_name\n",
    "        self.rundate = rundate\n",
    "        self.running_reward = 10\n",
    "\n",
    "    def get_trajectory(self):\n",
    "        '''\n",
    "        Gets a trajectory based on the running policy until it runs out of bounds or solves the envinronment\n",
    "        '''\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        state = self.env.reset()\n",
    "        for t in range(self.max_t):\n",
    "            if t == 0:\n",
    "                state_tensor = torch.tensor(state[0]).float()\n",
    "            else:\n",
    "                state_tensor = torch.tensor(state).float()\n",
    "            action, log_prob, _, = self.pqc.sample(state_tensor)\n",
    "            state, reward, done, _, _ = self.env.step(action)\n",
    "            \n",
    "            self.saved_log_probs.append(log_prob)\n",
    "            self.rewards.append(reward)\n",
    "\n",
    "            if done:\n",
    "                self.scores_deque.append(sum(self.rewards))\n",
    "                break\n",
    "\n",
    "    def update_policy(self):\n",
    "        '''\n",
    "        Computes the loss and gradients and updates the policy via gradient methods\n",
    "        '''\n",
    "\n",
    "        R = 0\n",
    "        policy_loss = []\n",
    "        returns = []\n",
    "        for r in self.rewards[::-1]:\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "\n",
    "        for log_prob, ret in zip(self.saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * ret)\n",
    "\n",
    "        policy_unsqueezed = [torch.unsqueeze(loss, 0) for loss in policy_loss]\n",
    "        self.loss = torch.cat(policy_unsqueezed).sum()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        self.loss.backward()    \n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def save_agent_data(self,main_path):\n",
    "        '''\n",
    "        Stores the model parameters into a json file\n",
    "\n",
    "        '''\n",
    "\n",
    "        agent_variables = {\n",
    "            \"Number of Qubits\": self.pqc.circuit.n_qubits,\n",
    "            \"Number of Layers\": self.pqc.circuit.n_layers,\n",
    "            \"Shots\": self.pqc.circuit.shots,\n",
    "            \"Input Scaling\": self.pqc.circuit.input_scaling,\n",
    "            \"Design\": self.pqc.circuit.design,\n",
    "            \"Differentiation Method\": self.pqc.circuit.diff_method,\n",
    "            \"Weight Initialization\": \"lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\",\n",
    "            \"Input Initialization\": get_function_representation(self.pqc.circuit.input_init),\n",
    "            \"Measure\": get_function_representation(self.pqc.circuit.measure),\n",
    "            \"Measure Qubits\": self.pqc.circuit.measure_qubits,\n",
    "            \"Policy Type\": self.pqc.policy.post_processing,\n",
    "            \"Softmax scheduling (in case policy is softmax)\": str(self.pqc.policy.beta_scheduling) + (\". Starting beta: \" + str(self.pqc.policy.beta) + \". Increase rate: \" + str(self.pqc.policy.increase_rate)),\n",
    "            \"Softmax output scalling (in case policy is softmax)\" : str(self.pqc.policy.output_scaling) + \". Output Initialization: \" + get_function_representation(self.pqc.policy.output_init),\n",
    "            \"Optimizers\": str(self.optimizer),\n",
    "            \"Envinronment Name\": str(self.env_name),\n",
    "            \"Gamma (discounting factor)\": self.gamma,\n",
    "        }\n",
    "\n",
    "        with open(os.path.join(main_path, \"agent_characteristics.json\"), \"w\") as f:\n",
    "            json.dump(agent_variables, f, indent=4)\n",
    "\n",
    "    def save_data(self,main_path):\n",
    "        '''\n",
    "        Saves the data into a .npz file of each episode\n",
    "\n",
    "        '''\n",
    "\n",
    "        run= os.path.join(main_path,str(self.file_name)+'_data.npz')\n",
    "\n",
    "        if not os.path.exists(main_path):\n",
    "            os.makedirs(main_path)\n",
    "\n",
    "        if os.path.exists(run):\n",
    "            data = np.load(run, allow_pickle=True)\n",
    "            old_episode_reward = data['episode_reward'].tolist()\n",
    "            old_loss = data['loss'].tolist()\n",
    "            old_runtime = data['runtime'].tolist()\n",
    "            old_params_gradients = data['params_gradients'].tolist()\n",
    "            old_input_params_gradients = data['input_params_gradients'].tolist()\n",
    "        else:\n",
    "            old_episode_reward = []\n",
    "            old_loss = []\n",
    "            old_runtime = []\n",
    "            old_params_gradients = []\n",
    "            old_input_params_gradients = []\n",
    "\n",
    "\n",
    "        old_episode_reward.append(self.scores_deque[-1])\n",
    "        old_loss.append(self.loss.item())\n",
    "        old_runtime.append(self.runtime)\n",
    "        old_params_gradients.append(tensor_to_list(self.pqc.get_gradients()[0]))\n",
    "        old_input_params_gradients.append(tensor_to_list(self.pqc.get_gradients()[1]))\n",
    "\n",
    "        np.savez_compressed(run, episode_reward = np.array(old_episode_reward),\n",
    "                                 loss = np.array(old_loss),\n",
    "                                 runtime = np.array(old_runtime),\n",
    "                                 params_gradients = np.array(old_params_gradients), \n",
    "                                 input_params_gradients = np.array(old_input_params_gradients))\n",
    "        \n",
    "        del old_episode_reward[:]\n",
    "        del old_loss[:]\n",
    "        del old_runtime[:]\n",
    "        del old_params_gradients[:]\n",
    "        del old_input_params_gradients[:]\n",
    "\n",
    "    def writer_function(self, writer, iteration):\n",
    "        '''\n",
    "        Stores data into a tensorboard session\n",
    "\n",
    "        '''\n",
    "        writer.add_scalar(\"Episode Reward\", np.mean(self.scores_deque), global_step=iteration)\n",
    "        writer.add_scalar(\"Runtime\", self.runtime, global_step=iteration)\n",
    "        writer.add_scalar(\"Loss\", self.loss.item(), global_step=iteration)\n",
    "        writer.add_scalar(\"Beta\", self.pqc.policy.beta, global_step=iteration)\n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        logs_dir = \"../../data\"\n",
    "        os.makedirs(logs_dir, exist_ok=True)\n",
    "        envinronment_folder = os.path.join(logs_dir, self.env_name)\n",
    "        os.makedirs(envinronment_folder, exist_ok=True)\n",
    "        experiment_folder = f\"{self.pqc.policy.post_processing}_{self.pqc.circuit.n_layers}layer_{self.rundate}\"\n",
    "        experiment_path = os.path.join(envinronment_folder, experiment_folder)\n",
    "        os.makedirs(experiment_path, exist_ok=True)\n",
    "        run = os.path.join(experiment_path,str(self.file_name))\n",
    "        os.makedirs(run, exist_ok=True)\n",
    "        writer = SummaryWriter(log_dir=run)\n",
    "        self.save_agent_data(experiment_path)\n",
    "        \n",
    "        for i in range(1, self.n_episodes):\n",
    "            start_time = time.time()\n",
    "            self.get_trajectory()\n",
    "            self.update_policy()\n",
    "            end_time = time.time()\n",
    "            self.runtime = end_time - start_time\n",
    "            self.writer_function(writer,i)\n",
    "            self.save_data(run)\n",
    "            self.pqc.policy.beta_schedule()\n",
    "            \n",
    "            if np.mean(self.scores_deque) > self.env.spec.reward_threshold:\n",
    "                print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i, np.mean(self.scores_deque)))\n",
    "                break\n",
    "            elif i % self.print_every == 0 and self.verbose == 1:\n",
    "                print('Episode {}\\tLast reward: {:.2f}\\tLast {} Episodes average reward: {:.2f}\\tRuntime: {:.2f}\\t '.format(i, self.scores_deque[-1], 10, np.mean(self.scores_deque), self.runtime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single agent runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tLast reward: 40.00\tLast 10 Episodes average reward: 40.00\tRuntime: 1.37\t \n",
      "Episode 2\tLast reward: 17.00\tLast 10 Episodes average reward: 28.50\tRuntime: 0.61\t \n",
      "Episode 3\tLast reward: 14.00\tLast 10 Episodes average reward: 23.67\tRuntime: 0.50\t \n",
      "Episode 4\tLast reward: 19.00\tLast 10 Episodes average reward: 22.50\tRuntime: 0.67\t \n",
      "Episode 5\tLast reward: 14.00\tLast 10 Episodes average reward: 20.80\tRuntime: 0.50\t \n",
      "Episode 6\tLast reward: 10.00\tLast 10 Episodes average reward: 19.00\tRuntime: 0.36\t \n",
      "Episode 7\tLast reward: 16.00\tLast 10 Episodes average reward: 18.57\tRuntime: 0.56\t \n",
      "Episode 8\tLast reward: 31.00\tLast 10 Episodes average reward: 20.12\tRuntime: 1.07\t \n",
      "Episode 9\tLast reward: 10.00\tLast 10 Episodes average reward: 19.00\tRuntime: 0.37\t \n",
      "Episode 10\tLast reward: 18.00\tLast 10 Episodes average reward: 18.90\tRuntime: 0.63\t \n",
      "Episode 11\tLast reward: 26.00\tLast 10 Episodes average reward: 17.50\tRuntime: 0.92\t \n",
      "Episode 12\tLast reward: 22.00\tLast 10 Episodes average reward: 18.00\tRuntime: 0.78\t \n",
      "Episode 13\tLast reward: 16.00\tLast 10 Episodes average reward: 18.20\tRuntime: 0.56\t \n",
      "Episode 14\tLast reward: 34.00\tLast 10 Episodes average reward: 19.70\tRuntime: 1.18\t \n",
      "Episode 15\tLast reward: 37.00\tLast 10 Episodes average reward: 22.00\tRuntime: 1.31\t \n",
      "Episode 16\tLast reward: 32.00\tLast 10 Episodes average reward: 24.20\tRuntime: 1.14\t \n",
      "Episode 17\tLast reward: 26.00\tLast 10 Episodes average reward: 25.20\tRuntime: 0.93\t \n",
      "Episode 18\tLast reward: 18.00\tLast 10 Episodes average reward: 23.90\tRuntime: 0.63\t \n",
      "Episode 19\tLast reward: 25.00\tLast 10 Episodes average reward: 25.40\tRuntime: 0.86\t \n",
      "Episode 20\tLast reward: 68.00\tLast 10 Episodes average reward: 30.40\tRuntime: 2.50\t \n",
      "Episode 21\tLast reward: 17.00\tLast 10 Episodes average reward: 29.50\tRuntime: 0.63\t \n",
      "Episode 22\tLast reward: 20.00\tLast 10 Episodes average reward: 29.30\tRuntime: 0.69\t \n",
      "Episode 23\tLast reward: 51.00\tLast 10 Episodes average reward: 32.80\tRuntime: 1.76\t \n",
      "Episode 24\tLast reward: 11.00\tLast 10 Episodes average reward: 30.50\tRuntime: 0.42\t \n",
      "Episode 25\tLast reward: 37.00\tLast 10 Episodes average reward: 30.50\tRuntime: 1.26\t \n",
      "Episode 26\tLast reward: 72.00\tLast 10 Episodes average reward: 34.50\tRuntime: 2.56\t \n",
      "Episode 27\tLast reward: 35.00\tLast 10 Episodes average reward: 35.40\tRuntime: 1.27\t \n",
      "Episode 28\tLast reward: 19.00\tLast 10 Episodes average reward: 35.50\tRuntime: 0.69\t \n",
      "Episode 29\tLast reward: 12.00\tLast 10 Episodes average reward: 34.20\tRuntime: 0.43\t \n",
      "Episode 30\tLast reward: 82.00\tLast 10 Episodes average reward: 35.60\tRuntime: 2.85\t \n",
      "Episode 31\tLast reward: 45.00\tLast 10 Episodes average reward: 38.40\tRuntime: 1.63\t \n",
      "Episode 32\tLast reward: 76.00\tLast 10 Episodes average reward: 44.00\tRuntime: 2.75\t \n",
      "Episode 33\tLast reward: 15.00\tLast 10 Episodes average reward: 40.40\tRuntime: 0.58\t \n",
      "Episode 34\tLast reward: 26.00\tLast 10 Episodes average reward: 41.90\tRuntime: 0.89\t \n",
      "Episode 35\tLast reward: 67.00\tLast 10 Episodes average reward: 44.90\tRuntime: 2.33\t \n",
      "Episode 36\tLast reward: 35.00\tLast 10 Episodes average reward: 41.20\tRuntime: 1.26\t \n",
      "Episode 37\tLast reward: 45.00\tLast 10 Episodes average reward: 42.20\tRuntime: 1.57\t \n",
      "Episode 38\tLast reward: 38.00\tLast 10 Episodes average reward: 44.10\tRuntime: 1.33\t \n",
      "Episode 39\tLast reward: 90.00\tLast 10 Episodes average reward: 51.90\tRuntime: 3.16\t \n",
      "Episode 40\tLast reward: 31.00\tLast 10 Episodes average reward: 46.80\tRuntime: 1.15\t \n",
      "Episode 41\tLast reward: 64.00\tLast 10 Episodes average reward: 48.70\tRuntime: 2.26\t \n",
      "Episode 42\tLast reward: 38.00\tLast 10 Episodes average reward: 44.90\tRuntime: 1.46\t \n",
      "Episode 43\tLast reward: 60.00\tLast 10 Episodes average reward: 49.40\tRuntime: 2.10\t \n",
      "Episode 44\tLast reward: 87.00\tLast 10 Episodes average reward: 55.50\tRuntime: 3.06\t \n",
      "Episode 45\tLast reward: 42.00\tLast 10 Episodes average reward: 53.00\tRuntime: 1.51\t \n",
      "Episode 46\tLast reward: 24.00\tLast 10 Episodes average reward: 51.90\tRuntime: 0.85\t \n",
      "Episode 47\tLast reward: 96.00\tLast 10 Episodes average reward: 57.00\tRuntime: 3.33\t \n",
      "Episode 48\tLast reward: 176.00\tLast 10 Episodes average reward: 70.80\tRuntime: 6.35\t \n",
      "Episode 49\tLast reward: 67.00\tLast 10 Episodes average reward: 68.50\tRuntime: 2.49\t \n",
      "Episode 50\tLast reward: 80.00\tLast 10 Episodes average reward: 73.40\tRuntime: 2.84\t \n",
      "Episode 51\tLast reward: 32.00\tLast 10 Episodes average reward: 70.20\tRuntime: 1.17\t \n",
      "Episode 52\tLast reward: 71.00\tLast 10 Episodes average reward: 73.50\tRuntime: 2.47\t \n",
      "Episode 53\tLast reward: 64.00\tLast 10 Episodes average reward: 73.90\tRuntime: 2.26\t \n",
      "Episode 54\tLast reward: 12.00\tLast 10 Episodes average reward: 66.40\tRuntime: 0.47\t \n",
      "Episode 55\tLast reward: 200.00\tLast 10 Episodes average reward: 82.20\tRuntime: 7.10\t \n",
      "Episode 56\tLast reward: 73.00\tLast 10 Episodes average reward: 87.10\tRuntime: 2.75\t \n",
      "Episode 57\tLast reward: 39.00\tLast 10 Episodes average reward: 81.40\tRuntime: 1.42\t \n",
      "Episode 58\tLast reward: 45.00\tLast 10 Episodes average reward: 68.30\tRuntime: 1.58\t \n",
      "Episode 59\tLast reward: 73.00\tLast 10 Episodes average reward: 68.90\tRuntime: 2.56\t \n",
      "Episode 60\tLast reward: 95.00\tLast 10 Episodes average reward: 70.40\tRuntime: 3.48\t \n",
      "Episode 61\tLast reward: 79.00\tLast 10 Episodes average reward: 75.10\tRuntime: 2.80\t \n",
      "Episode 62\tLast reward: 71.00\tLast 10 Episodes average reward: 75.10\tRuntime: 2.51\t \n",
      "Episode 63\tLast reward: 81.00\tLast 10 Episodes average reward: 76.80\tRuntime: 2.85\t \n",
      "Episode 64\tLast reward: 67.00\tLast 10 Episodes average reward: 82.30\tRuntime: 2.42\t \n",
      "Episode 65\tLast reward: 130.00\tLast 10 Episodes average reward: 75.30\tRuntime: 4.71\t \n",
      "Episode 66\tLast reward: 76.00\tLast 10 Episodes average reward: 75.60\tRuntime: 2.74\t \n",
      "Episode 67\tLast reward: 104.00\tLast 10 Episodes average reward: 82.10\tRuntime: 3.66\t \n",
      "Episode 68\tLast reward: 53.00\tLast 10 Episodes average reward: 82.90\tRuntime: 1.93\t \n",
      "Episode 69\tLast reward: 73.00\tLast 10 Episodes average reward: 82.90\tRuntime: 2.56\t \n",
      "Episode 70\tLast reward: 37.00\tLast 10 Episodes average reward: 77.10\tRuntime: 1.46\t \n",
      "Episode 71\tLast reward: 182.00\tLast 10 Episodes average reward: 87.40\tRuntime: 6.49\t \n",
      "Episode 72\tLast reward: 74.00\tLast 10 Episodes average reward: 87.70\tRuntime: 2.71\t \n",
      "Episode 73\tLast reward: 66.00\tLast 10 Episodes average reward: 86.20\tRuntime: 2.34\t \n",
      "Episode 74\tLast reward: 82.00\tLast 10 Episodes average reward: 87.70\tRuntime: 2.99\t \n",
      "Episode 75\tLast reward: 91.00\tLast 10 Episodes average reward: 83.80\tRuntime: 3.21\t \n",
      "Episode 76\tLast reward: 113.00\tLast 10 Episodes average reward: 87.50\tRuntime: 4.04\t \n",
      "Episode 77\tLast reward: 152.00\tLast 10 Episodes average reward: 92.30\tRuntime: 5.39\t \n",
      "Episode 78\tLast reward: 130.00\tLast 10 Episodes average reward: 100.00\tRuntime: 4.74\t \n",
      "Episode 79\tLast reward: 85.00\tLast 10 Episodes average reward: 101.20\tRuntime: 3.04\t \n",
      "Episode 80\tLast reward: 221.00\tLast 10 Episodes average reward: 119.60\tRuntime: 7.80\t \n",
      "Episode 81\tLast reward: 116.00\tLast 10 Episodes average reward: 113.00\tRuntime: 4.31\t \n",
      "Episode 82\tLast reward: 123.00\tLast 10 Episodes average reward: 117.90\tRuntime: 4.36\t \n",
      "Episode 83\tLast reward: 158.00\tLast 10 Episodes average reward: 127.10\tRuntime: 5.58\t \n",
      "Episode 84\tLast reward: 168.00\tLast 10 Episodes average reward: 135.70\tRuntime: 6.07\t \n",
      "Episode 85\tLast reward: 105.00\tLast 10 Episodes average reward: 137.10\tRuntime: 3.79\t \n",
      "Episode 86\tLast reward: 85.00\tLast 10 Episodes average reward: 134.30\tRuntime: 3.04\t \n",
      "Episode 87\tLast reward: 119.00\tLast 10 Episodes average reward: 131.00\tRuntime: 4.29\t \n",
      "Episode 88\tLast reward: 224.00\tLast 10 Episodes average reward: 140.40\tRuntime: 7.94\t \n",
      "Episode 89\tLast reward: 233.00\tLast 10 Episodes average reward: 155.20\tRuntime: 8.53\t \n",
      "Episode 90\tLast reward: 160.00\tLast 10 Episodes average reward: 149.10\tRuntime: 5.75\t \n",
      "Episode 91\tLast reward: 188.00\tLast 10 Episodes average reward: 156.30\tRuntime: 6.79\t \n",
      "Episode 92\tLast reward: 141.00\tLast 10 Episodes average reward: 158.10\tRuntime: 5.08\t \n",
      "Episode 93\tLast reward: 136.00\tLast 10 Episodes average reward: 155.90\tRuntime: 4.83\t \n",
      "Episode 94\tLast reward: 111.00\tLast 10 Episodes average reward: 150.20\tRuntime: 3.99\t \n",
      "Episode 95\tLast reward: 103.00\tLast 10 Episodes average reward: 150.00\tRuntime: 3.75\t \n",
      "Episode 96\tLast reward: 113.00\tLast 10 Episodes average reward: 152.80\tRuntime: 4.05\t \n",
      "Episode 97\tLast reward: 133.00\tLast 10 Episodes average reward: 154.20\tRuntime: 4.73\t \n",
      "Episode 98\tLast reward: 189.00\tLast 10 Episodes average reward: 150.70\tRuntime: 6.81\t \n",
      "Episode 99\tLast reward: 201.00\tLast 10 Episodes average reward: 147.50\tRuntime: 7.20\t \n",
      "Episode 100\tLast reward: 111.00\tLast 10 Episodes average reward: 142.60\tRuntime: 4.04\t \n",
      "Episode 101\tLast reward: 169.00\tLast 10 Episodes average reward: 140.70\tRuntime: 6.11\t \n",
      "Episode 102\tLast reward: 112.00\tLast 10 Episodes average reward: 137.80\tRuntime: 4.01\t \n",
      "Episode 103\tLast reward: 179.00\tLast 10 Episodes average reward: 142.10\tRuntime: 6.42\t \n",
      "Episode 104\tLast reward: 350.00\tLast 10 Episodes average reward: 166.00\tRuntime: 12.53\t \n",
      "Episode 105\tLast reward: 194.00\tLast 10 Episodes average reward: 175.10\tRuntime: 7.12\t \n",
      "Episode 106\tLast reward: 326.00\tLast 10 Episodes average reward: 196.40\tRuntime: 11.84\t \n",
      "Episode 107\tLast reward: 180.00\tLast 10 Episodes average reward: 201.10\tRuntime: 6.54\t \n",
      "Episode 108\tLast reward: 422.00\tLast 10 Episodes average reward: 224.40\tRuntime: 15.27\t \n",
      "Episode 109\tLast reward: 345.00\tLast 10 Episodes average reward: 238.80\tRuntime: 12.63\t \n",
      "Episode 110\tLast reward: 345.00\tLast 10 Episodes average reward: 238.80\tRuntime: 18.10\t \n",
      "Episode 111\tLast reward: 116.00\tLast 10 Episodes average reward: 239.30\tRuntime: 4.69\t \n",
      "Episode 112\tLast reward: 277.00\tLast 10 Episodes average reward: 250.10\tRuntime: 10.00\t \n",
      "Episode 113\tLast reward: 169.00\tLast 10 Episodes average reward: 255.80\tRuntime: 6.18\t \n",
      "Episode 114\tLast reward: 337.00\tLast 10 Episodes average reward: 271.60\tRuntime: 12.14\t \n",
      "Episode 115\tLast reward: 143.00\tLast 10 Episodes average reward: 250.90\tRuntime: 5.35\t \n",
      "Episode 116\tLast reward: 224.00\tLast 10 Episodes average reward: 253.90\tRuntime: 8.06\t \n",
      "Episode 117\tLast reward: 204.00\tLast 10 Episodes average reward: 241.70\tRuntime: 7.28\t \n",
      "Episode 118\tLast reward: 414.00\tLast 10 Episodes average reward: 265.10\tRuntime: 14.95\t \n",
      "Episode 119\tLast reward: 394.00\tLast 10 Episodes average reward: 262.30\tRuntime: 14.38\t \n",
      "Episode 120\tLast reward: 178.00\tLast 10 Episodes average reward: 245.60\tRuntime: 6.66\t \n",
      "Episode 121\tLast reward: 479.00\tLast 10 Episodes average reward: 281.90\tRuntime: 17.25\t \n",
      "Episode 122\tLast reward: 212.00\tLast 10 Episodes average reward: 275.40\tRuntime: 7.91\t \n",
      "Episode 123\tLast reward: 333.00\tLast 10 Episodes average reward: 291.80\tRuntime: 12.08\t \n",
      "Episode 124\tLast reward: 333.00\tLast 10 Episodes average reward: 291.80\tRuntime: 18.12\t \n",
      "Episode 125\tLast reward: 245.00\tLast 10 Episodes average reward: 282.60\tRuntime: 9.14\t \n",
      "Episode 126\tLast reward: 185.00\tLast 10 Episodes average reward: 286.80\tRuntime: 6.84\t \n",
      "Episode 127\tLast reward: 319.00\tLast 10 Episodes average reward: 296.30\tRuntime: 11.40\t \n",
      "Episode 128\tLast reward: 453.00\tLast 10 Episodes average reward: 321.20\tRuntime: 16.47\t \n",
      "Episode 129\tLast reward: 481.00\tLast 10 Episodes average reward: 327.90\tRuntime: 17.68\t \n",
      "Episode 130\tLast reward: 62.00\tLast 10 Episodes average reward: 294.70\tRuntime: 2.83\t \n",
      "Episode 131\tLast reward: 392.00\tLast 10 Episodes average reward: 316.10\tRuntime: 14.27\t \n",
      "Episode 132\tLast reward: 152.00\tLast 10 Episodes average reward: 283.40\tRuntime: 5.67\t \n",
      "Episode 133\tLast reward: 102.00\tLast 10 Episodes average reward: 272.40\tRuntime: 3.70\t \n",
      "Episode 134\tLast reward: 217.00\tLast 10 Episodes average reward: 260.80\tRuntime: 7.81\t \n",
      "Episode 135\tLast reward: 219.00\tLast 10 Episodes average reward: 258.20\tRuntime: 7.86\t \n",
      "Episode 136\tLast reward: 155.00\tLast 10 Episodes average reward: 255.20\tRuntime: 5.58\t \n",
      "Episode 137\tLast reward: 140.00\tLast 10 Episodes average reward: 237.30\tRuntime: 5.12\t \n",
      "Episode 138\tLast reward: 164.00\tLast 10 Episodes average reward: 208.40\tRuntime: 5.87\t \n",
      "Episode 139\tLast reward: 334.00\tLast 10 Episodes average reward: 193.70\tRuntime: 11.99\t \n",
      "Episode 140\tLast reward: 128.00\tLast 10 Episodes average reward: 200.30\tRuntime: 4.81\t \n",
      "Episode 141\tLast reward: 150.00\tLast 10 Episodes average reward: 176.10\tRuntime: 5.47\t \n",
      "Episode 142\tLast reward: 189.00\tLast 10 Episodes average reward: 179.80\tRuntime: 6.69\t \n",
      "Episode 143\tLast reward: 137.00\tLast 10 Episodes average reward: 183.30\tRuntime: 4.94\t \n",
      "Episode 144\tLast reward: 136.00\tLast 10 Episodes average reward: 175.20\tRuntime: 5.00\t \n",
      "Episode 145\tLast reward: 91.00\tLast 10 Episodes average reward: 162.40\tRuntime: 3.27\t \n",
      "Episode 146\tLast reward: 54.00\tLast 10 Episodes average reward: 152.30\tRuntime: 1.94\t \n",
      "Episode 147\tLast reward: 166.00\tLast 10 Episodes average reward: 154.90\tRuntime: 5.82\t \n",
      "Episode 148\tLast reward: 112.00\tLast 10 Episodes average reward: 149.70\tRuntime: 4.16\t \n",
      "Episode 149\tLast reward: 104.00\tLast 10 Episodes average reward: 126.70\tRuntime: 3.74\t \n",
      "Episode 150\tLast reward: 143.00\tLast 10 Episodes average reward: 128.20\tRuntime: 5.08\t \n",
      "Episode 151\tLast reward: 164.00\tLast 10 Episodes average reward: 129.60\tRuntime: 5.92\t \n",
      "Episode 152\tLast reward: 112.00\tLast 10 Episodes average reward: 121.90\tRuntime: 4.02\t \n",
      "Episode 153\tLast reward: 151.00\tLast 10 Episodes average reward: 123.30\tRuntime: 5.39\t \n",
      "Episode 154\tLast reward: 113.00\tLast 10 Episodes average reward: 121.00\tRuntime: 4.11\t \n",
      "Episode 155\tLast reward: 102.00\tLast 10 Episodes average reward: 122.10\tRuntime: 3.61\t \n",
      "Episode 156\tLast reward: 101.00\tLast 10 Episodes average reward: 126.80\tRuntime: 3.58\t \n",
      "Episode 157\tLast reward: 112.00\tLast 10 Episodes average reward: 121.40\tRuntime: 4.00\t \n",
      "Episode 158\tLast reward: 119.00\tLast 10 Episodes average reward: 122.10\tRuntime: 4.36\t \n",
      "Episode 159\tLast reward: 122.00\tLast 10 Episodes average reward: 123.90\tRuntime: 4.34\t \n",
      "Episode 160\tLast reward: 158.00\tLast 10 Episodes average reward: 125.40\tRuntime: 5.57\t \n",
      "Episode 161\tLast reward: 104.00\tLast 10 Episodes average reward: 119.40\tRuntime: 3.81\t \n",
      "Episode 162\tLast reward: 140.00\tLast 10 Episodes average reward: 122.20\tRuntime: 4.99\t \n",
      "Episode 163\tLast reward: 109.00\tLast 10 Episodes average reward: 118.00\tRuntime: 3.90\t \n",
      "Episode 164\tLast reward: 144.00\tLast 10 Episodes average reward: 121.10\tRuntime: 5.26\t \n",
      "Episode 165\tLast reward: 123.00\tLast 10 Episodes average reward: 123.20\tRuntime: 4.36\t \n",
      "Episode 166\tLast reward: 137.00\tLast 10 Episodes average reward: 126.80\tRuntime: 4.84\t \n",
      "Episode 167\tLast reward: 139.00\tLast 10 Episodes average reward: 129.50\tRuntime: 5.00\t \n",
      "Episode 168\tLast reward: 117.00\tLast 10 Episodes average reward: 129.30\tRuntime: 4.27\t \n",
      "Episode 169\tLast reward: 114.00\tLast 10 Episodes average reward: 128.50\tRuntime: 4.02\t \n",
      "Episode 170\tLast reward: 105.00\tLast 10 Episodes average reward: 123.20\tRuntime: 3.71\t \n",
      "Episode 171\tLast reward: 100.00\tLast 10 Episodes average reward: 122.80\tRuntime: 3.68\t \n",
      "Episode 172\tLast reward: 95.00\tLast 10 Episodes average reward: 118.30\tRuntime: 3.41\t \n",
      "Episode 173\tLast reward: 110.00\tLast 10 Episodes average reward: 118.40\tRuntime: 3.89\t \n",
      "Episode 174\tLast reward: 116.00\tLast 10 Episodes average reward: 115.60\tRuntime: 4.08\t \n",
      "Episode 175\tLast reward: 111.00\tLast 10 Episodes average reward: 114.40\tRuntime: 4.02\t \n",
      "Episode 176\tLast reward: 150.00\tLast 10 Episodes average reward: 115.70\tRuntime: 5.33\t \n",
      "Episode 177\tLast reward: 136.00\tLast 10 Episodes average reward: 115.40\tRuntime: 4.92\t \n",
      "Episode 178\tLast reward: 100.00\tLast 10 Episodes average reward: 113.70\tRuntime: 3.63\t \n",
      "Episode 179\tLast reward: 122.00\tLast 10 Episodes average reward: 114.50\tRuntime: 4.38\t \n",
      "Episode 180\tLast reward: 104.00\tLast 10 Episodes average reward: 114.40\tRuntime: 3.68\t \n",
      "Episode 181\tLast reward: 123.00\tLast 10 Episodes average reward: 116.70\tRuntime: 4.40\t \n",
      "Episode 182\tLast reward: 137.00\tLast 10 Episodes average reward: 120.90\tRuntime: 5.00\t \n",
      "Episode 183\tLast reward: 115.00\tLast 10 Episodes average reward: 121.40\tRuntime: 4.08\t \n",
      "Episode 184\tLast reward: 177.00\tLast 10 Episodes average reward: 127.50\tRuntime: 6.24\t \n",
      "Episode 185\tLast reward: 117.00\tLast 10 Episodes average reward: 128.10\tRuntime: 4.36\t \n",
      "Episode 186\tLast reward: 163.00\tLast 10 Episodes average reward: 129.40\tRuntime: 5.81\t \n",
      "Episode 187\tLast reward: 115.00\tLast 10 Episodes average reward: 127.30\tRuntime: 4.10\t \n",
      "Episode 188\tLast reward: 137.00\tLast 10 Episodes average reward: 131.00\tRuntime: 4.95\t \n",
      "Episode 189\tLast reward: 132.00\tLast 10 Episodes average reward: 132.00\tRuntime: 4.73\t \n",
      "Episode 190\tLast reward: 111.00\tLast 10 Episodes average reward: 132.70\tRuntime: 4.00\t \n",
      "Episode 191\tLast reward: 131.00\tLast 10 Episodes average reward: 133.50\tRuntime: 4.66\t \n",
      "Episode 192\tLast reward: 118.00\tLast 10 Episodes average reward: 131.60\tRuntime: 4.27\t \n",
      "Episode 193\tLast reward: 125.00\tLast 10 Episodes average reward: 132.60\tRuntime: 4.40\t \n",
      "Episode 194\tLast reward: 110.00\tLast 10 Episodes average reward: 125.90\tRuntime: 3.96\t \n",
      "Episode 195\tLast reward: 138.00\tLast 10 Episodes average reward: 128.00\tRuntime: 5.01\t \n",
      "Episode 196\tLast reward: 120.00\tLast 10 Episodes average reward: 123.70\tRuntime: 4.27\t \n",
      "Episode 197\tLast reward: 140.00\tLast 10 Episodes average reward: 126.20\tRuntime: 4.96\t \n",
      "Episode 198\tLast reward: 97.00\tLast 10 Episodes average reward: 122.20\tRuntime: 3.49\t \n",
      "Episode 199\tLast reward: 116.00\tLast 10 Episodes average reward: 120.60\tRuntime: 4.24\t \n",
      "Episode 200\tLast reward: 153.00\tLast 10 Episodes average reward: 124.80\tRuntime: 5.45\t \n",
      "Episode 201\tLast reward: 67.00\tLast 10 Episodes average reward: 118.40\tRuntime: 2.45\t \n",
      "Episode 202\tLast reward: 145.00\tLast 10 Episodes average reward: 121.10\tRuntime: 5.19\t \n",
      "Episode 203\tLast reward: 150.00\tLast 10 Episodes average reward: 123.60\tRuntime: 5.38\t \n",
      "Episode 204\tLast reward: 155.00\tLast 10 Episodes average reward: 128.10\tRuntime: 5.53\t \n",
      "Episode 205\tLast reward: 199.00\tLast 10 Episodes average reward: 134.20\tRuntime: 7.21\t \n",
      "Episode 206\tLast reward: 207.00\tLast 10 Episodes average reward: 142.90\tRuntime: 7.33\t \n",
      "Episode 207\tLast reward: 173.00\tLast 10 Episodes average reward: 146.20\tRuntime: 6.30\t \n",
      "Episode 208\tLast reward: 195.00\tLast 10 Episodes average reward: 156.00\tRuntime: 7.13\t \n",
      "Episode 209\tLast reward: 87.00\tLast 10 Episodes average reward: 153.10\tRuntime: 3.36\t \n",
      "Episode 210\tLast reward: 219.00\tLast 10 Episodes average reward: 159.70\tRuntime: 8.33\t \n",
      "Episode 211\tLast reward: 173.00\tLast 10 Episodes average reward: 170.30\tRuntime: 6.33\t \n",
      "Episode 212\tLast reward: 163.00\tLast 10 Episodes average reward: 172.10\tRuntime: 5.97\t \n",
      "Episode 213\tLast reward: 225.00\tLast 10 Episodes average reward: 179.60\tRuntime: 8.30\t \n",
      "Episode 214\tLast reward: 100.00\tLast 10 Episodes average reward: 174.10\tRuntime: 3.99\t \n",
      "Episode 215\tLast reward: 123.00\tLast 10 Episodes average reward: 166.50\tRuntime: 4.57\t \n",
      "Episode 216\tLast reward: 163.00\tLast 10 Episodes average reward: 162.10\tRuntime: 5.93\t \n",
      "Episode 217\tLast reward: 210.00\tLast 10 Episodes average reward: 165.80\tRuntime: 7.87\t \n",
      "Episode 218\tLast reward: 127.00\tLast 10 Episodes average reward: 159.00\tRuntime: 4.63\t \n",
      "Episode 219\tLast reward: 203.00\tLast 10 Episodes average reward: 170.60\tRuntime: 7.42\t \n",
      "Episode 220\tLast reward: 142.00\tLast 10 Episodes average reward: 162.90\tRuntime: 5.25\t \n",
      "Episode 221\tLast reward: 179.00\tLast 10 Episodes average reward: 163.50\tRuntime: 6.49\t \n",
      "Episode 222\tLast reward: 234.00\tLast 10 Episodes average reward: 170.60\tRuntime: 8.61\t \n",
      "Episode 223\tLast reward: 292.00\tLast 10 Episodes average reward: 177.30\tRuntime: 10.81\t \n",
      "Episode 224\tLast reward: 176.00\tLast 10 Episodes average reward: 184.90\tRuntime: 6.51\t \n",
      "Episode 225\tLast reward: 176.00\tLast 10 Episodes average reward: 184.90\tRuntime: 18.53\t \n",
      "Episode 226\tLast reward: 176.00\tLast 10 Episodes average reward: 184.90\tRuntime: 18.78\t \n",
      "Episode 227\tLast reward: 155.00\tLast 10 Episodes average reward: 188.10\tRuntime: 6.11\t \n",
      "Episode 228\tLast reward: 121.00\tLast 10 Episodes average reward: 183.90\tRuntime: 4.44\t \n",
      "Episode 229\tLast reward: 433.00\tLast 10 Episodes average reward: 206.20\tRuntime: 15.72\t \n",
      "Episode 230\tLast reward: 414.00\tLast 10 Episodes average reward: 234.90\tRuntime: 15.53\t \n",
      "Episode 231\tLast reward: 182.00\tLast 10 Episodes average reward: 232.80\tRuntime: 6.95\t \n",
      "Episode 232\tLast reward: 182.00\tLast 10 Episodes average reward: 232.80\tRuntime: 18.32\t \n",
      "Episode 233\tLast reward: 182.00\tLast 10 Episodes average reward: 232.80\tRuntime: 18.56\t \n",
      "Episode 234\tLast reward: 440.00\tLast 10 Episodes average reward: 262.60\tRuntime: 16.49\t \n",
      "Episode 235\tLast reward: 253.00\tLast 10 Episodes average reward: 270.00\tRuntime: 9.42\t \n",
      "Episode 236\tLast reward: 96.00\tLast 10 Episodes average reward: 256.20\tRuntime: 3.67\t \n",
      "Episode 237\tLast reward: 254.00\tLast 10 Episodes average reward: 252.40\tRuntime: 9.38\t \n",
      "Episode 238\tLast reward: 11.00\tLast 10 Episodes average reward: 235.90\tRuntime: 0.75\t \n",
      "Episode 239\tLast reward: 118.00\tLast 10 Episodes average reward: 232.20\tRuntime: 4.25\t \n",
      "Episode 240\tLast reward: 57.00\tLast 10 Episodes average reward: 225.80\tRuntime: 2.23\t \n",
      "Episode 241\tLast reward: 138.00\tLast 10 Episodes average reward: 196.30\tRuntime: 4.97\t \n",
      "Episode 242\tLast reward: 50.00\tLast 10 Episodes average reward: 159.90\tRuntime: 1.86\t \n",
      "Episode 243\tLast reward: 20.00\tLast 10 Episodes average reward: 143.70\tRuntime: 0.74\t \n",
      "Episode 244\tLast reward: 158.00\tLast 10 Episodes average reward: 115.50\tRuntime: 5.64\t \n",
      "Episode 245\tLast reward: 121.00\tLast 10 Episodes average reward: 102.30\tRuntime: 4.51\t \n",
      "Episode 246\tLast reward: 22.00\tLast 10 Episodes average reward: 94.90\tRuntime: 0.88\t \n",
      "Episode 247\tLast reward: 123.00\tLast 10 Episodes average reward: 81.80\tRuntime: 4.35\t \n",
      "Episode 248\tLast reward: 175.00\tLast 10 Episodes average reward: 98.20\tRuntime: 6.32\t \n",
      "Episode 249\tLast reward: 134.00\tLast 10 Episodes average reward: 99.80\tRuntime: 4.94\t \n",
      "Episode 250\tLast reward: 197.00\tLast 10 Episodes average reward: 113.80\tRuntime: 7.06\t \n",
      "Episode 251\tLast reward: 38.00\tLast 10 Episodes average reward: 103.80\tRuntime: 1.51\t \n",
      "Episode 252\tLast reward: 127.00\tLast 10 Episodes average reward: 111.50\tRuntime: 4.60\t \n",
      "Episode 253\tLast reward: 261.00\tLast 10 Episodes average reward: 135.60\tRuntime: 9.49\t \n",
      "Episode 254\tLast reward: 218.00\tLast 10 Episodes average reward: 141.60\tRuntime: 8.07\t \n",
      "Episode 255\tLast reward: 238.00\tLast 10 Episodes average reward: 153.30\tRuntime: 8.57\t \n",
      "Episode 256\tLast reward: 238.00\tLast 10 Episodes average reward: 153.30\tRuntime: 18.35\t \n",
      "Episode 257\tLast reward: 265.00\tLast 10 Episodes average reward: 177.60\tRuntime: 9.93\t \n",
      "Episode 258\tLast reward: 265.00\tLast 10 Episodes average reward: 177.60\tRuntime: 18.46\t \n",
      "Episode 259\tLast reward: 207.00\tLast 10 Episodes average reward: 186.00\tRuntime: 7.96\t \n",
      "Episode 260\tLast reward: 207.00\tLast 10 Episodes average reward: 186.00\tRuntime: 18.52\t \n",
      "Episode 261\tLast reward: 340.00\tLast 10 Episodes average reward: 202.50\tRuntime: 12.79\t \n",
      "Episode 262\tLast reward: 370.00\tLast 10 Episodes average reward: 226.10\tRuntime: 13.65\t \n",
      "Episode 263\tLast reward: 427.00\tLast 10 Episodes average reward: 249.10\tRuntime: 15.84\t \n",
      "Episode 264\tLast reward: 450.00\tLast 10 Episodes average reward: 290.30\tRuntime: 16.58\t \n",
      "Episode 265\tLast reward: 450.00\tLast 10 Episodes average reward: 290.30\tRuntime: 18.34\t \n",
      "Episode 266\tLast reward: 391.00\tLast 10 Episodes average reward: 316.70\tRuntime: 14.53\t \n",
      "Episode 267\tLast reward: 287.00\tLast 10 Episodes average reward: 319.30\tRuntime: 11.04\t \n",
      "Episode 268\tLast reward: 223.00\tLast 10 Episodes average reward: 319.80\tRuntime: 8.27\t \n",
      "Episode 269\tLast reward: 64.00\tLast 10 Episodes average reward: 302.40\tRuntime: 2.55\t \n",
      "Episode 270\tLast reward: 139.00\tLast 10 Episodes average reward: 289.80\tRuntime: 5.00\t \n",
      "Episode 271\tLast reward: 279.00\tLast 10 Episodes average reward: 297.00\tRuntime: 10.38\t \n",
      "Episode 272\tLast reward: 34.00\tLast 10 Episodes average reward: 266.40\tRuntime: 1.53\t \n",
      "Episode 273\tLast reward: 155.00\tLast 10 Episodes average reward: 244.90\tRuntime: 5.70\t \n",
      "Episode 274\tLast reward: 127.00\tLast 10 Episodes average reward: 214.90\tRuntime: 4.68\t \n",
      "Episode 275\tLast reward: 35.00\tLast 10 Episodes average reward: 173.40\tRuntime: 1.39\t \n",
      "Episode 276\tLast reward: 24.00\tLast 10 Episodes average reward: 136.70\tRuntime: 0.88\t \n",
      "Episode 277\tLast reward: 46.00\tLast 10 Episodes average reward: 112.60\tRuntime: 1.63\t \n",
      "Episode 278\tLast reward: 143.00\tLast 10 Episodes average reward: 104.60\tRuntime: 5.16\t \n",
      "Episode 279\tLast reward: 30.00\tLast 10 Episodes average reward: 101.20\tRuntime: 1.18\t \n",
      "Episode 280\tLast reward: 81.00\tLast 10 Episodes average reward: 95.40\tRuntime: 2.99\t \n",
      "Episode 281\tLast reward: 85.00\tLast 10 Episodes average reward: 76.00\tRuntime: 3.03\t \n",
      "Episode 282\tLast reward: 141.00\tLast 10 Episodes average reward: 86.70\tRuntime: 4.99\t \n",
      "Episode 283\tLast reward: 100.00\tLast 10 Episodes average reward: 81.20\tRuntime: 3.65\t \n",
      "Episode 284\tLast reward: 300.00\tLast 10 Episodes average reward: 98.50\tRuntime: 10.81\t \n",
      "Episode 285\tLast reward: 240.00\tLast 10 Episodes average reward: 119.00\tRuntime: 8.77\t \n",
      "Episode 286\tLast reward: 240.00\tLast 10 Episodes average reward: 119.00\tRuntime: 17.95\t \n",
      "Episode 287\tLast reward: 397.00\tLast 10 Episodes average reward: 156.30\tRuntime: 15.03\t \n",
      "Episode 288\tLast reward: 86.00\tLast 10 Episodes average reward: 160.30\tRuntime: 3.79\t \n",
      "Episode 289\tLast reward: 370.00\tLast 10 Episodes average reward: 183.00\tRuntime: 13.57\t \n",
      "Episode 290\tLast reward: 370.00\tLast 10 Episodes average reward: 183.00\tRuntime: 18.74\t \n",
      "Episode 291\tLast reward: 370.00\tLast 10 Episodes average reward: 183.00\tRuntime: 18.49\t \n",
      "Episode 292\tLast reward: 370.00\tLast 10 Episodes average reward: 183.00\tRuntime: 18.64\t \n",
      "Episode 293\tLast reward: 370.00\tLast 10 Episodes average reward: 183.00\tRuntime: 18.78\t \n",
      "Episode 294\tLast reward: 114.00\tLast 10 Episodes average reward: 191.40\tRuntime: 4.81\t \n",
      "Episode 295\tLast reward: 367.00\tLast 10 Episodes average reward: 220.00\tRuntime: 13.42\t \n",
      "Episode 296\tLast reward: 201.00\tLast 10 Episodes average reward: 231.60\tRuntime: 7.43\t \n",
      "Episode 297\tLast reward: 371.00\tLast 10 Episodes average reward: 254.60\tRuntime: 13.69\t \n",
      "Episode 298\tLast reward: 32.00\tLast 10 Episodes average reward: 247.80\tRuntime: 1.59\t \n",
      "Episode 299\tLast reward: 196.00\tLast 10 Episodes average reward: 237.40\tRuntime: 7.06\t \n",
      "Episode 300\tLast reward: 287.00\tLast 10 Episodes average reward: 242.10\tRuntime: 10.50\t \n",
      "Episode 301\tLast reward: 226.00\tLast 10 Episodes average reward: 225.00\tRuntime: 8.39\t \n",
      "Episode 302\tLast reward: 177.00\tLast 10 Episodes average reward: 234.10\tRuntime: 6.46\t \n",
      "Episode 303\tLast reward: 165.00\tLast 10 Episodes average reward: 213.60\tRuntime: 5.97\t \n",
      "Episode 304\tLast reward: 198.00\tLast 10 Episodes average reward: 222.00\tRuntime: 7.10\t \n",
      "Episode 305\tLast reward: 211.00\tLast 10 Episodes average reward: 206.40\tRuntime: 7.54\t \n",
      "Episode 306\tLast reward: 224.00\tLast 10 Episodes average reward: 208.70\tRuntime: 8.16\t \n",
      "Episode 307\tLast reward: 176.00\tLast 10 Episodes average reward: 189.20\tRuntime: 6.37\t \n",
      "Episode 308\tLast reward: 36.00\tLast 10 Episodes average reward: 189.60\tRuntime: 1.59\t \n",
      "Episode 309\tLast reward: 222.00\tLast 10 Episodes average reward: 192.20\tRuntime: 8.04\t \n",
      "Episode 310\tLast reward: 241.00\tLast 10 Episodes average reward: 187.60\tRuntime: 8.89\t \n",
      "Episode 311\tLast reward: 97.00\tLast 10 Episodes average reward: 174.70\tRuntime: 3.68\t \n",
      "Episode 312\tLast reward: 332.00\tLast 10 Episodes average reward: 190.20\tRuntime: 12.21\t \n",
      "Episode 313\tLast reward: 169.00\tLast 10 Episodes average reward: 190.60\tRuntime: 6.45\t \n",
      "Episode 314\tLast reward: 292.00\tLast 10 Episodes average reward: 200.00\tRuntime: 10.84\t \n",
      "Episode 315\tLast reward: 394.00\tLast 10 Episodes average reward: 218.30\tRuntime: 14.83\t \n",
      "Episode 316\tLast reward: 226.00\tLast 10 Episodes average reward: 218.50\tRuntime: 8.33\t \n",
      "Episode 317\tLast reward: 172.00\tLast 10 Episodes average reward: 218.10\tRuntime: 6.39\t \n",
      "Episode 318\tLast reward: 172.00\tLast 10 Episodes average reward: 218.10\tRuntime: 18.42\t \n",
      "Episode 319\tLast reward: 203.00\tLast 10 Episodes average reward: 234.80\tRuntime: 7.65\t \n",
      "Episode 320\tLast reward: 342.00\tLast 10 Episodes average reward: 246.80\tRuntime: 12.40\t \n",
      "Episode 321\tLast reward: 284.00\tLast 10 Episodes average reward: 251.10\tRuntime: 10.70\t \n",
      "Episode 322\tLast reward: 321.00\tLast 10 Episodes average reward: 273.50\tRuntime: 11.59\t \n",
      "Episode 323\tLast reward: 311.00\tLast 10 Episodes average reward: 271.40\tRuntime: 11.36\t \n",
      "Episode 324\tLast reward: 362.00\tLast 10 Episodes average reward: 290.70\tRuntime: 13.50\t \n",
      "Episode 325\tLast reward: 389.00\tLast 10 Episodes average reward: 300.40\tRuntime: 14.44\t \n",
      "Episode 326\tLast reward: 317.00\tLast 10 Episodes average reward: 292.70\tRuntime: 11.87\t \n",
      "Episode 327\tLast reward: 317.00\tLast 10 Episodes average reward: 292.70\tRuntime: 18.63\t \n",
      "Episode 328\tLast reward: 246.00\tLast 10 Episodes average reward: 294.70\tRuntime: 9.39\t \n",
      "Episode 329\tLast reward: 310.00\tLast 10 Episodes average reward: 308.50\tRuntime: 11.48\t \n",
      "Episode 330\tLast reward: 252.00\tLast 10 Episodes average reward: 313.40\tRuntime: 9.31\t \n",
      "Episode 331\tLast reward: 20.00\tLast 10 Episodes average reward: 281.20\tRuntime: 1.03\t \n",
      "Episode 332\tLast reward: 220.00\tLast 10 Episodes average reward: 274.80\tRuntime: 8.00\t \n",
      "Episode 333\tLast reward: 65.00\tLast 10 Episodes average reward: 249.20\tRuntime: 2.53\t \n",
      "Episode 334\tLast reward: 250.00\tLast 10 Episodes average reward: 243.10\tRuntime: 9.44\t \n",
      "Episode 335\tLast reward: 261.00\tLast 10 Episodes average reward: 233.00\tRuntime: 9.60\t \n",
      "Episode 336\tLast reward: 175.00\tLast 10 Episodes average reward: 211.60\tRuntime: 6.56\t \n",
      "Episode 337\tLast reward: 184.00\tLast 10 Episodes average reward: 198.30\tRuntime: 6.65\t \n",
      "Episode 338\tLast reward: 165.00\tLast 10 Episodes average reward: 190.20\tRuntime: 6.10\t \n",
      "Episode 339\tLast reward: 141.00\tLast 10 Episodes average reward: 173.30\tRuntime: 5.21\t \n",
      "Episode 340\tLast reward: 123.00\tLast 10 Episodes average reward: 160.40\tRuntime: 4.49\t \n",
      "Episode 341\tLast reward: 162.00\tLast 10 Episodes average reward: 174.60\tRuntime: 5.99\t \n",
      "Episode 342\tLast reward: 114.00\tLast 10 Episodes average reward: 164.00\tRuntime: 4.14\t \n",
      "Episode 343\tLast reward: 112.00\tLast 10 Episodes average reward: 168.70\tRuntime: 3.98\t \n",
      "Episode 344\tLast reward: 124.00\tLast 10 Episodes average reward: 156.10\tRuntime: 4.51\t \n",
      "Episode 345\tLast reward: 131.00\tLast 10 Episodes average reward: 143.10\tRuntime: 4.65\t \n",
      "Episode 346\tLast reward: 92.00\tLast 10 Episodes average reward: 134.80\tRuntime: 3.37\t \n",
      "Episode 347\tLast reward: 30.00\tLast 10 Episodes average reward: 119.40\tRuntime: 1.18\t \n",
      "Episode 348\tLast reward: 104.00\tLast 10 Episodes average reward: 113.30\tRuntime: 3.66\t \n",
      "Episode 349\tLast reward: 31.00\tLast 10 Episodes average reward: 102.30\tRuntime: 1.29\t \n",
      "Episode 350\tLast reward: 35.00\tLast 10 Episodes average reward: 93.50\tRuntime: 1.25\t \n",
      "Episode 351\tLast reward: 23.00\tLast 10 Episodes average reward: 79.60\tRuntime: 0.86\t \n",
      "Episode 352\tLast reward: 28.00\tLast 10 Episodes average reward: 71.00\tRuntime: 0.99\t \n",
      "Episode 353\tLast reward: 40.00\tLast 10 Episodes average reward: 63.80\tRuntime: 1.39\t \n",
      "Episode 354\tLast reward: 35.00\tLast 10 Episodes average reward: 54.90\tRuntime: 1.22\t \n",
      "Episode 355\tLast reward: 51.00\tLast 10 Episodes average reward: 46.90\tRuntime: 1.85\t \n",
      "Episode 356\tLast reward: 41.00\tLast 10 Episodes average reward: 41.80\tRuntime: 1.47\t \n",
      "Episode 357\tLast reward: 95.00\tLast 10 Episodes average reward: 48.30\tRuntime: 3.39\t \n",
      "Episode 358\tLast reward: 80.00\tLast 10 Episodes average reward: 45.90\tRuntime: 2.88\t \n",
      "Episode 359\tLast reward: 81.00\tLast 10 Episodes average reward: 50.90\tRuntime: 2.98\t \n",
      "Episode 360\tLast reward: 100.00\tLast 10 Episodes average reward: 57.40\tRuntime: 3.55\t \n",
      "Episode 361\tLast reward: 99.00\tLast 10 Episodes average reward: 65.00\tRuntime: 3.55\t \n",
      "Episode 362\tLast reward: 127.00\tLast 10 Episodes average reward: 74.90\tRuntime: 4.55\t \n",
      "Episode 363\tLast reward: 100.00\tLast 10 Episodes average reward: 80.90\tRuntime: 3.77\t \n",
      "Episode 364\tLast reward: 118.00\tLast 10 Episodes average reward: 89.20\tRuntime: 4.23\t \n",
      "Episode 365\tLast reward: 23.00\tLast 10 Episodes average reward: 86.40\tRuntime: 0.92\t \n",
      "Episode 366\tLast reward: 116.00\tLast 10 Episodes average reward: 93.90\tRuntime: 4.10\t \n",
      "Episode 367\tLast reward: 126.00\tLast 10 Episodes average reward: 97.00\tRuntime: 4.62\t \n",
      "Episode 368\tLast reward: 13.00\tLast 10 Episodes average reward: 90.30\tRuntime: 0.59\t \n",
      "Episode 369\tLast reward: 113.00\tLast 10 Episodes average reward: 93.50\tRuntime: 4.00\t \n",
      "Episode 370\tLast reward: 125.00\tLast 10 Episodes average reward: 96.00\tRuntime: 4.52\t \n",
      "Episode 371\tLast reward: 133.00\tLast 10 Episodes average reward: 99.40\tRuntime: 4.90\t \n",
      "Episode 372\tLast reward: 139.00\tLast 10 Episodes average reward: 100.60\tRuntime: 5.12\t \n",
      "Episode 373\tLast reward: 136.00\tLast 10 Episodes average reward: 104.20\tRuntime: 4.92\t \n",
      "Episode 374\tLast reward: 118.00\tLast 10 Episodes average reward: 104.20\tRuntime: 4.31\t \n",
      "Episode 375\tLast reward: 74.00\tLast 10 Episodes average reward: 109.30\tRuntime: 2.68\t \n",
      "Episode 376\tLast reward: 98.00\tLast 10 Episodes average reward: 107.50\tRuntime: 3.48\t \n",
      "Episode 377\tLast reward: 106.00\tLast 10 Episodes average reward: 105.50\tRuntime: 3.80\t \n",
      "Episode 378\tLast reward: 97.00\tLast 10 Episodes average reward: 113.90\tRuntime: 3.50\t \n",
      "Episode 379\tLast reward: 116.00\tLast 10 Episodes average reward: 114.20\tRuntime: 4.27\t \n",
      "Episode 380\tLast reward: 111.00\tLast 10 Episodes average reward: 112.80\tRuntime: 3.95\t \n",
      "Episode 381\tLast reward: 16.00\tLast 10 Episodes average reward: 101.10\tRuntime: 0.71\t \n",
      "Episode 382\tLast reward: 99.00\tLast 10 Episodes average reward: 97.10\tRuntime: 3.48\t \n",
      "Episode 383\tLast reward: 97.00\tLast 10 Episodes average reward: 93.20\tRuntime: 3.50\t \n",
      "Episode 384\tLast reward: 110.00\tLast 10 Episodes average reward: 92.40\tRuntime: 4.03\t \n",
      "Episode 385\tLast reward: 86.00\tLast 10 Episodes average reward: 93.60\tRuntime: 3.12\t \n",
      "Episode 386\tLast reward: 148.00\tLast 10 Episodes average reward: 98.60\tRuntime: 5.29\t \n",
      "Episode 387\tLast reward: 99.00\tLast 10 Episodes average reward: 97.90\tRuntime: 3.70\t \n",
      "Episode 388\tLast reward: 110.00\tLast 10 Episodes average reward: 99.20\tRuntime: 3.93\t \n",
      "Episode 389\tLast reward: 114.00\tLast 10 Episodes average reward: 99.00\tRuntime: 4.10\t \n",
      "Episode 390\tLast reward: 112.00\tLast 10 Episodes average reward: 99.10\tRuntime: 4.02\t \n",
      "Episode 391\tLast reward: 85.00\tLast 10 Episodes average reward: 106.00\tRuntime: 3.14\t \n",
      "Episode 392\tLast reward: 112.00\tLast 10 Episodes average reward: 107.30\tRuntime: 4.04\t \n",
      "Episode 393\tLast reward: 116.00\tLast 10 Episodes average reward: 109.20\tRuntime: 4.18\t \n",
      "Episode 394\tLast reward: 107.00\tLast 10 Episodes average reward: 108.90\tRuntime: 3.85\t \n",
      "Episode 395\tLast reward: 113.00\tLast 10 Episodes average reward: 111.60\tRuntime: 4.18\t \n",
      "Episode 396\tLast reward: 105.00\tLast 10 Episodes average reward: 107.30\tRuntime: 3.75\t \n",
      "Episode 397\tLast reward: 115.00\tLast 10 Episodes average reward: 108.90\tRuntime: 4.10\t \n",
      "Episode 398\tLast reward: 121.00\tLast 10 Episodes average reward: 110.00\tRuntime: 4.32\t \n",
      "Episode 399\tLast reward: 127.00\tLast 10 Episodes average reward: 111.30\tRuntime: 4.62\t \n",
      "Episode 400\tLast reward: 119.00\tLast 10 Episodes average reward: 112.00\tRuntime: 4.27\t \n",
      "Episode 401\tLast reward: 122.00\tLast 10 Episodes average reward: 115.70\tRuntime: 4.37\t \n",
      "Episode 402\tLast reward: 132.00\tLast 10 Episodes average reward: 117.70\tRuntime: 4.89\t \n",
      "Episode 403\tLast reward: 126.00\tLast 10 Episodes average reward: 118.70\tRuntime: 4.57\t \n",
      "Episode 404\tLast reward: 160.00\tLast 10 Episodes average reward: 124.00\tRuntime: 5.75\t \n",
      "Episode 405\tLast reward: 131.00\tLast 10 Episodes average reward: 125.80\tRuntime: 4.84\t \n",
      "Episode 406\tLast reward: 159.00\tLast 10 Episodes average reward: 131.20\tRuntime: 5.66\t \n",
      "Episode 407\tLast reward: 133.00\tLast 10 Episodes average reward: 133.00\tRuntime: 4.77\t \n",
      "Episode 408\tLast reward: 138.00\tLast 10 Episodes average reward: 134.70\tRuntime: 5.04\t \n",
      "Episode 409\tLast reward: 131.00\tLast 10 Episodes average reward: 135.10\tRuntime: 4.65\t \n",
      "Episode 410\tLast reward: 130.00\tLast 10 Episodes average reward: 136.20\tRuntime: 4.62\t \n",
      "Episode 411\tLast reward: 132.00\tLast 10 Episodes average reward: 137.20\tRuntime: 4.81\t \n",
      "Episode 412\tLast reward: 132.00\tLast 10 Episodes average reward: 137.20\tRuntime: 4.74\t \n",
      "Episode 413\tLast reward: 97.00\tLast 10 Episodes average reward: 134.30\tRuntime: 3.45\t \n",
      "Episode 414\tLast reward: 84.00\tLast 10 Episodes average reward: 126.70\tRuntime: 2.99\t \n",
      "Episode 415\tLast reward: 138.00\tLast 10 Episodes average reward: 127.40\tRuntime: 5.04\t \n",
      "Episode 416\tLast reward: 143.00\tLast 10 Episodes average reward: 125.80\tRuntime: 5.10\t \n",
      "Episode 417\tLast reward: 135.00\tLast 10 Episodes average reward: 126.00\tRuntime: 4.80\t \n",
      "Episode 418\tLast reward: 144.00\tLast 10 Episodes average reward: 126.60\tRuntime: 5.21\t \n",
      "Episode 419\tLast reward: 129.00\tLast 10 Episodes average reward: 126.40\tRuntime: 4.60\t \n",
      "Episode 420\tLast reward: 124.00\tLast 10 Episodes average reward: 125.80\tRuntime: 4.40\t \n",
      "Episode 421\tLast reward: 123.00\tLast 10 Episodes average reward: 124.90\tRuntime: 4.51\t \n",
      "Episode 422\tLast reward: 131.00\tLast 10 Episodes average reward: 124.80\tRuntime: 4.64\t \n",
      "Episode 423\tLast reward: 127.00\tLast 10 Episodes average reward: 127.80\tRuntime: 4.51\t \n",
      "Episode 424\tLast reward: 133.00\tLast 10 Episodes average reward: 132.70\tRuntime: 4.74\t \n",
      "Episode 425\tLast reward: 118.00\tLast 10 Episodes average reward: 130.70\tRuntime: 4.33\t \n",
      "Episode 426\tLast reward: 112.00\tLast 10 Episodes average reward: 127.60\tRuntime: 3.98\t \n",
      "Episode 427\tLast reward: 124.00\tLast 10 Episodes average reward: 126.50\tRuntime: 4.39\t \n",
      "Episode 428\tLast reward: 124.00\tLast 10 Episodes average reward: 124.50\tRuntime: 4.55\t \n",
      "Episode 429\tLast reward: 120.00\tLast 10 Episodes average reward: 123.60\tRuntime: 4.25\t \n",
      "Episode 430\tLast reward: 136.00\tLast 10 Episodes average reward: 124.80\tRuntime: 4.85\t \n",
      "Episode 431\tLast reward: 128.00\tLast 10 Episodes average reward: 125.30\tRuntime: 4.66\t \n",
      "Episode 432\tLast reward: 126.00\tLast 10 Episodes average reward: 124.80\tRuntime: 4.46\t \n",
      "Episode 433\tLast reward: 124.00\tLast 10 Episodes average reward: 124.50\tRuntime: 4.40\t \n",
      "Episode 434\tLast reward: 134.00\tLast 10 Episodes average reward: 124.60\tRuntime: 4.83\t \n",
      "Episode 435\tLast reward: 134.00\tLast 10 Episodes average reward: 126.20\tRuntime: 4.85\t \n",
      "Episode 436\tLast reward: 122.00\tLast 10 Episodes average reward: 127.20\tRuntime: 4.34\t \n",
      "Episode 437\tLast reward: 126.00\tLast 10 Episodes average reward: 127.40\tRuntime: 4.45\t \n",
      "Episode 438\tLast reward: 120.00\tLast 10 Episodes average reward: 127.00\tRuntime: 4.36\t \n",
      "Episode 439\tLast reward: 133.00\tLast 10 Episodes average reward: 128.30\tRuntime: 4.72\t \n",
      "Episode 440\tLast reward: 142.00\tLast 10 Episodes average reward: 128.90\tRuntime: 5.05\t \n",
      "Episode 441\tLast reward: 127.00\tLast 10 Episodes average reward: 128.80\tRuntime: 4.61\t \n",
      "Episode 442\tLast reward: 134.00\tLast 10 Episodes average reward: 129.60\tRuntime: 4.78\t \n",
      "Episode 443\tLast reward: 113.00\tLast 10 Episodes average reward: 128.50\tRuntime: 4.02\t \n",
      "Episode 444\tLast reward: 104.00\tLast 10 Episodes average reward: 125.50\tRuntime: 3.68\t \n",
      "Episode 445\tLast reward: 131.00\tLast 10 Episodes average reward: 125.20\tRuntime: 4.73\t \n",
      "Episode 446\tLast reward: 125.00\tLast 10 Episodes average reward: 125.50\tRuntime: 4.43\t \n",
      "Episode 447\tLast reward: 124.00\tLast 10 Episodes average reward: 125.30\tRuntime: 4.43\t \n",
      "Episode 448\tLast reward: 114.00\tLast 10 Episodes average reward: 124.70\tRuntime: 4.15\t \n",
      "Episode 449\tLast reward: 129.00\tLast 10 Episodes average reward: 124.30\tRuntime: 4.55\t \n",
      "Episode 450\tLast reward: 127.00\tLast 10 Episodes average reward: 122.80\tRuntime: 4.52\t \n",
      "Episode 451\tLast reward: 153.00\tLast 10 Episodes average reward: 125.40\tRuntime: 5.54\t \n",
      "Episode 452\tLast reward: 139.00\tLast 10 Episodes average reward: 125.90\tRuntime: 4.97\t \n",
      "Episode 453\tLast reward: 147.00\tLast 10 Episodes average reward: 129.30\tRuntime: 5.24\t \n",
      "Episode 454\tLast reward: 153.00\tLast 10 Episodes average reward: 134.20\tRuntime: 5.54\t \n",
      "Episode 455\tLast reward: 133.00\tLast 10 Episodes average reward: 134.40\tRuntime: 4.76\t \n",
      "Episode 456\tLast reward: 137.00\tLast 10 Episodes average reward: 135.60\tRuntime: 4.99\t \n",
      "Episode 457\tLast reward: 138.00\tLast 10 Episodes average reward: 137.00\tRuntime: 4.88\t \n",
      "Episode 458\tLast reward: 157.00\tLast 10 Episodes average reward: 141.30\tRuntime: 5.56\t \n",
      "Episode 459\tLast reward: 169.00\tLast 10 Episodes average reward: 145.30\tRuntime: 6.13\t \n",
      "Episode 460\tLast reward: 164.00\tLast 10 Episodes average reward: 149.00\tRuntime: 5.82\t \n",
      "Episode 461\tLast reward: 156.00\tLast 10 Episodes average reward: 149.30\tRuntime: 5.58\t \n",
      "Episode 462\tLast reward: 191.00\tLast 10 Episodes average reward: 154.50\tRuntime: 6.86\t \n",
      "Episode 463\tLast reward: 167.00\tLast 10 Episodes average reward: 156.50\tRuntime: 5.94\t \n",
      "Episode 464\tLast reward: 148.00\tLast 10 Episodes average reward: 156.00\tRuntime: 5.40\t \n",
      "Episode 465\tLast reward: 162.00\tLast 10 Episodes average reward: 158.90\tRuntime: 5.75\t \n",
      "Episode 466\tLast reward: 165.00\tLast 10 Episodes average reward: 161.70\tRuntime: 5.88\t \n",
      "Episode 467\tLast reward: 200.00\tLast 10 Episodes average reward: 167.90\tRuntime: 7.22\t \n",
      "Episode 468\tLast reward: 217.00\tLast 10 Episodes average reward: 173.90\tRuntime: 7.73\t \n",
      "Episode 469\tLast reward: 183.00\tLast 10 Episodes average reward: 175.30\tRuntime: 6.64\t \n",
      "Episode 470\tLast reward: 256.00\tLast 10 Episodes average reward: 184.50\tRuntime: 9.09\t \n",
      "Episode 471\tLast reward: 281.00\tLast 10 Episodes average reward: 197.00\tRuntime: 10.23\t \n",
      "Episode 472\tLast reward: 281.00\tLast 10 Episodes average reward: 197.00\tRuntime: 18.12\t \n",
      "Episode 473\tLast reward: 335.00\tLast 10 Episodes average reward: 211.40\tRuntime: 12.38\t \n",
      "Episode 474\tLast reward: 275.00\tLast 10 Episodes average reward: 222.20\tRuntime: 10.12\t \n",
      "Episode 475\tLast reward: 245.00\tLast 10 Episodes average reward: 231.90\tRuntime: 8.83\t \n",
      "Episode 476\tLast reward: 194.00\tLast 10 Episodes average reward: 235.10\tRuntime: 7.08\t \n",
      "Episode 477\tLast reward: 161.00\tLast 10 Episodes average reward: 234.70\tRuntime: 5.79\t \n",
      "Episode 478\tLast reward: 182.00\tLast 10 Episodes average reward: 232.90\tRuntime: 6.66\t \n",
      "Episode 479\tLast reward: 150.00\tLast 10 Episodes average reward: 226.20\tRuntime: 5.38\t \n",
      "Episode 480\tLast reward: 175.00\tLast 10 Episodes average reward: 225.40\tRuntime: 6.35\t \n",
      "Episode 481\tLast reward: 149.00\tLast 10 Episodes average reward: 214.70\tRuntime: 5.32\t \n",
      "Episode 482\tLast reward: 167.00\tLast 10 Episodes average reward: 203.30\tRuntime: 5.93\t \n",
      "Episode 483\tLast reward: 150.00\tLast 10 Episodes average reward: 184.80\tRuntime: 5.49\t \n",
      "Episode 484\tLast reward: 131.00\tLast 10 Episodes average reward: 170.40\tRuntime: 4.68\t \n",
      "Episode 485\tLast reward: 138.00\tLast 10 Episodes average reward: 159.70\tRuntime: 4.92\t \n",
      "Episode 486\tLast reward: 152.00\tLast 10 Episodes average reward: 155.50\tRuntime: 5.50\t \n",
      "Episode 487\tLast reward: 143.00\tLast 10 Episodes average reward: 153.70\tRuntime: 5.12\t \n",
      "Episode 488\tLast reward: 141.00\tLast 10 Episodes average reward: 149.60\tRuntime: 5.04\t \n",
      "Episode 489\tLast reward: 162.00\tLast 10 Episodes average reward: 150.80\tRuntime: 5.89\t \n",
      "Episode 490\tLast reward: 139.00\tLast 10 Episodes average reward: 147.20\tRuntime: 4.98\t \n",
      "Episode 491\tLast reward: 137.00\tLast 10 Episodes average reward: 146.00\tRuntime: 4.90\t \n",
      "Episode 492\tLast reward: 132.00\tLast 10 Episodes average reward: 142.50\tRuntime: 4.82\t \n",
      "Episode 493\tLast reward: 123.00\tLast 10 Episodes average reward: 139.80\tRuntime: 4.40\t \n",
      "Episode 494\tLast reward: 122.00\tLast 10 Episodes average reward: 138.90\tRuntime: 4.35\t \n",
      "Episode 495\tLast reward: 139.00\tLast 10 Episodes average reward: 139.00\tRuntime: 5.07\t \n",
      "Episode 496\tLast reward: 119.00\tLast 10 Episodes average reward: 135.70\tRuntime: 4.23\t \n",
      "Episode 497\tLast reward: 122.00\tLast 10 Episodes average reward: 133.60\tRuntime: 4.34\t \n",
      "Episode 498\tLast reward: 128.00\tLast 10 Episodes average reward: 132.30\tRuntime: 4.70\t \n",
      "Episode 499\tLast reward: 126.00\tLast 10 Episodes average reward: 128.70\tRuntime: 4.63\t \n",
      "Episode 500\tLast reward: 110.00\tLast 10 Episodes average reward: 125.80\tRuntime: 3.97\t \n",
      "Episode 501\tLast reward: 110.00\tLast 10 Episodes average reward: 123.10\tRuntime: 3.96\t \n",
      "Episode 502\tLast reward: 112.00\tLast 10 Episodes average reward: 121.10\tRuntime: 4.19\t \n",
      "Episode 503\tLast reward: 119.00\tLast 10 Episodes average reward: 120.70\tRuntime: 4.23\t \n",
      "Episode 504\tLast reward: 56.00\tLast 10 Episodes average reward: 114.10\tRuntime: 2.10\t \n",
      "Episode 505\tLast reward: 118.00\tLast 10 Episodes average reward: 112.00\tRuntime: 4.21\t \n",
      "Episode 506\tLast reward: 121.00\tLast 10 Episodes average reward: 112.20\tRuntime: 4.49\t \n",
      "Episode 507\tLast reward: 115.00\tLast 10 Episodes average reward: 111.50\tRuntime: 4.16\t \n",
      "Episode 508\tLast reward: 105.00\tLast 10 Episodes average reward: 109.20\tRuntime: 3.81\t \n",
      "Episode 509\tLast reward: 127.00\tLast 10 Episodes average reward: 109.30\tRuntime: 4.61\t \n",
      "Episode 510\tLast reward: 107.00\tLast 10 Episodes average reward: 109.00\tRuntime: 3.99\t \n",
      "Episode 511\tLast reward: 121.00\tLast 10 Episodes average reward: 110.10\tRuntime: 4.36\t \n",
      "Episode 512\tLast reward: 135.00\tLast 10 Episodes average reward: 112.40\tRuntime: 4.95\t \n",
      "Episode 513\tLast reward: 115.00\tLast 10 Episodes average reward: 112.00\tRuntime: 4.28\t \n",
      "Episode 514\tLast reward: 97.00\tLast 10 Episodes average reward: 116.10\tRuntime: 3.51\t \n",
      "Episode 515\tLast reward: 137.00\tLast 10 Episodes average reward: 118.00\tRuntime: 4.95\t \n",
      "Episode 516\tLast reward: 125.00\tLast 10 Episodes average reward: 118.40\tRuntime: 4.58\t \n",
      "Episode 517\tLast reward: 142.00\tLast 10 Episodes average reward: 121.10\tRuntime: 5.20\t \n",
      "Episode 518\tLast reward: 136.00\tLast 10 Episodes average reward: 124.20\tRuntime: 4.94\t \n",
      "Episode 519\tLast reward: 111.00\tLast 10 Episodes average reward: 122.60\tRuntime: 4.01\t \n",
      "Episode 520\tLast reward: 14.00\tLast 10 Episodes average reward: 113.30\tRuntime: 0.63\t \n",
      "Episode 521\tLast reward: 147.00\tLast 10 Episodes average reward: 115.90\tRuntime: 5.27\t \n",
      "Episode 522\tLast reward: 118.00\tLast 10 Episodes average reward: 114.20\tRuntime: 4.32\t \n",
      "Episode 523\tLast reward: 178.00\tLast 10 Episodes average reward: 120.50\tRuntime: 6.46\t \n",
      "Episode 524\tLast reward: 102.00\tLast 10 Episodes average reward: 121.00\tRuntime: 3.85\t \n",
      "Episode 525\tLast reward: 117.00\tLast 10 Episodes average reward: 119.00\tRuntime: 4.24\t \n",
      "Episode 526\tLast reward: 105.00\tLast 10 Episodes average reward: 117.00\tRuntime: 3.78\t \n",
      "Episode 527\tLast reward: 119.00\tLast 10 Episodes average reward: 114.70\tRuntime: 4.38\t \n",
      "Episode 528\tLast reward: 114.00\tLast 10 Episodes average reward: 112.50\tRuntime: 4.10\t \n",
      "Episode 529\tLast reward: 142.00\tLast 10 Episodes average reward: 115.60\tRuntime: 5.08\t \n",
      "Episode 530\tLast reward: 115.00\tLast 10 Episodes average reward: 125.70\tRuntime: 4.18\t \n",
      "Episode 531\tLast reward: 121.00\tLast 10 Episodes average reward: 123.10\tRuntime: 4.42\t \n",
      "Episode 532\tLast reward: 137.00\tLast 10 Episodes average reward: 125.00\tRuntime: 4.93\t \n",
      "Episode 533\tLast reward: 130.00\tLast 10 Episodes average reward: 120.20\tRuntime: 4.67\t \n",
      "Episode 534\tLast reward: 119.00\tLast 10 Episodes average reward: 121.90\tRuntime: 4.37\t \n",
      "Episode 535\tLast reward: 122.00\tLast 10 Episodes average reward: 122.40\tRuntime: 4.37\t \n",
      "Episode 536\tLast reward: 129.00\tLast 10 Episodes average reward: 124.80\tRuntime: 4.62\t \n",
      "Episode 537\tLast reward: 149.00\tLast 10 Episodes average reward: 127.80\tRuntime: 5.48\t \n",
      "Episode 538\tLast reward: 130.00\tLast 10 Episodes average reward: 129.40\tRuntime: 4.72\t \n",
      "Episode 539\tLast reward: 160.00\tLast 10 Episodes average reward: 131.20\tRuntime: 5.76\t \n",
      "Episode 540\tLast reward: 136.00\tLast 10 Episodes average reward: 133.30\tRuntime: 5.03\t \n",
      "Episode 541\tLast reward: 127.00\tLast 10 Episodes average reward: 133.90\tRuntime: 4.57\t \n",
      "Episode 542\tLast reward: 134.00\tLast 10 Episodes average reward: 133.60\tRuntime: 4.83\t \n",
      "Episode 543\tLast reward: 145.00\tLast 10 Episodes average reward: 135.10\tRuntime: 5.34\t \n",
      "Episode 544\tLast reward: 116.00\tLast 10 Episodes average reward: 134.80\tRuntime: 4.20\t \n",
      "Episode 545\tLast reward: 121.00\tLast 10 Episodes average reward: 134.70\tRuntime: 4.35\t \n",
      "Episode 546\tLast reward: 119.00\tLast 10 Episodes average reward: 133.70\tRuntime: 4.28\t \n",
      "Episode 547\tLast reward: 124.00\tLast 10 Episodes average reward: 131.20\tRuntime: 4.55\t \n",
      "Episode 548\tLast reward: 130.00\tLast 10 Episodes average reward: 131.20\tRuntime: 4.68\t \n",
      "Episode 549\tLast reward: 116.00\tLast 10 Episodes average reward: 126.80\tRuntime: 4.17\t \n",
      "Episode 550\tLast reward: 143.00\tLast 10 Episodes average reward: 127.50\tRuntime: 5.25\t \n",
      "Episode 551\tLast reward: 129.00\tLast 10 Episodes average reward: 127.70\tRuntime: 4.66\t \n",
      "Episode 552\tLast reward: 124.00\tLast 10 Episodes average reward: 126.70\tRuntime: 4.48\t \n",
      "Episode 553\tLast reward: 104.00\tLast 10 Episodes average reward: 122.60\tRuntime: 3.86\t \n",
      "Episode 554\tLast reward: 120.00\tLast 10 Episodes average reward: 123.00\tRuntime: 4.32\t \n",
      "Episode 555\tLast reward: 112.00\tLast 10 Episodes average reward: 122.10\tRuntime: 4.03\t \n",
      "Episode 556\tLast reward: 114.00\tLast 10 Episodes average reward: 121.60\tRuntime: 4.10\t \n",
      "Episode 557\tLast reward: 114.00\tLast 10 Episodes average reward: 120.60\tRuntime: 4.21\t \n",
      "Episode 558\tLast reward: 115.00\tLast 10 Episodes average reward: 119.10\tRuntime: 4.16\t \n",
      "Episode 559\tLast reward: 119.00\tLast 10 Episodes average reward: 119.40\tRuntime: 4.29\t \n",
      "Episode 560\tLast reward: 66.00\tLast 10 Episodes average reward: 111.70\tRuntime: 2.43\t \n",
      "Episode 561\tLast reward: 75.00\tLast 10 Episodes average reward: 106.30\tRuntime: 2.79\t \n",
      "Episode 562\tLast reward: 102.00\tLast 10 Episodes average reward: 104.10\tRuntime: 3.66\t \n",
      "Episode 563\tLast reward: 107.00\tLast 10 Episodes average reward: 104.40\tRuntime: 3.86\t \n",
      "Episode 564\tLast reward: 122.00\tLast 10 Episodes average reward: 104.60\tRuntime: 4.39\t \n",
      "Episode 565\tLast reward: 116.00\tLast 10 Episodes average reward: 105.00\tRuntime: 4.30\t \n",
      "Episode 566\tLast reward: 116.00\tLast 10 Episodes average reward: 105.20\tRuntime: 4.19\t \n",
      "Episode 567\tLast reward: 125.00\tLast 10 Episodes average reward: 106.30\tRuntime: 4.49\t \n",
      "Episode 568\tLast reward: 111.00\tLast 10 Episodes average reward: 105.90\tRuntime: 4.04\t \n",
      "Episode 569\tLast reward: 102.00\tLast 10 Episodes average reward: 104.20\tRuntime: 3.78\t \n",
      "Episode 570\tLast reward: 111.00\tLast 10 Episodes average reward: 108.70\tRuntime: 3.99\t \n",
      "Episode 571\tLast reward: 111.00\tLast 10 Episodes average reward: 112.30\tRuntime: 4.00\t \n",
      "Episode 572\tLast reward: 115.00\tLast 10 Episodes average reward: 113.60\tRuntime: 4.13\t \n",
      "Episode 573\tLast reward: 123.00\tLast 10 Episodes average reward: 115.20\tRuntime: 4.54\t \n",
      "Episode 574\tLast reward: 117.00\tLast 10 Episodes average reward: 114.70\tRuntime: 4.25\t \n",
      "Episode 575\tLast reward: 103.00\tLast 10 Episodes average reward: 113.40\tRuntime: 3.73\t \n",
      "Episode 576\tLast reward: 113.00\tLast 10 Episodes average reward: 113.10\tRuntime: 4.18\t \n",
      "Episode 577\tLast reward: 108.00\tLast 10 Episodes average reward: 111.40\tRuntime: 3.90\t \n",
      "Episode 578\tLast reward: 125.00\tLast 10 Episodes average reward: 112.80\tRuntime: 4.50\t \n",
      "Episode 579\tLast reward: 110.00\tLast 10 Episodes average reward: 113.60\tRuntime: 3.98\t \n",
      "Episode 580\tLast reward: 128.00\tLast 10 Episodes average reward: 115.30\tRuntime: 4.71\t \n",
      "Episode 581\tLast reward: 109.00\tLast 10 Episodes average reward: 115.10\tRuntime: 3.97\t \n",
      "Episode 582\tLast reward: 113.00\tLast 10 Episodes average reward: 114.90\tRuntime: 4.07\t \n",
      "Episode 583\tLast reward: 116.00\tLast 10 Episodes average reward: 114.20\tRuntime: 4.29\t \n",
      "Episode 584\tLast reward: 129.00\tLast 10 Episodes average reward: 115.40\tRuntime: 4.70\t \n",
      "Episode 585\tLast reward: 108.00\tLast 10 Episodes average reward: 115.90\tRuntime: 3.93\t \n",
      "Episode 586\tLast reward: 110.00\tLast 10 Episodes average reward: 115.60\tRuntime: 3.99\t \n",
      "Episode 587\tLast reward: 108.00\tLast 10 Episodes average reward: 115.60\tRuntime: 4.23\t \n",
      "Episode 588\tLast reward: 116.00\tLast 10 Episodes average reward: 114.70\tRuntime: 4.18\t \n",
      "Episode 589\tLast reward: 116.00\tLast 10 Episodes average reward: 115.30\tRuntime: 4.24\t \n",
      "Episode 590\tLast reward: 117.00\tLast 10 Episodes average reward: 114.20\tRuntime: 4.27\t \n",
      "Episode 591\tLast reward: 117.00\tLast 10 Episodes average reward: 115.00\tRuntime: 4.34\t \n",
      "Episode 592\tLast reward: 118.00\tLast 10 Episodes average reward: 115.50\tRuntime: 4.27\t \n",
      "Episode 593\tLast reward: 115.00\tLast 10 Episodes average reward: 115.40\tRuntime: 4.13\t \n",
      "Episode 594\tLast reward: 114.00\tLast 10 Episodes average reward: 113.90\tRuntime: 4.26\t \n",
      "Episode 595\tLast reward: 110.00\tLast 10 Episodes average reward: 114.10\tRuntime: 4.04\t \n",
      "Episode 596\tLast reward: 120.00\tLast 10 Episodes average reward: 115.10\tRuntime: 4.39\t \n",
      "Episode 597\tLast reward: 113.00\tLast 10 Episodes average reward: 115.60\tRuntime: 4.14\t \n",
      "Episode 598\tLast reward: 114.00\tLast 10 Episodes average reward: 115.40\tRuntime: 4.17\t \n",
      "Episode 599\tLast reward: 109.00\tLast 10 Episodes average reward: 114.70\tRuntime: 4.02\t \n",
      "Episode 600\tLast reward: 122.00\tLast 10 Episodes average reward: 115.20\tRuntime: 4.36\t \n",
      "Episode 601\tLast reward: 111.00\tLast 10 Episodes average reward: 114.60\tRuntime: 4.05\t \n",
      "Episode 602\tLast reward: 117.00\tLast 10 Episodes average reward: 114.50\tRuntime: 4.29\t \n",
      "Episode 603\tLast reward: 116.00\tLast 10 Episodes average reward: 114.60\tRuntime: 4.22\t \n",
      "Episode 604\tLast reward: 119.00\tLast 10 Episodes average reward: 115.10\tRuntime: 4.31\t \n",
      "Episode 605\tLast reward: 131.00\tLast 10 Episodes average reward: 117.20\tRuntime: 4.93\t \n",
      "Episode 606\tLast reward: 139.00\tLast 10 Episodes average reward: 119.10\tRuntime: 5.05\t \n",
      "Episode 607\tLast reward: 138.00\tLast 10 Episodes average reward: 121.60\tRuntime: 5.07\t \n",
      "Episode 608\tLast reward: 137.00\tLast 10 Episodes average reward: 123.90\tRuntime: 5.11\t \n",
      "Episode 609\tLast reward: 143.00\tLast 10 Episodes average reward: 127.30\tRuntime: 5.12\t \n",
      "Episode 610\tLast reward: 150.00\tLast 10 Episodes average reward: 130.10\tRuntime: 5.41\t \n",
      "Episode 611\tLast reward: 148.00\tLast 10 Episodes average reward: 133.80\tRuntime: 5.59\t \n",
      "Episode 612\tLast reward: 160.00\tLast 10 Episodes average reward: 138.10\tRuntime: 5.80\t \n",
      "Episode 613\tLast reward: 150.00\tLast 10 Episodes average reward: 141.50\tRuntime: 5.59\t \n",
      "Episode 614\tLast reward: 156.00\tLast 10 Episodes average reward: 145.20\tRuntime: 5.72\t \n",
      "Episode 615\tLast reward: 163.00\tLast 10 Episodes average reward: 148.40\tRuntime: 5.89\t \n",
      "Episode 616\tLast reward: 156.00\tLast 10 Episodes average reward: 150.10\tRuntime: 5.87\t \n",
      "Episode 617\tLast reward: 194.00\tLast 10 Episodes average reward: 155.70\tRuntime: 7.03\t \n",
      "Episode 618\tLast reward: 181.00\tLast 10 Episodes average reward: 160.10\tRuntime: 6.78\t \n",
      "Episode 619\tLast reward: 157.00\tLast 10 Episodes average reward: 161.50\tRuntime: 5.74\t \n",
      "Episode 620\tLast reward: 207.00\tLast 10 Episodes average reward: 167.20\tRuntime: 7.50\t \n",
      "Episode 621\tLast reward: 190.00\tLast 10 Episodes average reward: 171.40\tRuntime: 7.03\t \n",
      "Episode 622\tLast reward: 175.00\tLast 10 Episodes average reward: 172.90\tRuntime: 6.33\t \n",
      "Episode 623\tLast reward: 219.00\tLast 10 Episodes average reward: 179.80\tRuntime: 7.96\t \n",
      "Episode 624\tLast reward: 217.00\tLast 10 Episodes average reward: 185.90\tRuntime: 7.92\t \n",
      "Episode 625\tLast reward: 225.00\tLast 10 Episodes average reward: 192.10\tRuntime: 8.43\t \n",
      "Episode 626\tLast reward: 246.00\tLast 10 Episodes average reward: 201.10\tRuntime: 9.06\t \n",
      "Episode 627\tLast reward: 209.00\tLast 10 Episodes average reward: 202.60\tRuntime: 7.52\t \n",
      "Episode 628\tLast reward: 274.00\tLast 10 Episodes average reward: 211.90\tRuntime: 10.23\t \n",
      "Episode 629\tLast reward: 279.00\tLast 10 Episodes average reward: 224.10\tRuntime: 10.25\t \n",
      "Episode 630\tLast reward: 279.00\tLast 10 Episodes average reward: 224.10\tRuntime: 18.51\t \n",
      "Episode 631\tLast reward: 307.00\tLast 10 Episodes average reward: 234.10\tRuntime: 11.70\t \n",
      "Episode 632\tLast reward: 307.00\tLast 10 Episodes average reward: 234.10\tRuntime: 18.62\t \n",
      "Episode 633\tLast reward: 307.00\tLast 10 Episodes average reward: 234.10\tRuntime: 19.04\t \n",
      "Episode 634\tLast reward: 307.00\tLast 10 Episodes average reward: 234.10\tRuntime: 18.81\t \n",
      "Episode 635\tLast reward: 359.00\tLast 10 Episodes average reward: 251.00\tRuntime: 13.66\t \n",
      "Episode 636\tLast reward: 307.00\tLast 10 Episodes average reward: 264.20\tRuntime: 11.40\t \n",
      "Episode 637\tLast reward: 391.00\tLast 10 Episodes average reward: 281.40\tRuntime: 14.35\t \n",
      "Episode 638\tLast reward: 379.00\tLast 10 Episodes average reward: 297.60\tRuntime: 14.13\t \n",
      "Episode 639\tLast reward: 294.00\tLast 10 Episodes average reward: 304.50\tRuntime: 11.12\t \n",
      "Episode 640\tLast reward: 404.00\tLast 10 Episodes average reward: 320.30\tRuntime: 15.05\t \n",
      "Episode 641\tLast reward: 276.00\tLast 10 Episodes average reward: 327.00\tRuntime: 10.32\t \n",
      "Episode 642\tLast reward: 321.00\tLast 10 Episodes average reward: 331.70\tRuntime: 11.98\t \n",
      "Episode 643\tLast reward: 297.00\tLast 10 Episodes average reward: 333.50\tRuntime: 11.10\t \n",
      "Episode 644\tLast reward: 305.00\tLast 10 Episodes average reward: 333.30\tRuntime: 11.35\t \n",
      "Episode 645\tLast reward: 407.00\tLast 10 Episodes average reward: 338.10\tRuntime: 15.13\t \n",
      "Episode 646\tLast reward: 276.00\tLast 10 Episodes average reward: 335.00\tRuntime: 10.34\t \n",
      "Episode 647\tLast reward: 233.00\tLast 10 Episodes average reward: 319.20\tRuntime: 8.62\t \n",
      "Episode 648\tLast reward: 314.00\tLast 10 Episodes average reward: 312.70\tRuntime: 11.52\t \n",
      "Episode 649\tLast reward: 274.00\tLast 10 Episodes average reward: 310.70\tRuntime: 10.14\t \n",
      "Episode 650\tLast reward: 269.00\tLast 10 Episodes average reward: 297.20\tRuntime: 10.02\t \n",
      "Episode 651\tLast reward: 266.00\tLast 10 Episodes average reward: 296.20\tRuntime: 9.90\t \n",
      "Episode 652\tLast reward: 258.00\tLast 10 Episodes average reward: 289.90\tRuntime: 9.57\t \n",
      "Episode 653\tLast reward: 200.00\tLast 10 Episodes average reward: 280.20\tRuntime: 7.36\t \n",
      "Episode 654\tLast reward: 191.00\tLast 10 Episodes average reward: 268.80\tRuntime: 6.95\t \n",
      "Episode 655\tLast reward: 210.00\tLast 10 Episodes average reward: 249.10\tRuntime: 7.68\t \n",
      "Episode 656\tLast reward: 227.00\tLast 10 Episodes average reward: 244.20\tRuntime: 8.37\t \n",
      "Episode 657\tLast reward: 199.00\tLast 10 Episodes average reward: 240.80\tRuntime: 7.46\t \n",
      "Episode 658\tLast reward: 171.00\tLast 10 Episodes average reward: 226.50\tRuntime: 6.37\t \n",
      "Episode 659\tLast reward: 188.00\tLast 10 Episodes average reward: 217.90\tRuntime: 6.96\t \n",
      "Episode 660\tLast reward: 216.00\tLast 10 Episodes average reward: 212.60\tRuntime: 7.95\t \n",
      "Episode 661\tLast reward: 161.00\tLast 10 Episodes average reward: 202.10\tRuntime: 5.98\t \n",
      "Episode 662\tLast reward: 169.00\tLast 10 Episodes average reward: 193.20\tRuntime: 6.23\t \n",
      "Episode 663\tLast reward: 166.00\tLast 10 Episodes average reward: 189.80\tRuntime: 6.04\t \n",
      "Episode 664\tLast reward: 194.00\tLast 10 Episodes average reward: 190.10\tRuntime: 7.16\t \n",
      "Episode 665\tLast reward: 206.00\tLast 10 Episodes average reward: 189.70\tRuntime: 7.52\t \n",
      "Episode 666\tLast reward: 192.00\tLast 10 Episodes average reward: 186.20\tRuntime: 7.02\t \n",
      "Episode 667\tLast reward: 212.00\tLast 10 Episodes average reward: 187.50\tRuntime: 7.78\t \n",
      "Episode 668\tLast reward: 208.00\tLast 10 Episodes average reward: 191.20\tRuntime: 7.71\t \n",
      "Episode 669\tLast reward: 187.00\tLast 10 Episodes average reward: 191.10\tRuntime: 6.85\t \n",
      "Episode 670\tLast reward: 209.00\tLast 10 Episodes average reward: 190.40\tRuntime: 7.64\t \n",
      "Episode 671\tLast reward: 247.00\tLast 10 Episodes average reward: 199.00\tRuntime: 9.10\t \n",
      "Episode 672\tLast reward: 260.00\tLast 10 Episodes average reward: 208.10\tRuntime: 9.69\t \n",
      "Episode 673\tLast reward: 298.00\tLast 10 Episodes average reward: 221.30\tRuntime: 11.05\t \n",
      "Episode 674\tLast reward: 298.00\tLast 10 Episodes average reward: 221.30\tRuntime: 18.65\t \n",
      "Episode 675\tLast reward: 425.00\tLast 10 Episodes average reward: 244.40\tRuntime: 16.10\t \n",
      "Episode 676\tLast reward: 425.00\tLast 10 Episodes average reward: 244.40\tRuntime: 18.72\t \n",
      "Episode 677\tLast reward: 425.00\tLast 10 Episodes average reward: 244.40\tRuntime: 18.85\t \n",
      "Episode 678\tLast reward: 425.00\tLast 10 Episodes average reward: 244.40\tRuntime: 19.12\t \n",
      "Episode 679\tLast reward: 425.00\tLast 10 Episodes average reward: 244.40\tRuntime: 18.96\t \n",
      "Episode 680\tLast reward: 278.00\tLast 10 Episodes average reward: 251.60\tRuntime: 10.70\t \n",
      "Episode 681\tLast reward: 328.00\tLast 10 Episodes average reward: 265.20\tRuntime: 12.22\t \n",
      "Episode 682\tLast reward: 289.00\tLast 10 Episodes average reward: 272.90\tRuntime: 10.86\t \n",
      "Episode 683\tLast reward: 201.00\tLast 10 Episodes average reward: 272.20\tRuntime: 7.42\t \n",
      "Episode 684\tLast reward: 209.00\tLast 10 Episodes average reward: 274.40\tRuntime: 7.78\t \n",
      "Episode 685\tLast reward: 188.00\tLast 10 Episodes average reward: 272.30\tRuntime: 6.89\t \n",
      "Episode 686\tLast reward: 210.00\tLast 10 Episodes average reward: 268.60\tRuntime: 7.76\t \n",
      "Episode 687\tLast reward: 178.00\tLast 10 Episodes average reward: 260.40\tRuntime: 6.52\t \n",
      "Episode 688\tLast reward: 169.00\tLast 10 Episodes average reward: 247.50\tRuntime: 6.28\t \n",
      "Episode 689\tLast reward: 157.00\tLast 10 Episodes average reward: 220.70\tRuntime: 5.75\t \n",
      "Episode 690\tLast reward: 161.00\tLast 10 Episodes average reward: 209.00\tRuntime: 5.90\t \n",
      "Episode 691\tLast reward: 159.00\tLast 10 Episodes average reward: 192.10\tRuntime: 5.90\t \n",
      "Episode 692\tLast reward: 155.00\tLast 10 Episodes average reward: 178.70\tRuntime: 5.58\t \n",
      "Episode 693\tLast reward: 156.00\tLast 10 Episodes average reward: 174.20\tRuntime: 5.76\t \n",
      "Episode 694\tLast reward: 162.00\tLast 10 Episodes average reward: 169.50\tRuntime: 5.89\t \n",
      "Episode 695\tLast reward: 165.00\tLast 10 Episodes average reward: 167.20\tRuntime: 6.00\t \n",
      "Episode 696\tLast reward: 145.00\tLast 10 Episodes average reward: 160.70\tRuntime: 5.44\t \n",
      "Episode 697\tLast reward: 150.00\tLast 10 Episodes average reward: 157.90\tRuntime: 5.47\t \n",
      "Episode 698\tLast reward: 161.00\tLast 10 Episodes average reward: 157.10\tRuntime: 5.93\t \n",
      "Episode 699\tLast reward: 153.00\tLast 10 Episodes average reward: 156.70\tRuntime: 5.60\t \n",
      "Episode 700\tLast reward: 157.00\tLast 10 Episodes average reward: 156.30\tRuntime: 5.71\t \n",
      "Episode 701\tLast reward: 148.00\tLast 10 Episodes average reward: 155.20\tRuntime: 5.50\t \n",
      "Episode 702\tLast reward: 189.00\tLast 10 Episodes average reward: 158.60\tRuntime: 6.88\t \n",
      "Episode 703\tLast reward: 149.00\tLast 10 Episodes average reward: 157.90\tRuntime: 5.44\t \n",
      "Episode 704\tLast reward: 156.00\tLast 10 Episodes average reward: 157.30\tRuntime: 5.78\t \n",
      "Episode 705\tLast reward: 166.00\tLast 10 Episodes average reward: 157.40\tRuntime: 6.05\t \n",
      "Episode 706\tLast reward: 157.00\tLast 10 Episodes average reward: 158.60\tRuntime: 5.79\t \n",
      "Episode 707\tLast reward: 170.00\tLast 10 Episodes average reward: 160.60\tRuntime: 6.17\t \n",
      "Episode 708\tLast reward: 169.00\tLast 10 Episodes average reward: 161.40\tRuntime: 6.16\t \n",
      "Episode 709\tLast reward: 171.00\tLast 10 Episodes average reward: 163.20\tRuntime: 6.41\t \n",
      "Episode 710\tLast reward: 163.00\tLast 10 Episodes average reward: 163.80\tRuntime: 5.98\t \n",
      "Episode 711\tLast reward: 153.00\tLast 10 Episodes average reward: 164.30\tRuntime: 5.71\t \n",
      "Episode 712\tLast reward: 167.00\tLast 10 Episodes average reward: 162.10\tRuntime: 6.12\t \n",
      "Episode 713\tLast reward: 153.00\tLast 10 Episodes average reward: 162.50\tRuntime: 5.60\t \n",
      "Episode 714\tLast reward: 160.00\tLast 10 Episodes average reward: 162.90\tRuntime: 5.92\t \n",
      "Episode 715\tLast reward: 149.00\tLast 10 Episodes average reward: 161.20\tRuntime: 5.58\t \n",
      "Episode 716\tLast reward: 153.00\tLast 10 Episodes average reward: 160.80\tRuntime: 5.65\t \n",
      "Episode 717\tLast reward: 162.00\tLast 10 Episodes average reward: 160.00\tRuntime: 5.81\t \n",
      "Episode 718\tLast reward: 154.00\tLast 10 Episodes average reward: 158.50\tRuntime: 5.63\t \n",
      "Episode 719\tLast reward: 168.00\tLast 10 Episodes average reward: 158.20\tRuntime: 6.28\t \n",
      "Episode 720\tLast reward: 148.00\tLast 10 Episodes average reward: 156.70\tRuntime: 5.41\t \n",
      "Episode 721\tLast reward: 152.00\tLast 10 Episodes average reward: 156.60\tRuntime: 5.64\t \n",
      "Episode 722\tLast reward: 150.00\tLast 10 Episodes average reward: 154.90\tRuntime: 5.47\t \n",
      "Episode 723\tLast reward: 144.00\tLast 10 Episodes average reward: 154.00\tRuntime: 5.31\t \n",
      "Episode 724\tLast reward: 149.00\tLast 10 Episodes average reward: 152.90\tRuntime: 5.48\t \n",
      "Episode 725\tLast reward: 162.00\tLast 10 Episodes average reward: 154.20\tRuntime: 5.88\t \n",
      "Episode 726\tLast reward: 147.00\tLast 10 Episodes average reward: 153.60\tRuntime: 5.26\t \n",
      "Episode 727\tLast reward: 171.00\tLast 10 Episodes average reward: 154.50\tRuntime: 6.29\t \n",
      "Episode 728\tLast reward: 165.00\tLast 10 Episodes average reward: 155.60\tRuntime: 5.92\t \n",
      "Episode 729\tLast reward: 182.00\tLast 10 Episodes average reward: 157.00\tRuntime: 6.74\t \n",
      "Episode 730\tLast reward: 172.00\tLast 10 Episodes average reward: 159.40\tRuntime: 6.27\t \n",
      "Episode 731\tLast reward: 179.00\tLast 10 Episodes average reward: 162.10\tRuntime: 6.44\t \n",
      "Episode 732\tLast reward: 162.00\tLast 10 Episodes average reward: 163.30\tRuntime: 6.02\t \n",
      "Episode 733\tLast reward: 180.00\tLast 10 Episodes average reward: 166.90\tRuntime: 6.47\t \n",
      "Episode 734\tLast reward: 201.00\tLast 10 Episodes average reward: 172.10\tRuntime: 7.47\t \n",
      "Episode 735\tLast reward: 192.00\tLast 10 Episodes average reward: 175.10\tRuntime: 6.99\t \n",
      "Episode 736\tLast reward: 168.00\tLast 10 Episodes average reward: 177.20\tRuntime: 6.16\t \n",
      "Episode 737\tLast reward: 185.00\tLast 10 Episodes average reward: 178.60\tRuntime: 6.79\t \n",
      "Episode 738\tLast reward: 159.00\tLast 10 Episodes average reward: 178.00\tRuntime: 5.84\t \n",
      "Episode 739\tLast reward: 186.00\tLast 10 Episodes average reward: 178.40\tRuntime: 6.88\t \n",
      "Episode 740\tLast reward: 199.00\tLast 10 Episodes average reward: 181.10\tRuntime: 7.26\t \n",
      "Episode 741\tLast reward: 123.00\tLast 10 Episodes average reward: 175.50\tRuntime: 4.60\t \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 52\u001b[0m\n\u001b[0;32m     50\u001b[0m verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     51\u001b[0m reinforce_update \u001b[38;5;241m=\u001b[39m ReinforceUpdate(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose)\n\u001b[1;32m---> 52\u001b[0m reinforce_update\u001b[38;5;241m.\u001b[39mtrain()\n",
      "Cell \u001b[1;32mIn[11], line 164\u001b[0m, in \u001b[0;36mReinforceUpdate.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_episodes):\n\u001b[0;32m    163\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_trajectory()\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_policy()\n\u001b[0;32m    166\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[1;32mIn[11], line 32\u001b[0m, in \u001b[0;36mReinforceUpdate.get_trajectory\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     31\u001b[0m     state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(state)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m---> 32\u001b[0m action, log_prob, _, \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpqc\u001b[38;5;241m.\u001b[39msample(state_tensor)\n\u001b[0;32m     33\u001b[0m state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msaved_log_probs\u001b[38;5;241m.\u001b[39mappend(log_prob)\n",
      "Cell \u001b[1;32mIn[5], line 20\u001b[0m, in \u001b[0;36mQuantumPolicyModel.sample\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m     17\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m    Samples an action from the action probability distribution aka policy\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     policy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(inputs)\n\u001b[0;32m     21\u001b[0m     dist \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mCategorical(policy)\n\u001b[0;32m     22\u001b[0m     action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n",
      "Cell \u001b[1;32mIn[5], line 12\u001b[0m, in \u001b[0;36mQuantumPolicyModel.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m      9\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m    Input state is fed to the circuit - its output is then fed to the post processing \u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcircuit\u001b[38;5;241m.\u001b[39minput(inputs)\n\u001b[0;32m     13\u001b[0m     probs_processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39minput(probs)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m probs_processed\n",
      "Cell \u001b[1;32mIn[3], line 56\u001b[0m, in \u001b[0;36mCircuitGenerator.input\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minput\u001b[39m(\u001b[38;5;28mself\u001b[39m,inputs):\n\u001b[1;32m---> 56\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcircuit(inputs)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\pennylane\\qnn\\torch.py:402\u001b[0m, in \u001b[0;36mTorchLayer.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    399\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mreshape(inputs, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m    401\u001b[0m \u001b[38;5;66;03m# calculate the forward pass as usual\u001b[39;00m\n\u001b[1;32m--> 402\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate_qnode(inputs)\n\u001b[0;32m    404\u001b[0m \u001b[38;5;66;03m# reshape to the correct number of batch dims\u001b[39;00m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_batch_dim:\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\pennylane\\qnn\\torch.py:431\u001b[0m, in \u001b[0;36mTorchLayer._evaluate_qnode\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    429\u001b[0m     res \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mreshape(r, (x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m res]\n\u001b[1;32m--> 431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mhstack(res)\u001b[38;5;241m.\u001b[39mtype(x\u001b[38;5;241m.\u001b[39mdtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_qubits = 4\n",
    "n_layers = 4      #set to 1 if data_reuploading is off\n",
    "shots = None\n",
    "input_scaling = True\n",
    "design = 'jerbi_circuit' \n",
    "diff_method = 'backprop' \n",
    "weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "input_init = torch.nn.init.ones_\n",
    "measure = measure_expval_pairs\n",
    "measure_qubits = None\n",
    "circuit = CircuitGenerator(n_qubits, \n",
    "                           n_layers,\n",
    "                           shots,\n",
    "                           input_scaling,\n",
    "                           design,\n",
    "                           diff_method,\n",
    "                           weight_init,\n",
    "                           input_init,\n",
    "                           measure,\n",
    "                           measure_qubits)\n",
    "\n",
    "\n",
    "n_actions = 2\n",
    "post_processing = 'softmax'\n",
    "beta_scheduling = False\n",
    "beta = 1\n",
    "increase_rate = 0.003\n",
    "output_scaling = True\n",
    "output_init = torch.nn.init.ones_\n",
    "policy_type = PolicyType(n_actions, \n",
    "                         post_processing, \n",
    "                         beta_scheduling, \n",
    "                         beta, increase_rate, \n",
    "                         output_scaling, \n",
    "                         output_init)\n",
    "\n",
    "pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "\n",
    "lr_list= [0.025, 0.075, 0.075]  # [weights, input_weights, output_weights]\n",
    "circuit_params = list(circuit.parameters())\n",
    "policy_params = list(policy_type.parameters())\n",
    "params = circuit_params + policy_params\n",
    "optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "env_name = 'CartPole-v1'\n",
    "env = gym.make(env_name)\n",
    "n_episodes = 1000\n",
    "max_t = 500\n",
    "gamma = 0.98\n",
    "print_every = 1\n",
    "verbose = 1\n",
    "reinforce_update = ReinforceUpdate(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose)\n",
    "reinforce_update.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple agent runs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Contiguous\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel runs\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return f\"Agent {file_name}: Training completed\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 4      #set to 1 if data_reuploading is off\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.uniform_\n",
    "    measure = measure_expval_pairs\n",
    "    measure_qubits = None\n",
    "    circuit = CircuitGenerator(n_qubits, \n",
    "                            n_layers,\n",
    "                            shots,\n",
    "                            input_scaling,\n",
    "                            design,\n",
    "                            diff_method,\n",
    "                            weight_init,\n",
    "                            input_init,\n",
    "                            measure,\n",
    "                            measure_qubits)\n",
    "\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'raw_contiguous'\n",
    "    policy_type = PolicyType(n_actions, \n",
    "                            post_processing)\n",
    "\n",
    "    pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "\n",
    "    lr_list= [0.025, 0.075, 0.075]  # [weights, input_weights, output_weights]\n",
    "    circuit_params = list(circuit.parameters())\n",
    "    policy_params = list(policy_type.parameters())\n",
    "    params = circuit_params + policy_params\n",
    "    env_name = 'CartPole-v1'\n",
    "    env = gym.make(env_name)\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 10\n",
    "    verbose = 1\n",
    "        \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, i) for i in range(num_agents))\n",
    "    print(results)\n",
    "    time.sleep(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel runs\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return f\"Agent {file_name}: Training completed\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 4      #set to 1 if data_reuploading is off\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.uniform_\n",
    "    measure = measure_expval_pairs\n",
    "    measure_qubits = None\n",
    "    circuit = CircuitGenerator(n_qubits, \n",
    "                            n_layers,\n",
    "                            shots,\n",
    "                            input_scaling,\n",
    "                            design,\n",
    "                            diff_method,\n",
    "                            weight_init,\n",
    "                            input_init,\n",
    "                            measure,\n",
    "                            measure_qubits)\n",
    "\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'raw_parity'\n",
    "    policy_type = PolicyType(n_actions, \n",
    "                            post_processing)\n",
    "\n",
    "    pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "\n",
    "    lr_list= [0.025, 0.075, 0.075]  # [weights, input_weights, output_weights]\n",
    "    circuit_params = list(circuit.parameters())\n",
    "    policy_params = list(policy_type.parameters())\n",
    "    params = circuit_params + policy_params\n",
    "    env_name = 'CartPole-v1'\n",
    "    env = gym.make(env_name)\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 10\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')  \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, i+10, rundate) for i in range(num_agents))\n",
    "    print(results)\n",
    "    time.sleep(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Agent 0: Training completed', 'Agent 1: Training completed', 'Agent 2: Training completed', 'Agent 3: Training completed', 'Agent 4: Training completed', 'Agent 5: Training completed', 'Agent 6: Training completed', 'Agent 7: Training completed', 'Agent 8: Training completed', 'Agent 9: Training completed']\n"
     ]
    }
   ],
   "source": [
    "# Parallel runs\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return f\"Agent {file_name}: Training completed\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 4      #set to 1 if data_reuploading is off\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.ones_\n",
    "    measure = measure_expval_pairs\n",
    "    measure_qubits = None\n",
    "    circuit = CircuitGenerator(n_qubits, \n",
    "                            n_layers,\n",
    "                            shots,\n",
    "                            input_scaling,\n",
    "                            design,\n",
    "                            diff_method,\n",
    "                            weight_init,\n",
    "                            input_init,\n",
    "                            measure,\n",
    "                            measure_qubits)\n",
    "\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = True\n",
    "    beta = .5\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = False\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_type = PolicyType(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "\n",
    "    lr_list= [0.025, 0.075, 0.075]  # [weights, input_weights, output_weights]\n",
    "    circuit_params = list(circuit.parameters())\n",
    "    policy_params = list(policy_type.parameters())\n",
    "    params = circuit_params + policy_params\n",
    "    optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "    env_name = 'CartPole-v1'\n",
    "    env = gym.make(env_name)\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 10\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, i, rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Agent 0: Training completed', 'Agent 1: Training completed', 'Agent 2: Training completed', 'Agent 3: Training completed', 'Agent 4: Training completed', 'Agent 5: Training completed', 'Agent 6: Training completed', 'Agent 7: Training completed', 'Agent 8: Training completed', 'Agent 9: Training completed']\n"
     ]
    }
   ],
   "source": [
    "# Parallel runs\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return f\"Agent {file_name}: Training completed\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 4      #set to 1 if data_reuploading is off\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.ones_\n",
    "    measure = measure_expval_pairs\n",
    "    measure_qubits = None\n",
    "    circuit = CircuitGenerator(n_qubits, \n",
    "                            n_layers,\n",
    "                            shots,\n",
    "                            input_scaling,\n",
    "                            design,\n",
    "                            diff_method,\n",
    "                            weight_init,\n",
    "                            input_init,\n",
    "                            measure,\n",
    "                            measure_qubits)\n",
    "\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_type = PolicyType(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, \n",
    "                            increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "\n",
    "    lr_list= [0.025, 0.075, 0.075]  # [weights, input_weights, output_weights]\n",
    "    circuit_params = list(circuit.parameters())\n",
    "    policy_params = list(policy_type.parameters())\n",
    "    params = circuit_params + policy_params\n",
    "    optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "    env_name = 'CartPole-v1'\n",
    "    env = gym.make(env_name)\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 10\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, i, rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Agent 0: Training completed', 'Agent 1: Training completed', 'Agent 2: Training completed', 'Agent 3: Training completed', 'Agent 4: Training completed', 'Agent 5: Training completed', 'Agent 6: Training completed', 'Agent 7: Training completed', 'Agent 8: Training completed', 'Agent 9: Training completed']\n"
     ]
    }
   ],
   "source": [
    "# Parallel runs\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return f\"Agent {file_name}: Training completed\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 4      #set to 1 if data_reuploading is off\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.uniform_\n",
    "    measure = measure_expval_pairs\n",
    "    measure_qubits = None\n",
    "    circuit = CircuitGenerator(n_qubits, \n",
    "                            n_layers,\n",
    "                            shots,\n",
    "                            input_scaling,\n",
    "                            design,\n",
    "                            diff_method,\n",
    "                            weight_init,\n",
    "                            input_init,\n",
    "                            measure,\n",
    "                            measure_qubits)\n",
    "\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = True\n",
    "    beta = .5\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.uniform_\n",
    "    policy_type = PolicyType(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "\n",
    "    lr_list= [0.025, 0.075, 0.075]  # [weights, input_weights, output_weights]\n",
    "    circuit_params = list(circuit.parameters())\n",
    "    policy_params = list(policy_type.parameters())\n",
    "    params = circuit_params + policy_params\n",
    "    optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "    env_name = 'CartPole-v1'\n",
    "    env = gym.make(env_name)\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 10\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, i, rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Agent 0: Training completed', 'Agent 1: Training completed', 'Agent 2: Training completed', 'Agent 3: Training completed', 'Agent 4: Training completed', 'Agent 5: Training completed', 'Agent 6: Training completed', 'Agent 7: Training completed', 'Agent 8: Training completed', 'Agent 9: Training completed']\n"
     ]
    }
   ],
   "source": [
    "# Parallel runs\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return f\"Agent {file_name}: Training completed\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 4      #set to 1 if data_reuploading is off\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.uniform_\n",
    "    measure = measure_expval_pairs\n",
    "    measure_qubits = None\n",
    "    circuit = CircuitGenerator(n_qubits, \n",
    "                            n_layers,\n",
    "                            shots,\n",
    "                            input_scaling,\n",
    "                            design,\n",
    "                            diff_method,\n",
    "                            weight_init,\n",
    "                            input_init,\n",
    "                            measure,\n",
    "                            measure_qubits)\n",
    "\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = False\n",
    "    output_init = torch.nn.init.uniform_\n",
    "    policy_type = PolicyType(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "\n",
    "    lr_list= [0.025, 0.075, 0.075]  # [weights, input_weights, output_weights]\n",
    "    circuit_params = list(circuit.parameters())\n",
    "    policy_params = list(policy_type.parameters())\n",
    "    params = circuit_params + policy_params\n",
    "    optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "    env_name = 'CartPole-v1'\n",
    "    env = gym.make(env_name)\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 10\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, i, rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Studying different inverse temperature schedulings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel runs\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return f\"Agent {file_name}: Training completed\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 4      #set to 1 if data_reuploading is off\n",
    "    n_actions = 2\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.uniform_\n",
    "    measure = measure_expval_pairs\n",
    "    measure_qubits = None\n",
    "    circuit = CircuitGenerator( n_qubits, \n",
    "                                n_layers,\n",
    "                                shots,\n",
    "                                input_scaling,\n",
    "                                design,\n",
    "                                diff_method,\n",
    "                                weight_init,\n",
    "                                input_init,\n",
    "                                measure,\n",
    "                                measure_qubits)\n",
    "\n",
    "    post_processing = 'softmax'\n",
    "    beta_list = [.1, .2, .3, .4, .5, .6, .7, .8, .9, 1.]\n",
    "    increase_rate = 0.003\n",
    "    policy_type_list = [PolicyType(n_qubits, n_actions, post_processing, i, increase_rate) for i in beta_list]\n",
    "\n",
    "    pqc_list = [QuantumPolicyModel(circuit,policy_type) for policy_type in policy_type_list]\n",
    "\n",
    "    lr_list= [0.025,0.075]\n",
    "    params= circuit.parameters()\n",
    "    optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "    env_name = 'CartPole-v1'\n",
    "    env = gym.make(env_name)\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 10\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(pqc_list[i], optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, beta_list[i], rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel runs\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return f\"Agent {file_name}: Training completed\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 4      #set to 1 if data_reuploading is off\n",
    "    n_actions = 2\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.uniform_\n",
    "    measure = measure_expval_pairs\n",
    "    measure_qubits = None\n",
    "    circuit = CircuitGenerator( n_qubits, \n",
    "                                n_layers,\n",
    "                                shots,\n",
    "                                input_scaling,\n",
    "                                design,\n",
    "                                diff_method,\n",
    "                                weight_init,\n",
    "                                input_init,\n",
    "                                measure,\n",
    "                                measure_qubits)\n",
    "\n",
    "    post_processing = 'softmax'\n",
    "    beta = 0.35\n",
    "    increase_rate_list = [.0005, .001, .002, .0035, .005, .0065, .008, .0095, .01, .02]\n",
    "    policy_type_list = [PolicyType(n_qubits, n_actions, post_processing, beta, i) for i in increase_rate_list]\n",
    "\n",
    "    pqc_list = [QuantumPolicyModel(circuit,policy_type) for policy_type in policy_type_list]\n",
    "\n",
    "    lr_list= [0.025,0.075]\n",
    "    params= circuit.parameters()\n",
    "    optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "    env_name = 'CartPole-v1'\n",
    "    env = gym.make(env_name)\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 10\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(pqc_list[i], optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, increase_rate_list[i], rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Studying different learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listToString(s):\n",
    " \n",
    "    # initialize an empty string\n",
    "    str1 = \"\"\n",
    " \n",
    "    # traverse in the string\n",
    "    for ele in s:\n",
    "        str1 += ele\n",
    " \n",
    "    # return string\n",
    "    return str1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel runs\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return f\"Agent {file_name}: Training completed\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 4      #set to 1 if data_reuploading is off\n",
    "    n_actions = 2\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.uniform_\n",
    "    measure = measure_probs\n",
    "    measure_qubits = None\n",
    "    circuit = CircuitGenerator( n_qubits, \n",
    "                                n_layers,\n",
    "                                shots,\n",
    "                                input_scaling,\n",
    "                                design,\n",
    "                                diff_method,\n",
    "                                weight_init,\n",
    "                                input_init,\n",
    "                                measure,\n",
    "                                measure_qubits)\n",
    "\n",
    "\n",
    "    post_processing = 'raw_parity'\n",
    "    policy_type = PolicyType(n_qubits, n_actions, post_processing)\n",
    "\n",
    "\n",
    "    pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "    \n",
    "\n",
    "    pqc = pqc\n",
    "    lr_list= [[0.01,0.01],[0.025,0.01],[0.05,0.01],[0.1,0.01],[0.01,0.25],[0.01,0.05],[0.01,0.1],[0.25,0.25],[0.50,0.50],[0.1,0.1]]\n",
    "    params= [circuit.parameters() for i in range(len(lr_list))]\n",
    "    optimizers= [create_optimizer_with_lr(param,lr) for param,lr in zip(params,lr_list)]\n",
    "    env_name = 'CartPole-v1'\n",
    "    env = gym.make(env_name)\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 10\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')  \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(pqc, optimizers[i], env, env_name, n_episodes, max_t, gamma, print_every, verbose, lr_list[i], rundate) for i in range(num_agents))\n",
    "    print(results)\n",
    "    time.sleep(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel runs\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return f\"Agent {file_name}: Training completed\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 4      #set to 1 if data_reuploading is off\n",
    "    n_actions = 2\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.uniform_\n",
    "    measure = measure_probs\n",
    "    measure_qubits = None\n",
    "    circuit = CircuitGenerator( n_qubits, \n",
    "                                n_layers,\n",
    "                                shots,\n",
    "                                input_scaling,\n",
    "                                design,\n",
    "                                diff_method,\n",
    "                                weight_init,\n",
    "                                input_init,\n",
    "                                measure,\n",
    "                                measure_qubits)\n",
    "\n",
    "\n",
    "    post_processing = 'raw_parity'\n",
    "    policy_type = PolicyType(n_qubits, n_actions, post_processing)\n",
    "\n",
    "\n",
    "    pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "    \n",
    "\n",
    "    pqc = pqc\n",
    "    lr_list= [[0.01,0.01],[0.025,0.01],[0.05,0.01],[0.1,0.01],[0.01,0.25],[0.01,0.05],[0.01,0.1],[0.25,0.25],[0.50,0.50],[0.1,0.1]]\n",
    "    params= [circuit.parameters() for i in range(len(lr_list))]\n",
    "    optimizers= [create_optimizer_with_lr(param, lr, use_amsgrad=True) for param,lr in zip(params,lr_list)]\n",
    "    env_name = 'CartPole-v1'\n",
    "    env = gym.make(env_name)\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 10\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')  \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(pqc, optimizers[i], env, env_name, n_episodes, max_t, gamma, print_every, verbose, lr_list[i], rundate) for i in range(num_agents))\n",
    "    print(results)\n",
    "    time.sleep(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
