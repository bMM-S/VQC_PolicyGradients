{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pennylane as qml \n",
    "import torch\n",
    "\n",
    "def create_circuit(n_qubits,n_layers=None,circ = \"simplified_two_design\",fim=False, shots=None):\n",
    "\n",
    "    dev = qml.device(\"default.qubit.torch\", wires=n_qubits, shots=shots)\n",
    "\n",
    "    def RZRY(params):\n",
    "        #qml.SpecialUnitary(params, wires=range(n_qubits))\n",
    "        #qml.SimplifiedTwoDesign(initial_layer_weights=init_params, weights=params, wires=range(n_qubits))\n",
    "        #qml.AngleEmbedding(params,wires=range(n_qubits))\n",
    "        for q in range(n_qubits):\n",
    "            qml.Hadamard(wires=q)\n",
    "\n",
    "        for w in range(n_layers): \n",
    "            for q in range(n_qubits):\n",
    "                index = w * (2*n_qubits) + q * 2\n",
    "                qml.RZ(params[index],wires=q)\n",
    "                qml.RY(params[index + 1],wires=q)\n",
    "        \n",
    "        qml.broadcast(qml.CNOT , wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "        \n",
    "        return qml.probs(wires=range(n_qubits))\n",
    "\n",
    "    def S2D(init_params,params,measurement_qubits=0,prod_approx=False):\n",
    "        #qml.SpecialUnitary(params, wires=range(n_qubits))\n",
    "        qml.SimplifiedTwoDesign(initial_layer_weights=init_params, weights=params, wires=range(n_qubits))\n",
    "        \n",
    "        #qml.broadcast(qml.CNOT , wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "        if not prod_approx:\n",
    "            return qml.probs(wires=list(range(measurement_qubits)))\n",
    "        else:\n",
    "            return [qml.probs(i) for i in range(measurement_qubits)]\n",
    "\n",
    "    def SU(params):\n",
    "        qml.SpecialUnitary(params, wires=range(n_qubits))\n",
    "        \n",
    "        ZZ = qml.operation.Tensor(qml.PauliZ(0), qml.PauliZ(1))\n",
    "        for i in range(2,n_qubits):\n",
    "            ZZ = qml.operation.Tensor(ZZ, qml.PauliZ(i))\n",
    "\n",
    "        return qml.expval(ZZ)\n",
    "    \n",
    "    def simmpleRZRY(params,cnots=True):\n",
    "        qml.broadcast(qml.Hadamard, wires=range(n_qubits), pattern=\"single\")\n",
    "        qml.broadcast(qml.RZ, wires=range(n_qubits), pattern=\"single\", parameters=params[0])\n",
    "        qml.broadcast(qml.RY, wires=range(n_qubits), pattern=\"single\", parameters=params[1])\n",
    "        if cnots:\n",
    "            qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"chain\")\n",
    "\n",
    "            return qml.expval(qml.PauliZ(n_qubits-1))\n",
    "        else:\n",
    "            ZZ = qml.operation.Tensor(qml.PauliZ(0), qml.PauliZ(1))\n",
    "            for i in range(2,n_qubits):\n",
    "                ZZ = qml.operation.Tensor(ZZ, qml.PauliZ(i))\n",
    "\n",
    "            return qml.expval(ZZ)\n",
    "        \n",
    "    def RY(params,y=True,probs=False,prod=False, entanglement=None):\n",
    "        #qml.broadcast(qml.Hadamard, wires=range(n_qubits), pattern=\"single\")\n",
    "        qml.broadcast(qml.RY, wires=range(n_qubits), pattern=\"single\", parameters=params)\n",
    "        #qml.broadcast(qml.CZ, wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "\n",
    "        if entanglement==\"all_to_all\":\n",
    "            qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "        \n",
    "        if y==True:\n",
    "            #YY = qml.operation.Tensor(qml.PauliY(0), qml.PauliY(1))\n",
    "            YY = [qml.PauliZ(0), qml.PauliZ(1)]\n",
    "            for i in range(2,n_qubits):\n",
    "                #YY = qml.operation.Tensor(YY, qml.PauliY(i))\n",
    "                YY.append(qml.PauliZ(i))\n",
    "            \n",
    "            #return [qml.expval(i) for i in YY]\n",
    "            return qml.expval(YY)\n",
    "\n",
    "        elif probs==False:\n",
    "\n",
    "            ZZ = qml.operation.Tensor(qml.PauliZ(0), qml.PauliZ(1))\n",
    "            #ZZ = [qml.PauliZ(0), qml.PauliZ(1)]\n",
    "            for i in range(2,n_qubits):\n",
    "                ZZ = qml.operation.Tensor(ZZ, qml.PauliZ(i))        \n",
    "                #ZZ.append(qml.PauliZ(i))        \n",
    "\n",
    "            #return [qml.expval(i) for i in ZZ]\n",
    "            return qml.expval(ZZ)\n",
    "\n",
    "        else:\n",
    "            if prod:\n",
    "                return [qml.probs(i) for i in range(n_qubits)]\n",
    "            else:\n",
    "                return qml.probs(wires=range(n_qubits))\n",
    "            \n",
    "        \n",
    "        \n",
    "    def GHZ(params,measurement_qubits=0):\n",
    "        qml.RY(params,wires=0)\n",
    "        qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"chain\")\n",
    "\n",
    "        return qml.probs(wires=range(measurement_qubits))\n",
    "\n",
    "    def random_product_state(params,gate_sequence=None):\n",
    "                \n",
    "        for i in range(n_qubits):\n",
    "            qml.RY(np.pi / 4, wires=i)\n",
    "\n",
    "        for ll in range(len(params)):\n",
    "\n",
    "            for i in range(n_qubits):\n",
    "                gate_sequence[\"{}{}\".format(ll,i)](params[ll][i], wires=i)\n",
    "\n",
    "            #for i in range(n_qubits - 1):\n",
    "                #qml.CZ(wires=[i, i + 1])\n",
    "    def SEL(params, measurement_qubits=0):\n",
    "        qml.StronglyEntanglingLayers(params, wires=range(n_qubits))\n",
    "        return qml.probs(wires=range(measurement_qubits))\n",
    "    \n",
    "    def RL(params, measurement_qubits=0):\n",
    "        qml.RandomLayers(params, ratio_imprim=0.8 ,imprimitive=qml.CZ, wires=range(n_qubits))\n",
    "        return qml.probs(wires=range(measurement_qubits))\n",
    "    \n",
    "    if circ == \"rzry\":\n",
    "        qcircuit = RZRY\n",
    "    elif circ == \"simplified_two_design\":\n",
    "        qcircuit = S2D\n",
    "    elif circ == \"special_unitary\":\n",
    "        qcircuit = SU\n",
    "    elif circ == \"simpleRZRY\":\n",
    "        qcircuit = simmpleRZRY\n",
    "    elif circ == \"RY\":\n",
    "        qcircuit = RY\n",
    "    elif circ == \"ghz\":\n",
    "        qcircuit = GHZ\n",
    "    elif circ == \"random_product_state\":\n",
    "        qcircuit = random_product_state\n",
    "    elif circ == \"SEL\":\n",
    "        qcircuit = SEL\n",
    "    elif circ == \"RL\":\n",
    "        qcircuit = RL\n",
    "    if not fim:\n",
    "        circuit = qml.QNode(qcircuit, dev,interface=\"torch\", diff_method=\"backprop\")\n",
    "    else:\n",
    "        circuit = qml.QNode(qcircuit, dev)\n",
    "\n",
    "    return circuit\n",
    "\n",
    "def compute_gradient(log_prob, w):\n",
    "    \"\"\"Compute gradient of the log probability with respect to weights.\n",
    "    \n",
    "    Args:\n",
    "    - log_prob (torch.Tensor): The log probability tensor.\n",
    "    - w (torch.Tensor): The weights tensor, with requires_grad=True.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The gradient of log_prob with respect to w, flattened.\n",
    "    \"\"\"\n",
    "    if w.grad is not None:\n",
    "        w.grad.zero_()\n",
    "    log_prob.backward(retain_graph=True)\n",
    "    \n",
    "    if w.grad is None:\n",
    "        raise ValueError(\"The gradient for the given log_prob with respect to w is None.\")\n",
    "    \n",
    "    return w.grad.view(-1).detach().numpy()\n",
    "\n",
    "def policy(probs, policy_type=\"contiguous-like\", n_actions=2, n_qubits=1):\n",
    "\n",
    "    if policy_type == \"contiguous-like\":\n",
    "        return probs\n",
    "    elif policy_type == \"parity-like\":\n",
    "        policy = torch.zeros(n_actions)\n",
    "        for i in range(len(probs)):\n",
    "            a=[]\n",
    "            for m in range(int(np.log2(n_actions))):\n",
    "                if m==0:    \n",
    "                    bitstring = np.binary_repr(i,width=n_qubits)\n",
    "                else:\n",
    "                    bitstring = np.binary_repr(i,width=n_qubits)[:-m]\n",
    "                \n",
    "                a.append(bitstring.count(\"1\") % 2)\n",
    "            policy[int(\"\".join(str(x) for x in a),2)] += probs[i]\n",
    "\n",
    "        return policy    \n",
    "    \n",
    "def compute_policy_and_gradient(args):\n",
    "    n_qubits, shapes, type , n_actions, policy_type, clamp = args\n",
    "\n",
    "    if policy_type == \"parity-like\":\n",
    "        measure_qubits = n_qubits\n",
    "    else:\n",
    "        measure_qubits = int(np.log2(n_actions))\n",
    "\n",
    "    qc = create_circuit(n_qubits, circ=type, fim=False, shots=None)\n",
    "\n",
    "    if type == \"simplified_two_design\":\n",
    "        weights = [np.random.uniform(-np.pi,np.pi,size=shape) for shape in shapes]    \n",
    "        weights_tensor_init = torch.tensor(weights[0], requires_grad=False)\n",
    "        weights_tensor_params = torch.tensor(weights[1], requires_grad=True)\n",
    "        \n",
    "        probs = qc(weights_tensor_init,weights_tensor_params, measurement_qubits=measure_qubits)\n",
    "\n",
    "    else:\n",
    "        weights = [np.random.uniform(-np.pi,np.pi,size=shape) for shape in shapes]    \n",
    "        weights_tensor_params = torch.tensor(weights, requires_grad=True)\n",
    "\n",
    "        probs = qc(weights_tensor_params, measurement_qubits=measure_qubits)\n",
    "\n",
    "    pi = policy(probs, policy_type=policy_type, n_actions=n_actions, n_qubits=n_qubits)\n",
    "    if clamp is not None:\n",
    "        pi = torch.clamp(pi, clamp, 1)\n",
    "\n",
    "    dist = torch.distributions.Categorical(probs=pi)\n",
    "    \n",
    "    action = dist.sample()\n",
    "    log_prob = dist.log_prob(action)\n",
    "\n",
    "    gradient_no_clamp = np.linalg.norm(compute_gradient(log_prob, weights_tensor_params), 2)\n",
    "    return gradient_no_clamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(policy, optimizer, env, n_episodes=1000, max_t=1000, gamma=1.0, print_every=5):\n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "    scores = []\n",
    "    average_scores = []\n",
    "    runtime_sum = 0\n",
    "    for e in range(1, n_episodes):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        # Collect trajectory\n",
    "        for t in range(max_t):\n",
    "            # Sample the action from current policy\n",
    "            if t==0:\n",
    "                state_tensor = torch.tensor(state[0]).float()\n",
    "            else:\n",
    "                state_tensor = torch.tensor(state).float()\n",
    "            action, log_prob, _, = policy.sample(state_tensor)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    # Total expected reward\n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "    #standardized returns\n",
    "        R=0\n",
    "        policy_loss = []\n",
    "        returns = []\n",
    "        for r in rewards[::-1]:\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0,R)\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + np.finfo(np.float32).eps)\n",
    "\n",
    "        for log_prob, R in zip(saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "\n",
    "        policy_unsqueezed = [torch.unsqueeze(loss, 0) for loss in policy_loss]\n",
    "        policy_sum = torch.cat(policy_unsqueezed).sum()\n",
    "\n",
    "    # Backpropagation\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        policy_sum.backward()\n",
    "        optimizer.step()\n",
    "        end_time = time.time()\n",
    "        runtime = end_time-start_time\n",
    "        \n",
    "        runtime_sum += runtime\n",
    "        if e % print_every == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tLast {}\\tEpisodes average reward: {:.2f}\\tRuntime: {:.2f}'.format(e, scores_deque[-1], print_every, np.mean(scores_deque), runtime_sum))\n",
    "            runtime_sum = 0\n",
    "        if np.mean(scores_deque) == 500:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(e, np.mean(scores_deque)))\n",
    "            break\n",
    "    return scores, policy.gradient_list, average_scores  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def save_training_data(self):\n",
    "        ''' \n",
    "        Saves training data into json files\n",
    "        '''\n",
    "        current_directory = os.path.dirname(__file__)\n",
    "        folder_name = f\"{str(self.env_name)}_{self.pqc.policy.post_processing}_{self.pqc.circuit.n_layers}\"\n",
    "        folder_path = os.path.join(current_directory, folder_name)\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "        episode_data = [self.scores_deque, \n",
    "                        self.runtime, \n",
    "                        self.loss.item(), \n",
    "                        tensor_to_list(self.pqc.get_gradients()[0]), \n",
    "                        tensor_to_list(self.pqc.get_gradients()[1])]\n",
    "\n",
    "        if folder_path is not None:\n",
    "            file_path = os.path.join(self.folder_path, f\"{self.file_name}.json\")\n",
    "            if os.path.exists(file_path):\n",
    "                with open(file_path, 'r') as f:\n",
    "                    existing_data = json.load(f)\n",
    "                existing_data.append(episode_data)\n",
    "                with open(file_path, 'w') as f:\n",
    "                    json.dump(existing_data, f, indent=4)\n",
    "            else:\n",
    "                with open(file_path, 'w') as f:\n",
    "                    json.dump([episode_data], f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import gym\n",
    "from collections import deque\n",
    "\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir ../../data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "\n",
    "def tensor_to_list(tensor):\n",
    "    \"\"\"\n",
    "    Convert a tensor or numpy array to a nested list.\n",
    "    \"\"\"\n",
    "    if isinstance(tensor, list):\n",
    "        return [tensor_to_list(t) for t in tensor]\n",
    "    elif isinstance(tensor, dict):\n",
    "        return {key: tensor_to_list(value) for key, value in tensor.items()}\n",
    "    elif isinstance(tensor, np.ndarray):\n",
    "        return tensor.tolist()\n",
    "    elif isinstance(tensor, torch.Tensor):\n",
    "        return tensor.tolist()\n",
    "    else:\n",
    "        return tensor\n",
    "\n",
    "def measure_probs(qubits):\n",
    "    return qml.probs(wires=range(qubits)) \n",
    "\n",
    "def measure_expval_pairs(qubits):\n",
    "    expvals = []\n",
    "    for i in range(qubits // 2):\n",
    "        expvals.append(qml.expval(qml.PauliZ(2*i) @ qml.PauliZ(2*i + 1)))\n",
    "    return expvals\n",
    "    \n",
    "def create_optimizer_with_lr(params, lr_list, use_amsgrad=False):\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': p, 'lr': lr} for p, lr in zip(params, lr_list)\n",
    "    ], amsgrad=use_amsgrad)\n",
    "    return optimizer\n",
    "\n",
    "def get_function_representation(func):\n",
    "    if callable(func):\n",
    "        # Check if the function is a lambda\n",
    "        if func.__name__ == \"<lambda>\":\n",
    "            # Optionally, check if the function has a custom description attribute\n",
    "            return f\"{func.__module__}.<lambda>\" + (getattr(func, 'description', ''))\n",
    "        else:\n",
    "            return f\"{func.__module__}.{func.__name__}\"\n",
    "    return \"Unknown Function Type\"\n",
    "\n",
    "def jerbi_circuit(n_qubits, n_layers, shots, input_scaling, diff_method, weight_init, input_init, measure, measure_qubits):\n",
    "\n",
    "    if shots is None:\n",
    "        dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "    else:\n",
    "        dev = qml.device(\"default.qubit\", wires=n_qubits, shots=shots)\n",
    "    \n",
    "    if n_layers < 1:\n",
    "        raise ValueError(\"Number of layers can't take values below 1\")\n",
    "    \n",
    "    weight_shapes = {\"params\": (n_layers + 1, n_qubits, 2),\n",
    "                    \"input_params\": (n_layers, n_qubits, 2)}\n",
    "    init_method   = {\"params\": weight_init,\n",
    "                    \"input_params\": input_init}\n",
    "    \n",
    "    @qml.qnode(dev, interface='torch', diff_method=diff_method)\n",
    "    def qnode(inputs, params, input_params):\n",
    "    #in case n_qubits != input length\n",
    "        if n_qubits > len(inputs) and n_qubits % len(inputs) == 0:\n",
    "            multiplier = n_qubits // len(inputs)\n",
    "            inputs = torch.cat([inputs] * multiplier)\n",
    "        elif n_qubits != len(inputs) and n_qubits % len(inputs) != 0:\n",
    "            raise ValueError('Number of qubits cannot be divided by input lenght')\n",
    "\n",
    "    #hadamard\n",
    "        qml.broadcast(qml.Hadamard, wires=range(n_qubits), pattern=\"single\")\n",
    "        \n",
    "        for layer in range(n_layers):\n",
    "            for wire in range(n_qubits):\n",
    "                qml.RZ(params[layer][wire][0], wires=wire)\n",
    "                qml.RY(params[layer][wire][1], wires=wire)\n",
    "\n",
    "            qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"chain\")\n",
    "\n",
    "            if input_scaling:\n",
    "                for wire in range(n_qubits):\n",
    "                    qml.RY(input_params[layer][wire][0] * inputs[wire], wires=wire)\n",
    "                    qml.RZ(input_params[layer][wire][1] * inputs[wire], wires=wire)\n",
    "            else:\n",
    "                for wire in range(n_qubits):\n",
    "                    qml.RY(inputs[wire], wires=wire)\n",
    "                    qml.RZ(inputs[wire], wires=wire)\n",
    "\n",
    "        for wire in range(n_qubits):\n",
    "            qml.RZ(params[-1][wire][0], wires=wire)\n",
    "            qml.RY(params[-1][wire][1], wires=wire)\n",
    "            \n",
    "        return measure(measure_qubits)\n",
    "\n",
    "    model = qml.qnn.TorchLayer(qnode, weight_shapes=weight_shapes, init_method=init_method)  \n",
    "    \n",
    "    return model\n",
    "    \n",
    "def S2D(n_qubits, n_layers, shots, input_scaling, diff_method, weight_init, input_init, measure_type, observables, measure_qubits):\n",
    "\n",
    "    dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "    observables = observables if observables is not None else None\n",
    "    \n",
    "    shapes = qml.SimplifiedTwoDesign.shape(n_layers=n_layers, n_wires=n_qubits)\n",
    "\n",
    "    weight_shapes = {\"params\": shapes[1],\n",
    "                     \"input_params\": shapes[0]}\n",
    "    \n",
    "    init_method   = {\"params\": weight_init,\n",
    "                     \"input_params\": input_init}\n",
    "\n",
    "    @qml.qnode(dev, interface='torch', diff_method='parameter-shift')\n",
    "    def qnode(inputs, params, input_params):\n",
    "\n",
    "        return measure_selection(measure_type, observables, measure_qubits)\n",
    "\n",
    "    model = qml.qnn.TorchLayer(qnode, weight_shapes=weight_shapes, init_method=init_method)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircuitGenerator(nn.Module):\n",
    "\n",
    "    def __init__(self, n_qubits, n_layers,  shots = None, input_scaling = True,\n",
    "                design = 'jerbi_circuit', diff_method = 'backprop', weight_init = torch.nn.init.uniform_, \n",
    "                input_init = torch.nn.init.ones_, measure = None, measure_qubits = None):\n",
    "        super(CircuitGenerator, self).__init__()\n",
    "        '''\n",
    "\n",
    "        Creates a parameterized quantum circuit based on the arguments:\n",
    "\n",
    "            n_qubits(int) = Number of qubits\n",
    "            n_layers(int) = Number of layers (0 if no data re-uploading)\n",
    "            shots(int) = Number of times the circuit gets executed\n",
    "            input_scaling(bool) = Input parameters are used if True (input*input_params)\n",
    "            design(str) = The PQC ansatz design ('jerbi_circuit')\n",
    "            diff_method(str) = Differentiation method ('best', 'backprop', 'parameter-shift', ...)\n",
    "            weight_init (function) = How PQC weights are initialized (.uniform_, .ones_, ...)\n",
    "            input_init (function) = How input weights are initialized (.uniform_, .ones_, ...)\n",
    "            measure (function) = Measure function (measure_probs, measure_expval_pairs)\n",
    "            measure_qubits (int) = Number of qubits to be measured (in some cases might be equal to the number of qubits)\n",
    "            \n",
    "        '''\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.shots = shots\n",
    "        self.input_scaling = input_scaling\n",
    "        self.design = design\n",
    "        self.diff_method = diff_method\n",
    "        self.weight_init = weight_init\n",
    "        self.input_init = input_init\n",
    "        if measure is None:\n",
    "            self.measure = measure_probs\n",
    "        else:\n",
    "            self.measure = measure\n",
    "\n",
    "        if measure_qubits is None:\n",
    "            self.measure_qubits = n_qubits\n",
    "        else:\n",
    "            self.measure_qubits = measure_qubits\n",
    "\n",
    "        if self.design == 'jerbi_circuit':\n",
    "            self.circuit = jerbi_circuit(n_qubits = self.n_qubits,\n",
    "                                        n_layers = self.n_layers,\n",
    "                                        shots = self.shots,\n",
    "                                        input_scaling = self.input_scaling,\n",
    "                                        diff_method = self.diff_method,\n",
    "                                        weight_init = self.weight_init,\n",
    "                                        input_init = self.input_init,\n",
    "                                        measure = self.measure,\n",
    "                                        measure_qubits = self.measure_qubits)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported circuit type\")\n",
    "\n",
    "    def input(self,inputs):\n",
    "\n",
    "        outputs = self.circuit(inputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyType(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_actions, post_processing = 'raw_contiguous', \n",
    "                 beta_scheduling = False, beta = 1, increase_rate = 0.003, \n",
    "                 output_scaling = False, output_init = torch.nn.init.ones_):\n",
    "        super(PolicyType, self).__init__()\n",
    "\n",
    "        '''\n",
    "        Determines the type of policy used based on the arguments:\n",
    "\n",
    "            n_actions(int) = Number of actions\n",
    "            post_processing(str) = Type of policy ('raw_contiguous', 'raw_parity', 'softmax')\n",
    "            beta_scheduling(bool) = Inverse temperature parameter used in softmax (used if set to True)\n",
    "            beta(float) = Beta parameter or inverse temperature (used only for softmax)\n",
    "            increase_rate(float) = Amount added to beta at the end of each episode (used only for softmax)\n",
    "            output_scaling(bool) = Output parameters are used if True\n",
    "            output_init(function) = How the output parameters are initialized\n",
    "            \n",
    "        '''\n",
    "        self.n_actions = n_actions\n",
    "        self.post_processing = post_processing\n",
    "        self.beta_scheduling = beta_scheduling\n",
    "        self.beta = beta\n",
    "        self.increase_rate = increase_rate\n",
    "        self.output_scaling = output_scaling\n",
    "        self.output_init = output_init\n",
    "\n",
    "        if self.output_scaling == True:\n",
    "            self.output_params = nn.parameter.Parameter(torch.Tensor(self.n_actions), requires_grad=True)\n",
    "            self.output_init(self.output_params)\n",
    "        else:\n",
    "            self.register_parameter('w_input', None)\n",
    "\n",
    "\n",
    "    def input(self,probs):\n",
    "        if self.post_processing == 'raw_contiguous':\n",
    "            policy = self.raw_contiguous(probs)\n",
    "        elif self.post_processing == 'raw_parity':\n",
    "            policy = self.raw_parity(probs)\n",
    "        elif self.post_processing == 'softmax':\n",
    "            policy = self.softmax(probs)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid post-processing method specified.\")\n",
    "        return policy\n",
    "\n",
    "    def raw_contiguous(self,probs):\n",
    "        \n",
    "        probs_flatten = probs.flatten()\n",
    "        chunk_size = len(probs_flatten) // self.n_actions\n",
    "        remainder = len(probs_flatten) % self.n_actions\n",
    "\n",
    "        policy = []\n",
    "\n",
    "        for i in range(self.n_actions):\n",
    "            start = i * chunk_size\n",
    "            end = (i + 1) * chunk_size\n",
    "\n",
    "            if i < remainder:\n",
    "                end += 1\n",
    "\n",
    "            # Update the original policy list instead of creating a new one\n",
    "            policy.append(sum(probs_flatten[start:end]))\n",
    "\n",
    "        policy_tensor = torch.stack(policy)\n",
    "        return policy_tensor\n",
    "        \n",
    "    def raw_parity(self,probs):\n",
    "\n",
    "        if self.n_actions % 2 != 0:\n",
    "            raise ValueError('For parity-like policy, n_actions must be an even number')\n",
    "        \n",
    "        probs_flatten = probs.flatten()\n",
    "        policy = torch.zeros(self.n_actions)\n",
    "        counter = 0\n",
    "        for prob in probs_flatten:\n",
    "            policy[counter] += prob\n",
    "            counter += 1\n",
    "            if counter == self.n_actions:\n",
    "                counter = 0\n",
    "        \n",
    "        return policy\n",
    "    \n",
    "    def softmax(self, probs):\n",
    "        \n",
    "        if self.output_scaling == True:\n",
    "            probs *= self.output_params\n",
    "\n",
    "        if len(probs) == self.n_actions:\n",
    "            scaled_output = probs * self.beta\n",
    "            softmax_output = F.softmax(scaled_output, dim=0)\n",
    "            return softmax_output\n",
    "        else:\n",
    "            probs_flatten = probs.flatten()\n",
    "            chunk_size = len(probs_flatten) // self.n_actions\n",
    "            remainder = len(probs_flatten) % self.n_actions\n",
    "\n",
    "            policy = []\n",
    "\n",
    "            for i in range(self.n_actions):\n",
    "                start = i * chunk_size\n",
    "                end = (i + 1) * chunk_size\n",
    "\n",
    "                if i < remainder:\n",
    "                    end += 1\n",
    "\n",
    "                # Update the original policy list instead of creating a new one\n",
    "                policy.append(sum(probs_flatten[start:end]))\n",
    "            policy_tensor = torch.stack(policy)\n",
    "            softmax_output = F.softmax(policy_tensor * self.beta, dim=0)\n",
    "            return softmax_output\n",
    "        \n",
    "    def beta_schedule(self):\n",
    "        if self.beta_scheduling == True:\n",
    "            if self.post_processing == 'softmax':\n",
    "                self.beta += self.increase_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumPolicyModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, circuit, policy):\n",
    "        super(QuantumPolicyModel, self).__init__()\n",
    "        self.circuit = circuit\n",
    "        self.policy = policy\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        Input state is fed to the circuit - its output is then fed to the post processing \n",
    "        '''\n",
    "        probs = self.circuit.input(inputs)\n",
    "        probs_processed = self.policy.input(probs)\n",
    "        return probs_processed\n",
    "    \n",
    "    def sample(self, inputs):\n",
    "        '''\n",
    "        Samples an action from the action probability distribution aka policy\n",
    "        '''\n",
    "        policy = self.forward(inputs)\n",
    "        dist = torch.distributions.Categorical(policy)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action), policy\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        '''\n",
    "        Returns the values of each set of parameters\n",
    "        '''\n",
    "        parameter_values = [param.clone().detach().numpy().flatten() for param in self.circuit.parameters()]\n",
    "        return parameter_values\n",
    "    \n",
    "    def get_gradients(self):\n",
    "        '''\n",
    "        Returns the gradient values of each set of parameters from both circuit and policy\n",
    "        '''\n",
    "        gradients = []\n",
    "\n",
    "        # Get gradients from circuit parameters\n",
    "        circuit_gradients = [torch.flatten(param.grad.clone().detach()) if param.grad is not None else torch.flatten(torch.zeros_like(param)) for param in self.circuit.parameters()]\n",
    "        gradients.extend(circuit_gradients)\n",
    "\n",
    "        # Get gradients from policy parameters\n",
    "        policy_gradients = [torch.flatten(param.grad.clone().detach()) if param.grad is not None else torch.flatten(torch.zeros_like(param)) for param in self.policy.parameters()]\n",
    "        gradients.extend(policy_gradients)\n",
    "\n",
    "        return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceUpdate():\n",
    "\n",
    "    def __init__(self, pqc, optimizer, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name = None, rundate = None):\n",
    "        \n",
    "        self.pqc = pqc\n",
    "        self.optimizer = optimizer\n",
    "        self.env = env\n",
    "        self.env_name = env_name\n",
    "        self.n_episodes = n_episodes\n",
    "        self.max_t = max_t\n",
    "        self.gamma = gamma\n",
    "        self.scores_deque = deque(maxlen=print_every)\n",
    "        self.print_every = print_every\n",
    "        self.verbose = verbose\n",
    "        self.file_name = file_name\n",
    "        self.rundate = rundate\n",
    "        self.running_reward = 10\n",
    "\n",
    "    def get_trajectory(self):\n",
    "        '''\n",
    "        Gets a trajectory based on the running policy until it runs out of bounds or solves the envinronment\n",
    "        '''\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        state = self.env.reset()\n",
    "        for t in range(self.max_t):\n",
    "            if t == 0:\n",
    "                state_tensor = torch.tensor(state[0]).float()\n",
    "            else:\n",
    "                state_tensor = torch.tensor(state).float()\n",
    "            action, log_prob, _, = self.pqc.sample(state_tensor)\n",
    "            state, reward, done, _, _ = self.env.step(action)\n",
    "            \n",
    "            self.saved_log_probs.append(log_prob)\n",
    "            self.rewards.append(reward)\n",
    "\n",
    "            if done:\n",
    "                self.scores_deque.append(sum(self.rewards))\n",
    "                break\n",
    "\n",
    "    def update_policy(self):\n",
    "        '''\n",
    "        Computes the loss and gradients and updates the policy via gradient methods\n",
    "        '''\n",
    "\n",
    "        R = 0\n",
    "        policy_loss = []\n",
    "        returns = []\n",
    "        for r in self.rewards[::-1]:\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "\n",
    "        for log_prob, ret in zip(self.saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * ret)\n",
    "\n",
    "        policy_unsqueezed = [torch.unsqueeze(loss, 0) for loss in policy_loss]\n",
    "        self.loss = torch.cat(policy_unsqueezed).sum()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        self.loss.backward()    \n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def save_agent_data(self,main_path):\n",
    "        '''\n",
    "        Stores the model parameters into a json file\n",
    "\n",
    "        '''\n",
    "\n",
    "        agent_variables = {\n",
    "            \"Number of Qubits\": self.pqc.circuit.n_qubits,\n",
    "            \"Number of Layers\": self.pqc.circuit.n_layers,\n",
    "            \"Shots\": self.pqc.circuit.shots,\n",
    "            \"Input Scaling\": self.pqc.circuit.input_scaling,\n",
    "            \"Design\": self.pqc.circuit.design,\n",
    "            \"Differentiation Method\": self.pqc.circuit.diff_method,\n",
    "            \"Weight Initiation\": \"lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\",\n",
    "            \"Input_init\": get_function_representation(self.pqc.circuit.input_init),\n",
    "            \"Measure\": get_function_representation(self.pqc.circuit.measure),\n",
    "            \"Measure Qubits\": self.pqc.circuit.measure_qubits,\n",
    "            \"Policy Type\": self.pqc.policy.post_processing,\n",
    "            \"Softmax scheduling (in case policy is softmax)\": (\"Starting beta: \" + str(self.pqc.policy.beta) + \". Increase rate: \" + str(self.pqc.policy.increase_rate)),\n",
    "            \"Optimizers\": str(self.optimizer),\n",
    "            \"Envinronment Name\": str(self.env_name),\n",
    "            \"Gamma (discounting factor)\": self.gamma,\n",
    "        }\n",
    "\n",
    "        with open(os.path.join(main_path, \"agent_characteristics.json\"), \"w\") as f:\n",
    "            json.dump(agent_variables, f, indent=4)\n",
    "\n",
    "    def save_data(self,main_path):\n",
    "        '''\n",
    "        Saves the data into a .npz file of each episode\n",
    "\n",
    "        '''\n",
    "\n",
    "        run= os.path.join(main_path,str(self.file_name)+'_data.npz')\n",
    "\n",
    "        if not os.path.exists(main_path):\n",
    "            os.makedirs(main_path)\n",
    "\n",
    "        if os.path.exists(run):\n",
    "            data = np.load(run, allow_pickle=True)\n",
    "            old_episode_reward = data['episode_reward'].tolist()\n",
    "            old_loss = data['loss'].tolist()\n",
    "            old_runtime = data['runtime'].tolist()\n",
    "            old_params_gradients = data['params_gradients'].tolist()\n",
    "            old_input_params_gradients = data['input_params_gradients'].tolist()\n",
    "        else:\n",
    "            old_episode_reward = []\n",
    "            old_loss = []\n",
    "            old_runtime = []\n",
    "            old_params_gradients = []\n",
    "            old_input_params_gradients = []\n",
    "\n",
    "\n",
    "        old_episode_reward.append(self.scores_deque[-1])\n",
    "        old_loss.append(self.loss.item())\n",
    "        old_runtime.append(self.runtime)\n",
    "        old_params_gradients.append(tensor_to_list(self.pqc.get_gradients()[0]))\n",
    "        old_input_params_gradients.append(tensor_to_list(self.pqc.get_gradients()[1]))\n",
    "\n",
    "        np.savez_compressed(run, episode_reward = np.array(old_episode_reward),\n",
    "                                 loss = np.array(old_loss),\n",
    "                                 runtime = np.array(old_runtime),\n",
    "                                 params_gradients = np.array(old_params_gradients), \n",
    "                                 input_params_gradients = np.array(old_input_params_gradients))\n",
    "        \n",
    "        del old_episode_reward[:]\n",
    "        del old_loss[:]\n",
    "        del old_runtime[:]\n",
    "        del old_params_gradients[:]\n",
    "        del old_input_params_gradients[:]\n",
    "\n",
    "    def writer_function(self, writer, iteration):\n",
    "        '''\n",
    "        Stores data into a tensorboard session\n",
    "\n",
    "        '''\n",
    "        writer.add_scalar(\"Episode Reward\", np.mean(self.scores_deque), global_step=iteration)\n",
    "        writer.add_scalar(\"Runtime\", self.runtime, global_step=iteration)\n",
    "        writer.add_scalar(\"Loss\", self.loss.item(), global_step=iteration)\n",
    "        writer.add_scalar(\"Beta\", self.pqc.policy.beta, global_step=iteration)\n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        logs_dir = \"../../data\"\n",
    "        os.makedirs(logs_dir, exist_ok=True)\n",
    "        envinronment_folder = os.path.join(logs_dir, self.env_name)\n",
    "        os.makedirs(envinronment_folder, exist_ok=True)\n",
    "        experiment_folder = f\"{self.pqc.policy.post_processing}_{self.pqc.circuit.n_layers}layer_{self.rundate}\"\n",
    "        experiment_path = os.path.join(envinronment_folder, experiment_folder)\n",
    "        os.makedirs(experiment_path, exist_ok=True)\n",
    "        run = os.path.join(experiment_path,str(self.file_name))\n",
    "        os.makedirs(run, exist_ok=True)\n",
    "        writer = SummaryWriter(log_dir=run)\n",
    "        self.save_agent_data(experiment_path)\n",
    "        \n",
    "        for i in range(1, self.n_episodes):\n",
    "            start_time = time.time()\n",
    "            self.get_trajectory()\n",
    "            self.update_policy()\n",
    "            end_time = time.time()\n",
    "            self.runtime = end_time - start_time\n",
    "            self.writer_function(writer,i)\n",
    "            self.save_data(run)\n",
    "            self.pqc.policy.beta_schedule()\n",
    "            \n",
    "            if np.mean(self.scores_deque) > self.env.spec.reward_threshold:\n",
    "                print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i, np.mean(self.scores_deque)))\n",
    "                break\n",
    "            elif i % self.print_every == 0 and self.verbose == 1:\n",
    "                print('Episode {}\\tLast reward: {:.2f}\\tLast {} Episodes average reward: {:.2f}\\tRuntime: {:.2f}\\t '.format(i, self.scores_deque[-1], self.print_every, np.mean(self.scores_deque), self.runtime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single agent runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 4\n",
    "n_layers = 4      #set to 1 if data_reuploading is off\n",
    "shots = None\n",
    "input_scaling = True\n",
    "design = 'jerbi_circuit' \n",
    "diff_method = 'backprop' \n",
    "weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "input_init = torch.nn.init.uniform_\n",
    "measure = measure_expval_pairs\n",
    "measure_qubits = None\n",
    "circuit = CircuitGenerator(n_qubits, \n",
    "                           n_layers,\n",
    "                           shots,\n",
    "                           input_scaling,\n",
    "                           design,\n",
    "                           diff_method,\n",
    "                           weight_init,\n",
    "                           input_init,\n",
    "                           measure,\n",
    "                           measure_qubits)\n",
    "\n",
    "\n",
    "n_actions = 2\n",
    "post_processing = 'softmax'\n",
    "beta_scheduling = True\n",
    "beta = .5\n",
    "increase_rate = 0.003\n",
    "output_scaling = False\n",
    "output_init = torch.nn.init.uniform_\n",
    "policy_type = PolicyType(n_actions, \n",
    "                         post_processing, \n",
    "                         beta_scheduling, \n",
    "                         beta, increase_rate, \n",
    "                         output_scaling, \n",
    "                         output_init)\n",
    "\n",
    "pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "\n",
    "lr_list= [0.025, 0.075, 0.075]  # [weights, input_weights, output_weights]\n",
    "circuit_params = list(circuit.parameters())\n",
    "policy_params = list(policy_type.parameters())\n",
    "params = circuit_params + policy_params\n",
    "optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "env_name = 'CartPole-v1'\n",
    "env = gym.make(env_name)\n",
    "n_episodes = 1000\n",
    "max_t = 500\n",
    "gamma = 0.98\n",
    "print_every = 10\n",
    "verbose = 1\n",
    "reinforce_update = ReinforceUpdate(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose)\n",
    "reinforce_update.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple agent runs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Contiguous\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel runs\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return f\"Agent {file_name}: Training completed\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 4      #set to 1 if data_reuploading is off\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.uniform_\n",
    "    measure = measure_expval_pairs\n",
    "    measure_qubits = None\n",
    "    circuit = CircuitGenerator(n_qubits, \n",
    "                            n_layers,\n",
    "                            shots,\n",
    "                            input_scaling,\n",
    "                            design,\n",
    "                            diff_method,\n",
    "                            weight_init,\n",
    "                            input_init,\n",
    "                            measure,\n",
    "                            measure_qubits)\n",
    "\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'raw_contiguous'\n",
    "    policy_type = PolicyType(n_actions, \n",
    "                            post_processing)\n",
    "\n",
    "    pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "\n",
    "    lr_list= [0.025, 0.075, 0.075]  # [weights, input_weights, output_weights]\n",
    "    circuit_params = list(circuit.parameters())\n",
    "    policy_params = list(policy_type.parameters())\n",
    "    params = circuit_params + policy_params\n",
    "    env_name = 'CartPole-v1'\n",
    "    env = gym.make(env_name)\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 10\n",
    "    verbose = 1\n",
    "        \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, i) for i in range(num_agents))\n",
    "    print(results)\n",
    "    time.sleep(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel runs\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return f\"Agent {file_name}: Training completed\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 4      #set to 1 if data_reuploading is off\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.uniform_\n",
    "    measure = measure_expval_pairs\n",
    "    measure_qubits = None\n",
    "    circuit = CircuitGenerator(n_qubits, \n",
    "                            n_layers,\n",
    "                            shots,\n",
    "                            input_scaling,\n",
    "                            design,\n",
    "                            diff_method,\n",
    "                            weight_init,\n",
    "                            input_init,\n",
    "                            measure,\n",
    "                            measure_qubits)\n",
    "\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'raw_parity'\n",
    "    policy_type = PolicyType(n_actions, \n",
    "                            post_processing)\n",
    "\n",
    "    pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "\n",
    "    lr_list= [0.025, 0.075, 0.075]  # [weights, input_weights, output_weights]\n",
    "    circuit_params = list(circuit.parameters())\n",
    "    policy_params = list(policy_type.parameters())\n",
    "    params = circuit_params + policy_params\n",
    "    env_name = 'CartPole-v1'\n",
    "    env = gym.make(env_name)\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 10\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')  \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, i+10, rundate) for i in range(num_agents))\n",
    "    print(results)\n",
    "    time.sleep(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel runs\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return f\"Agent {file_name}: Training completed\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 4      #set to 1 if data_reuploading is off\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.uniform_\n",
    "    measure = measure_expval_pairs\n",
    "    measure_qubits = None\n",
    "    circuit = CircuitGenerator(n_qubits, \n",
    "                            n_layers,\n",
    "                            shots,\n",
    "                            input_scaling,\n",
    "                            design,\n",
    "                            diff_method,\n",
    "                            weight_init,\n",
    "                            input_init,\n",
    "                            measure,\n",
    "                            measure_qubits)\n",
    "\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = True\n",
    "    beta = .5\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = False\n",
    "    output_init = torch.nn.init.uniform_\n",
    "    policy_type = PolicyType(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "\n",
    "    lr_list= [0.025, 0.075, 0.075]  # [weights, input_weights, output_weights]\n",
    "    circuit_params = list(circuit.parameters())\n",
    "    policy_params = list(policy_type.parameters())\n",
    "    params = circuit_params + policy_params\n",
    "    optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "    env_name = 'CartPole-v1'\n",
    "    env = gym.make(env_name)\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 10\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, i, rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Studying different inverse temperature schedulings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel runs\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return f\"Agent {file_name}: Training completed\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 4      #set to 1 if data_reuploading is off\n",
    "    n_actions = 2\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.uniform_\n",
    "    measure = measure_expval_pairs\n",
    "    measure_qubits = None\n",
    "    circuit = CircuitGenerator( n_qubits, \n",
    "                                n_layers,\n",
    "                                shots,\n",
    "                                input_scaling,\n",
    "                                design,\n",
    "                                diff_method,\n",
    "                                weight_init,\n",
    "                                input_init,\n",
    "                                measure,\n",
    "                                measure_qubits)\n",
    "\n",
    "    post_processing = 'softmax'\n",
    "    beta_list = [.1, .2, .3, .4, .5, .6, .7, .8, .9, 1.]\n",
    "    increase_rate = 0.003\n",
    "    policy_type_list = [PolicyType(n_qubits, n_actions, post_processing, i, increase_rate) for i in beta_list]\n",
    "\n",
    "    pqc_list = [QuantumPolicyModel(circuit,policy_type) for policy_type in policy_type_list]\n",
    "\n",
    "    lr_list= [0.025,0.075]\n",
    "    params= circuit.parameters()\n",
    "    optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "    env_name = 'CartPole-v1'\n",
    "    env = gym.make(env_name)\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 10\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(pqc_list[i], optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, beta_list[i], rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel runs\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return f\"Agent {file_name}: Training completed\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 4      #set to 1 if data_reuploading is off\n",
    "    n_actions = 2\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.uniform_\n",
    "    measure = measure_expval_pairs\n",
    "    measure_qubits = None\n",
    "    circuit = CircuitGenerator( n_qubits, \n",
    "                                n_layers,\n",
    "                                shots,\n",
    "                                input_scaling,\n",
    "                                design,\n",
    "                                diff_method,\n",
    "                                weight_init,\n",
    "                                input_init,\n",
    "                                measure,\n",
    "                                measure_qubits)\n",
    "\n",
    "    post_processing = 'softmax'\n",
    "    beta = 0.35\n",
    "    increase_rate_list = [.0005, .001, .002, .0035, .005, .0065, .008, .0095, .01, .02]\n",
    "    policy_type_list = [PolicyType(n_qubits, n_actions, post_processing, beta, i) for i in increase_rate_list]\n",
    "\n",
    "    pqc_list = [QuantumPolicyModel(circuit,policy_type) for policy_type in policy_type_list]\n",
    "\n",
    "    lr_list= [0.025,0.075]\n",
    "    params= circuit.parameters()\n",
    "    optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "    env_name = 'CartPole-v1'\n",
    "    env = gym.make(env_name)\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 10\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(pqc_list[i], optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, increase_rate_list[i], rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Studying different learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listToString(s):\n",
    " \n",
    "    # initialize an empty string\n",
    "    str1 = \"\"\n",
    " \n",
    "    # traverse in the string\n",
    "    for ele in s:\n",
    "        str1 += ele\n",
    " \n",
    "    # return string\n",
    "    return str1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel runs\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return f\"Agent {file_name}: Training completed\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 4      #set to 1 if data_reuploading is off\n",
    "    n_actions = 2\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.uniform_\n",
    "    measure = measure_probs\n",
    "    measure_qubits = None\n",
    "    circuit = CircuitGenerator( n_qubits, \n",
    "                                n_layers,\n",
    "                                shots,\n",
    "                                input_scaling,\n",
    "                                design,\n",
    "                                diff_method,\n",
    "                                weight_init,\n",
    "                                input_init,\n",
    "                                measure,\n",
    "                                measure_qubits)\n",
    "\n",
    "\n",
    "    post_processing = 'raw_parity'\n",
    "    policy_type = PolicyType(n_qubits, n_actions, post_processing)\n",
    "\n",
    "\n",
    "    pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "    \n",
    "\n",
    "    pqc = pqc\n",
    "    lr_list= [[0.01,0.01],[0.025,0.01],[0.05,0.01],[0.1,0.01],[0.01,0.25],[0.01,0.05],[0.01,0.1],[0.25,0.25],[0.50,0.50],[0.1,0.1]]\n",
    "    params= [circuit.parameters() for i in range(len(lr_list))]\n",
    "    optimizers= [create_optimizer_with_lr(param,lr) for param,lr in zip(params,lr_list)]\n",
    "    env_name = 'CartPole-v1'\n",
    "    env = gym.make(env_name)\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 10\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')  \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(pqc, optimizers[i], env, env_name, n_episodes, max_t, gamma, print_every, verbose, lr_list[i], rundate) for i in range(num_agents))\n",
    "    print(results)\n",
    "    time.sleep(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel runs\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return f\"Agent {file_name}: Training completed\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 4      #set to 1 if data_reuploading is off\n",
    "    n_actions = 2\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.uniform_\n",
    "    measure = measure_probs\n",
    "    measure_qubits = None\n",
    "    circuit = CircuitGenerator( n_qubits, \n",
    "                                n_layers,\n",
    "                                shots,\n",
    "                                input_scaling,\n",
    "                                design,\n",
    "                                diff_method,\n",
    "                                weight_init,\n",
    "                                input_init,\n",
    "                                measure,\n",
    "                                measure_qubits)\n",
    "\n",
    "\n",
    "    post_processing = 'raw_parity'\n",
    "    policy_type = PolicyType(n_qubits, n_actions, post_processing)\n",
    "\n",
    "\n",
    "    pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "    \n",
    "\n",
    "    pqc = pqc\n",
    "    lr_list= [[0.01,0.01],[0.025,0.01],[0.05,0.01],[0.1,0.01],[0.01,0.25],[0.01,0.05],[0.01,0.1],[0.25,0.25],[0.50,0.50],[0.1,0.1]]\n",
    "    params= [circuit.parameters() for i in range(len(lr_list))]\n",
    "    optimizers= [create_optimizer_with_lr(param, lr, use_amsgrad=True) for param,lr in zip(params,lr_list)]\n",
    "    env_name = 'CartPole-v1'\n",
    "    env = gym.make(env_name)\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 10\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')  \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(pqc, optimizers[i], env, env_name, n_episodes, max_t, gamma, print_every, verbose, lr_list[i], rundate) for i in range(num_agents))\n",
    "    print(results)\n",
    "    time.sleep(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
