{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pennylane as qml \n",
    "import torch\n",
    "\n",
    "def create_circuit(n_qubits,n_layers=None,circ = \"simplified_two_design\",fim=False, shots=None):\n",
    "\n",
    "    dev = qml.device(\"default.qubit.torch\", wires=n_qubits, shots=shots)\n",
    "\n",
    "    def RZRY(params):\n",
    "        #qml.SpecialUnitary(params, wires=range(n_qubits))\n",
    "        #qml.SimplifiedTwoDesign(initial_layer_weights=init_params, weights=params, wires=range(n_qubits))\n",
    "        #qml.AngleEmbedding(params,wires=range(n_qubits))\n",
    "        for q in range(n_qubits):\n",
    "            qml.Hadamard(wires=q)\n",
    "\n",
    "        for w in range(n_layers): \n",
    "            for q in range(n_qubits):\n",
    "                index = w * (2*n_qubits) + q * 2\n",
    "                qml.RZ(params[index],wires=q)\n",
    "                qml.RY(params[index + 1],wires=q)\n",
    "        \n",
    "        qml.broadcast(qml.CNOT , wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "        \n",
    "        return qml.probs(wires=range(n_qubits))\n",
    "\n",
    "    def S2D(init_params,params,measurement_qubits=0,prod_approx=False):\n",
    "        #qml.SpecialUnitary(params, wires=range(n_qubits))\n",
    "        qml.SimplifiedTwoDesign(initial_layer_weights=init_params, weights=params, wires=range(n_qubits))\n",
    "        \n",
    "        #qml.broadcast(qml.CNOT , wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "        if not prod_approx:\n",
    "            return qml.probs(wires=list(range(measurement_qubits)))\n",
    "        else:\n",
    "            return [qml.probs(i) for i in range(measurement_qubits)]\n",
    "\n",
    "    def SU(params):\n",
    "        qml.SpecialUnitary(params, wires=range(n_qubits))\n",
    "        \n",
    "        ZZ = qml.operation.Tensor(qml.PauliZ(0), qml.PauliZ(1))\n",
    "        for i in range(2,n_qubits):\n",
    "            ZZ = qml.operation.Tensor(ZZ, qml.PauliZ(i))\n",
    "\n",
    "        return qml.expval(ZZ)\n",
    "    \n",
    "    def simmpleRZRY(params,cnots=True):\n",
    "        qml.broadcast(qml.Hadamard, wires=range(n_qubits), pattern=\"single\")\n",
    "        qml.broadcast(qml.RZ, wires=range(n_qubits), pattern=\"single\", parameters=params[0])\n",
    "        qml.broadcast(qml.RY, wires=range(n_qubits), pattern=\"single\", parameters=params[1])\n",
    "        if cnots:\n",
    "            qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"chain\")\n",
    "\n",
    "            return qml.expval(qml.PauliZ(n_qubits-1))\n",
    "        else:\n",
    "            ZZ = qml.operation.Tensor(qml.PauliZ(0), qml.PauliZ(1))\n",
    "            for i in range(2,n_qubits):\n",
    "                ZZ = qml.operation.Tensor(ZZ, qml.PauliZ(i))\n",
    "\n",
    "            return qml.expval(ZZ)\n",
    "        \n",
    "    def RY(params,y=True,probs=False,prod=False, entanglement=None):\n",
    "        #qml.broadcast(qml.Hadamard, wires=range(n_qubits), pattern=\"single\")\n",
    "        qml.broadcast(qml.RY, wires=range(n_qubits), pattern=\"single\", parameters=params)\n",
    "        #qml.broadcast(qml.CZ, wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "\n",
    "        if entanglement==\"all_to_all\":\n",
    "            qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "        \n",
    "        if y==True:\n",
    "            #YY = qml.operation.Tensor(qml.PauliY(0), qml.PauliY(1))\n",
    "            YY = [qml.PauliZ(0), qml.PauliZ(1)]\n",
    "            for i in range(2,n_qubits):\n",
    "                #YY = qml.operation.Tensor(YY, qml.PauliY(i))\n",
    "                YY.append(qml.PauliZ(i))\n",
    "            \n",
    "            #return [qml.expval(i) for i in YY]\n",
    "            return qml.expval(YY)\n",
    "\n",
    "        elif probs==False:\n",
    "\n",
    "            ZZ = qml.operation.Tensor(qml.PauliZ(0), qml.PauliZ(1))\n",
    "            #ZZ = [qml.PauliZ(0), qml.PauliZ(1)]\n",
    "            for i in range(2,n_qubits):\n",
    "                ZZ = qml.operation.Tensor(ZZ, qml.PauliZ(i))        \n",
    "                #ZZ.append(qml.PauliZ(i))        \n",
    "\n",
    "            #return [qml.expval(i) for i in ZZ]\n",
    "            return qml.expval(ZZ)\n",
    "\n",
    "        else:\n",
    "            if prod:\n",
    "                return [qml.probs(i) for i in range(n_qubits)]\n",
    "            else:\n",
    "                return qml.probs(wires=range(n_qubits))\n",
    "            \n",
    "        \n",
    "        \n",
    "    def GHZ(params,measurement_qubits=0):\n",
    "        qml.RY(params,wires=0)\n",
    "        qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"chain\")\n",
    "\n",
    "        return qml.probs(wires=range(measurement_qubits))\n",
    "\n",
    "    def random_product_state(params,gate_sequence=None):\n",
    "                \n",
    "        for i in range(n_qubits):\n",
    "            qml.RY(np.pi / 4, wires=i)\n",
    "\n",
    "        for ll in range(len(params)):\n",
    "\n",
    "            for i in range(n_qubits):\n",
    "                gate_sequence[\"{}{}\".format(ll,i)](params[ll][i], wires=i)\n",
    "\n",
    "            #for i in range(n_qubits - 1):\n",
    "                #qml.CZ(wires=[i, i + 1])\n",
    "    def SEL(params, measurement_qubits=0):\n",
    "        qml.StronglyEntanglingLayers(params, wires=range(n_qubits))\n",
    "        return qml.probs(wires=range(measurement_qubits))\n",
    "    \n",
    "    def RL(params, measurement_qubits=0):\n",
    "        qml.RandomLayers(params, ratio_imprim=0.8 ,imprimitive=qml.CZ, wires=range(n_qubits))\n",
    "        return qml.probs(wires=range(measurement_qubits))\n",
    "    \n",
    "    if circ == \"rzry\":\n",
    "        qcircuit = RZRY\n",
    "    elif circ == \"simplified_two_design\":\n",
    "        qcircuit = S2D\n",
    "    elif circ == \"special_unitary\":\n",
    "        qcircuit = SU\n",
    "    elif circ == \"simpleRZRY\":\n",
    "        qcircuit = simmpleRZRY\n",
    "    elif circ == \"RY\":\n",
    "        qcircuit = RY\n",
    "    elif circ == \"ghz\":\n",
    "        qcircuit = GHZ\n",
    "    elif circ == \"random_product_state\":\n",
    "        qcircuit = random_product_state\n",
    "    elif circ == \"SEL\":\n",
    "        qcircuit = SEL\n",
    "    elif circ == \"RL\":\n",
    "        qcircuit = RL\n",
    "    if not fim:\n",
    "        circuit = qml.QNode(qcircuit, dev,interface=\"torch\", diff_method=\"backprop\")\n",
    "    else:\n",
    "        circuit = qml.QNode(qcircuit, dev)\n",
    "\n",
    "    return circuit\n",
    "\n",
    "def compute_gradient(log_prob, w):\n",
    "    \"\"\"Compute gradient of the log probability with respect to weights.\n",
    "    \n",
    "    Args:\n",
    "    - log_prob (torch.Tensor): The log probability tensor.\n",
    "    - w (torch.Tensor): The weights tensor, with requires_grad=True.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The gradient of log_prob with respect to w, flattened.\n",
    "    \"\"\"\n",
    "    if w.grad is not None:\n",
    "        w.grad.zero_()\n",
    "    log_prob.backward(retain_graph=True)\n",
    "    \n",
    "    if w.grad is None:\n",
    "        raise ValueError(\"The gradient for the given log_prob with respect to w is None.\")\n",
    "    \n",
    "    return w.grad.view(-1).detach().numpy()\n",
    "\n",
    "def policy(probs, policy_type=\"contiguous-like\", n_actions=2, n_qubits=1):\n",
    "\n",
    "    if policy_type == \"contiguous-like\":\n",
    "        return probs\n",
    "    elif policy_type == \"parity-like\":\n",
    "        policy = torch.zeros(n_actions)\n",
    "        for i in range(len(probs)):\n",
    "            a=[]\n",
    "            for m in range(int(np.log2(n_actions))):\n",
    "                if m==0:    \n",
    "                    bitstring = np.binary_repr(i,width=n_qubits)\n",
    "                else:\n",
    "                    bitstring = np.binary_repr(i,width=n_qubits)[:-m]\n",
    "                \n",
    "                a.append(bitstring.count(\"1\") % 2)\n",
    "            policy[int(\"\".join(str(x) for x in a),2)] += probs[i]\n",
    "\n",
    "        return policy    \n",
    "    \n",
    "def compute_policy_and_gradient(args):\n",
    "    n_qubits, shapes, type , n_actions, policy_type, clamp = args\n",
    "\n",
    "    if policy_type == \"parity-like\":\n",
    "        measure_qubits = n_qubits\n",
    "    else:\n",
    "        measure_qubits = int(np.log2(n_actions))\n",
    "\n",
    "    qc = create_circuit(n_qubits, circ=type, fim=False, shots=None)\n",
    "\n",
    "    if type == \"simplified_two_design\":\n",
    "        weights = [np.random.uniform(-np.pi,np.pi,size=shape) for shape in shapes]    \n",
    "        weights_tensor_init = torch.tensor(weights[0], requires_grad=False)\n",
    "        weights_tensor_params = torch.tensor(weights[1], requires_grad=True)\n",
    "        \n",
    "        probs = qc(weights_tensor_init,weights_tensor_params, measurement_qubits=measure_qubits)\n",
    "\n",
    "    else:\n",
    "        weights = [np.random.uniform(-np.pi,np.pi,size=shape) for shape in shapes]    \n",
    "        weights_tensor_params = torch.tensor(weights, requires_grad=True)\n",
    "\n",
    "        probs = qc(weights_tensor_params, measurement_qubits=measure_qubits)\n",
    "\n",
    "    pi = policy(probs, policy_type=policy_type, n_actions=n_actions, n_qubits=n_qubits)\n",
    "    if clamp is not None:\n",
    "        pi = torch.clamp(pi, clamp, 1)\n",
    "\n",
    "    dist = torch.distributions.Categorical(probs=pi)\n",
    "    \n",
    "    action = dist.sample()\n",
    "    log_prob = dist.log_prob(action)\n",
    "\n",
    "    gradient_no_clamp = np.linalg.norm(compute_gradient(log_prob, weights_tensor_params), 2)\n",
    "    return gradient_no_clamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(policy, optimizer, env, n_episodes=1000, max_t=1000, gamma=1.0, print_every=5):\n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "    scores = []\n",
    "    average_scores = []\n",
    "    runtime_sum = 0\n",
    "    for e in range(1, n_episodes):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        # Collect trajectory\n",
    "        for t in range(max_t):\n",
    "            # Sample the action from current policy\n",
    "            if t==0:\n",
    "                state_tensor = torch.tensor(state[0]).float()\n",
    "            else:\n",
    "                state_tensor = torch.tensor(state).float()\n",
    "            action, log_prob, _, = policy.sample(state_tensor)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    # Total expected reward\n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "    #standardized returns\n",
    "        R=0\n",
    "        policy_loss = []\n",
    "        returns = []\n",
    "        for r in rewards[::-1]:\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0,R)\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + np.finfo(np.float32).eps)\n",
    "\n",
    "        for log_prob, R in zip(saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "\n",
    "        policy_unsqueezed = [torch.unsqueeze(loss, 0) for loss in policy_loss]\n",
    "        policy_sum = torch.cat(policy_unsqueezed).sum()\n",
    "\n",
    "    # Backpropagation\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        policy_sum.backward()\n",
    "        optimizer.step()\n",
    "        end_time = time.time()\n",
    "        runtime = end_time-start_time\n",
    "        \n",
    "        runtime_sum += runtime\n",
    "        if e % print_every == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tLast {}\\tEpisodes average reward: {:.2f}\\tRuntime: {:.2f}'.format(e, scores_deque[-1], print_every, np.mean(scores_deque), runtime_sum))\n",
    "            runtime_sum = 0\n",
    "        if np.mean(scores_deque) == 500:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(e, np.mean(scores_deque)))\n",
    "            break\n",
    "    return scores, policy.gradient_list, average_scores  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def save_training_data(self):\n",
    "        ''' \n",
    "        Saves training data into json files\n",
    "        '''\n",
    "        current_directory = os.path.dirname(__file__)\n",
    "        folder_name = f\"{str(self.env_name)}_{self.pqc.policy.post_processing}_{self.pqc.circuit.n_layers}\"\n",
    "        folder_path = os.path.join(current_directory, folder_name)\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "        episode_data = [self.scores_deque, \n",
    "                        self.runtime, \n",
    "                        self.loss.item(), \n",
    "                        tensor_to_list(self.pqc.get_gradients()[0]), \n",
    "                        tensor_to_list(self.pqc.get_gradients()[1])]\n",
    "\n",
    "        if folder_path is not None:\n",
    "            file_path = os.path.join(self.folder_path, f\"{self.file_name}.json\")\n",
    "            if os.path.exists(file_path):\n",
    "                with open(file_path, 'r') as f:\n",
    "                    existing_data = json.load(f)\n",
    "                existing_data.append(episode_data)\n",
    "                with open(file_path, 'w') as f:\n",
    "                    json.dump(existing_data, f, indent=4)\n",
    "            else:\n",
    "                with open(file_path, 'w') as f:\n",
    "                    json.dump([episode_data], f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import gym\n",
    "from collections import deque\n",
    "\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "\n",
    "def tensor_to_list(tensor):\n",
    "    \"\"\"\n",
    "    Convert a tensor or numpy array to a nested list.\n",
    "    \"\"\"\n",
    "    if isinstance(tensor, list):\n",
    "        return [tensor_to_list(t) for t in tensor]\n",
    "    elif isinstance(tensor, dict):\n",
    "        return {key: tensor_to_list(value) for key, value in tensor.items()}\n",
    "    elif isinstance(tensor, np.ndarray):\n",
    "        return tensor.tolist()\n",
    "    elif isinstance(tensor, torch.Tensor):\n",
    "        return tensor.tolist()\n",
    "    else:\n",
    "        return tensor\n",
    "\n",
    "def measure_probs(qubits):\n",
    "    return qml.probs(wires=range(qubits)) \n",
    "\n",
    "def measure_expval_pairs(qubits):\n",
    "    expvals = []\n",
    "    for i in range(qubits // 2):\n",
    "        expvals.append(qml.expval(qml.PauliZ(2*i) @ qml.PauliZ(2*i + 1)))\n",
    "    return expvals\n",
    "    \n",
    "def create_optimizer_with_lr(params, lr_list):\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': params, 'lr': lr} for params, lr in zip(params, lr_list)\n",
    "    ])\n",
    "    return optimizer\n",
    "\n",
    "def jerbi_circuit(n_qubits, n_layers, shots, input_scaling, diff_method, weight_init, input_init, measure, measure_qubits):\n",
    "\n",
    "    if shots is None:\n",
    "        dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "    else:\n",
    "        dev = qml.device(\"default.qubit\", wires=n_qubits, shots=shots)\n",
    "    \n",
    "    if n_layers < 1:\n",
    "        raise ValueError(\"Number of layers can't take values below 1\")\n",
    "    \n",
    "    weight_shapes = {\"params\": (n_layers + 1, n_qubits, 2),\n",
    "                    \"input_params\": (n_layers, n_qubits, 2)}\n",
    "    init_method   = {\"params\": weight_init,\n",
    "                    \"input_params\": input_init}\n",
    "    \n",
    "    @qml.qnode(dev, interface='torch', diff_method=diff_method)\n",
    "    def qnode(inputs, params, input_params):\n",
    "    #in case n_qubits != input length\n",
    "        if n_qubits > len(inputs) and n_qubits % len(inputs) == 0:\n",
    "            multiplier = n_qubits // len(inputs)\n",
    "            inputs = torch.cat([inputs] * multiplier)\n",
    "        elif n_qubits != len(inputs) and n_qubits % len(inputs) != 0:\n",
    "            raise ValueError('Number of qubits cannot be divided by input lenght')\n",
    "\n",
    "    #hadamard\n",
    "        qml.broadcast(qml.Hadamard, wires=range(n_qubits), pattern=\"single\")\n",
    "        \n",
    "        for layer in range(n_layers):\n",
    "            for wire in range(n_qubits):\n",
    "                qml.RZ(params[layer][wire][0], wires=wire)\n",
    "                qml.RY(params[layer][wire][1], wires=wire)\n",
    "\n",
    "            qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"chain\")\n",
    "\n",
    "            if input_scaling:\n",
    "                for wire in range(n_qubits):\n",
    "                    qml.RY(input_params[layer][wire][0] * inputs[wire], wires=wire)\n",
    "                    qml.RZ(input_params[layer][wire][1] * inputs[wire], wires=wire)\n",
    "            else:\n",
    "                for wire in range(n_qubits):\n",
    "                    qml.RY(inputs[wire], wires=wire)\n",
    "                    qml.RZ(inputs[wire], wires=wire)\n",
    "\n",
    "        for wire in range(n_qubits):\n",
    "            qml.RZ(params[-1][wire][0], wires=wire)\n",
    "            qml.RY(params[-1][wire][1], wires=wire)\n",
    "            \n",
    "        return measure(measure_qubits)\n",
    "\n",
    "    model = qml.qnn.TorchLayer(qnode, weight_shapes=weight_shapes, init_method=init_method)  \n",
    "    \n",
    "    return model\n",
    "    \n",
    "def S2D(n_qubits, n_layers, shots, input_scaling, diff_method, weight_init, input_init, measure_type, observables, measure_qubits):\n",
    "\n",
    "    dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "    observables = observables if observables is not None else None\n",
    "    \n",
    "    shapes = qml.SimplifiedTwoDesign.shape(n_layers=n_layers, n_wires=n_qubits)\n",
    "\n",
    "    weight_shapes = {\"params\": shapes[1],\n",
    "                     \"input_params\": shapes[0]}\n",
    "    \n",
    "    init_method   = {\"params\": weight_init,\n",
    "                     \"input_params\": input_init}\n",
    "\n",
    "    @qml.qnode(dev, interface='torch', diff_method='parameter-shift')\n",
    "    def qnode(inputs, params, input_params):\n",
    "\n",
    "        return measure_selection(measure_type, observables, measure_qubits)\n",
    "\n",
    "    model = qml.qnn.TorchLayer(qnode, weight_shapes=weight_shapes, init_method=init_method)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircuitGenerator(nn.Module):\n",
    "\n",
    "    def __init__(self, n_qubits, n_layers,  shots= None, input_scaling=True, design='jerbi_circuit', diff_method = 'backprop', weight_init=torch.nn.init.uniform_, input_init = torch.nn.init.ones_, measure = None, measure_qubits = None):\n",
    "        super(CircuitGenerator, self).__init__()\n",
    "        '''\n",
    "\n",
    "        Creates a parameterized quantum circuit based on the arguments:\n",
    "\n",
    "            n_qubits(int) = Number of qubits\n",
    "            n_layers(int) = Number of layers (0 if no data re-uploading)\n",
    "            shots(int) = Number of times the circuit gets executed\n",
    "            input_scaling(bool) = Input parameters are used if True (input*input_params)\n",
    "            design(str) = The PQC ansatz design ('jerbi_circuit')\n",
    "            diff_method(str) = Differentiation method ('best', 'backprop', 'parameter-shift', ...)\n",
    "            weight_init (torch.nn.init) = How PQC weights are initialized (.uniform_, .ones_, ...)\n",
    "            input_init (torch.nn.init) = How input weights are initialized (.uniform_, .ones_, ...)\n",
    "            measure (function) = Measure function (measure_probs, measure_expval_pairs)\n",
    "            measure_qubits (int) = Number of qubits to be measured (in some cases might be equal to the number of qubits)\n",
    "            \n",
    "        '''\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.shots = shots\n",
    "        self.input_scaling = input_scaling\n",
    "        self.design = design\n",
    "        self.diff_method = diff_method\n",
    "        self.weight_init = weight_init\n",
    "        self.input_init = input_init\n",
    "        if measure is None:\n",
    "            self.measure = measure_probs\n",
    "        else:\n",
    "            self.measure = measure\n",
    "\n",
    "        if measure_qubits is None:\n",
    "            self.measure_qubits = n_qubits\n",
    "        else:\n",
    "            self.measure_qubits = measure_qubits\n",
    "\n",
    "        if self.design == 'jerbi_circuit':\n",
    "            self.circuit = jerbi_circuit(n_qubits = self.n_qubits,\n",
    "                                        n_layers = self.n_layers,\n",
    "                                        shots = self.shots,\n",
    "                                        input_scaling = self.input_scaling,\n",
    "                                        diff_method = self.diff_method,\n",
    "                                        weight_init = self.weight_init,\n",
    "                                        input_init = self.input_init,\n",
    "                                        measure = self.measure,\n",
    "                                        measure_qubits = self.measure_qubits)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported circuit type\")\n",
    "\n",
    "    def input(self,inputs):\n",
    "\n",
    "        outputs = self.circuit(inputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyType(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_qubits, n_actions, post_processing = 'raw_contiguous'):\n",
    "        self.n_actions = n_actions\n",
    "        self.n_qubits = n_qubits\n",
    "        self.post_processing = post_processing\n",
    "        self.T = 1\n",
    "\n",
    "    def input(self,probs):\n",
    "        if self.post_processing == 'raw_contiguous':\n",
    "            policy = self.raw_contiguous(probs)\n",
    "        elif self.post_processing == 'raw_parity':\n",
    "            policy = self.raw_parity(probs)\n",
    "        elif self.post_processing == 'softmax':\n",
    "            policy = self.softmax(probs)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid post-processing method specified.\")\n",
    "        return policy\n",
    "\n",
    "    def raw_contiguous(self,probs):\n",
    "        \n",
    "        probs_flatten = probs.flatten()\n",
    "        chunk_size = len(probs_flatten) // self.n_actions\n",
    "        remainder = len(probs_flatten) % self.n_actions\n",
    "\n",
    "        policy = []\n",
    "\n",
    "        for i in range(self.n_actions):\n",
    "            start = i * chunk_size\n",
    "            end = (i + 1) * chunk_size\n",
    "\n",
    "            if i < remainder:\n",
    "                end += 1\n",
    "\n",
    "            # Update the original policy list instead of creating a new one\n",
    "            policy.append(sum(probs_flatten[start:end]))\n",
    "\n",
    "        policy_tensor = torch.stack(policy)\n",
    "        return policy_tensor\n",
    "        \n",
    "    def raw_parity(self,probs):\n",
    "\n",
    "        if self.n_actions % 2 != 0:\n",
    "            raise ValueError('For parity-like policy, n_actions must be an even number')\n",
    "        \n",
    "        probs_flatten = probs.flatten()\n",
    "        policy = torch.zeros(self.n_actions)\n",
    "        counter = 0\n",
    "        for prob in probs_flatten:\n",
    "            policy[counter] += prob\n",
    "            counter += 1\n",
    "            if counter == self.n_actions:\n",
    "                counter = 0\n",
    "        \n",
    "        return policy\n",
    "    \n",
    "    def softmax(self, probs):\n",
    "        if len(probs) == self.n_actions:\n",
    "            scaled_output = probs * self.T\n",
    "            softmax_output = F.softmax(scaled_output, dim=0)\n",
    "            return softmax_output\n",
    "        else:\n",
    "            probs_flatten = probs.flatten()\n",
    "            chunk_size = len(probs_flatten) // self.n_actions\n",
    "            remainder = len(probs_flatten) % self.n_actions\n",
    "\n",
    "            policy = []\n",
    "\n",
    "            for i in range(self.n_actions):\n",
    "                start = i * chunk_size\n",
    "                end = (i + 1) * chunk_size\n",
    "\n",
    "                if i < remainder:\n",
    "                    end += 1\n",
    "\n",
    "                # Update the original policy list instead of creating a new one\n",
    "                policy.append(sum(probs_flatten[start:end]))\n",
    "            policy_tensor = torch.stack(policy)\n",
    "            softmax_output = F.softmax(policy_tensor/self.T, dim=0)\n",
    "            return softmax_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumPolicyModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, circuit, policy):\n",
    "        super(QuantumPolicyModel, self).__init__()\n",
    "        self.circuit = circuit\n",
    "        self.policy = policy\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        Input state is fed to the circuit - its output is then fed to the post processing \n",
    "        '''\n",
    "        probs = self.circuit.input(inputs)\n",
    "        probs_processed = self.policy.input(probs)\n",
    "        return probs_processed\n",
    "    \n",
    "    def sample(self, inputs):\n",
    "        '''\n",
    "        Samples an action from the action probability distribution aka policy\n",
    "        '''\n",
    "        policy = self.forward(inputs)\n",
    "        dist = torch.distributions.Categorical(policy)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action), policy\n",
    "    \n",
    "    def T_schedule(self,current_step,total_steps):\n",
    "\n",
    "        max_T = 1.0  # Initial temperature\n",
    "        min_T = 0.1  # Final temperature\n",
    "        decay_rate = 0.0025  # Decay rate\n",
    "\n",
    "        self.policy.T = max_T * (1 - decay_rate * current_step / total_steps)\n",
    "        self.policy.T = max(self.policy.T, min_T)\n",
    "\n",
    "    def get_parameters(self):\n",
    "        '''\n",
    "        Returns the values of each set of parameters\n",
    "        '''\n",
    "        parameter_values = [param.clone().detach().numpy().flatten() for param in self.circuit.parameters()]\n",
    "        return parameter_values\n",
    "    \n",
    "    def get_gradients(self):\n",
    "        '''\n",
    "        Returns the gradient values of each set of parameters\n",
    "        '''\n",
    "        gradients = [torch.flatten(param.grad.clone().detach()) if param.grad is not None else torch.flatten(torch.zeros_like(param)) for name, param in self.circuit.named_parameters()]\n",
    "        return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceUpdate():\n",
    "\n",
    "    def __init__(self, pqc, optimizer, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name = None):\n",
    "        \n",
    "        self.pqc = pqc\n",
    "        self.optimizer = optimizer\n",
    "        self.env = env\n",
    "        self.env_name = env_name\n",
    "        self.n_episodes = n_episodes\n",
    "        self.max_t = max_t\n",
    "        self.gamma = gamma\n",
    "        self.scores_deque = deque(maxlen=print_every)\n",
    "        self.print_every = print_every\n",
    "        self.verbose = verbose\n",
    "        self.file_name = file_name\n",
    "        self.running_reward = 10\n",
    "\n",
    "    def get_trajectory(self):\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        state = self.env.reset()\n",
    "        for t in range(self.max_t):\n",
    "            if t == 0:\n",
    "                state_tensor = torch.tensor(state[0]).float()\n",
    "            else:\n",
    "                state_tensor = torch.tensor(state).float()\n",
    "            action, log_prob, _, = self.pqc.sample(state_tensor)\n",
    "            state, reward, done, _, _ = self.env.step(action)\n",
    "            \n",
    "            self.saved_log_probs.append(log_prob)\n",
    "            self.rewards.append(reward)\n",
    "\n",
    "            if done:\n",
    "                self.scores_deque.append(sum(self.rewards))\n",
    "                break\n",
    "\n",
    "    def update_policy(self):\n",
    "\n",
    "        R = 0\n",
    "        policy_loss = []\n",
    "        returns = []\n",
    "        for r in self.rewards[::-1]:\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "\n",
    "        for log_prob, ret in zip(self.saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * ret)\n",
    "\n",
    "        policy_unsqueezed = [torch.unsqueeze(loss, 0) for loss in policy_loss]\n",
    "        self.loss = torch.cat(policy_unsqueezed).sum()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        self.loss.backward()    \n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def save_agent_data(self,path):\n",
    "\n",
    "        agent_variables = {\n",
    "            \"Number of Qubits\": self.pqc.circuit.n_qubits,\n",
    "            \"Number of Layers\": self.pqc.circuit.n_layers,\n",
    "            \"Shots\": self.pqc.circuit.shots,\n",
    "            \"Input Scaling\": self.pqc.circuit.input_scaling,\n",
    "            \"Design\": self.pqc.circuit.design,\n",
    "            \"Differentiation Method\": self.pqc.circuit.diff_method,\n",
    "            \"Weight Initiation\": str(self.pqc.circuit.weight_init),\n",
    "            \"Input_init\": str(self.pqc.circuit.input_init),\n",
    "            \"Measure\": str(self.pqc.circuit.measure),\n",
    "            \"Measure Qubits\": self.pqc.circuit.measure_qubits,\n",
    "            \"Policy Type\": self.pqc.policy.post_processing,\n",
    "            \"Optimizers\": str(self.optimizer),\n",
    "            \"Envinronment Name\": str(self.env_name),\n",
    "            \"Gamma (discounting factor)\": self.gamma,\n",
    "        }\n",
    "\n",
    "        with open(os.path.join(path, \"agent_characteristics.json\"), \"w\") as f:\n",
    "            json.dump(agent_variables, f, indent=4)\n",
    "\n",
    "    def save_gradients(self,path):\n",
    "\n",
    "        run = os.path.join(path,str(self.file_name)+'.npz')\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "        if os.path.exists(run):\n",
    "            data = np.load(run, allow_pickle=True)\n",
    "            old_gradients_A = data['gradients_A'].tolist()\n",
    "            print(old_gradients_A)\n",
    "            old_gradients_B = data['gradients_B'].tolist()\n",
    "        else:\n",
    "\n",
    "            old_gradients_A = []\n",
    "            old_gradients_B = []\n",
    "\n",
    "\n",
    "        old_gradients_A.append(tensor_to_list(self.pqc.get_gradients()[0]))\n",
    "        old_gradients_B.append(tensor_to_list(self.pqc.get_gradients()[1]))\n",
    "\n",
    "        np.savez_compressed(run, gradients_A=np.array(old_gradients_A), gradients_B=np.array(old_gradients_B))\n",
    "        \n",
    "        del old_gradients_A[:]\n",
    "        del old_gradients_B[:]\n",
    "\n",
    "    def writer_function(self, writer, iteration):\n",
    "\n",
    "        writer.add_scalar(\"Episode Reward\", np.mean(self.scores_deque), global_step=iteration)\n",
    "        writer.add_scalar(\"Running Reward\", self.running_reward, global_step=iteration)\n",
    "        writer.add_scalar(\"Runtime\", self.runtime, global_step=iteration)\n",
    "        writer.add_scalar(\"Loss\", self.loss.item(), global_step=iteration)\n",
    "\n",
    "    def train(self):\n",
    "        \n",
    "        logs_dir = \"data\"\n",
    "        os.makedirs(logs_dir, exist_ok=True)\n",
    "        envinronment_folder = os.path.join(logs_dir, self.env_name)\n",
    "        os.makedirs(envinronment_folder, exist_ok=True)\n",
    "        experiment_folder = f\"{self.pqc.policy.post_processing}_{self.pqc.circuit.n_layers}layer\"\n",
    "        experiment_path = os.path.join(envinronment_folder, experiment_folder)\n",
    "        os.makedirs(experiment_path, exist_ok=True)\n",
    "        run = os.path.join(experiment_path,str(self.file_name))\n",
    "        os.makedirs(run, exist_ok=True)\n",
    "        writer = SummaryWriter(log_dir=run)\n",
    "        self.save_agent_data(experiment_path)\n",
    "        \n",
    "        for i in range(1, self.n_episodes):\n",
    "            start_time = time.time()\n",
    "            self.get_trajectory()\n",
    "            self.pqc.T_schedule(i,self.n_episodes)\n",
    "            self.update_policy()\n",
    "            end_time = time.time()\n",
    "            self.runtime = end_time - start_time\n",
    "            self.writer_function(writer,i)\n",
    "            self.save_gradients(run)\n",
    "            self.running_reward = (self.running_reward * 0.99) + (len(self.rewards) * 0.01)\n",
    "            \n",
    "            if self.running_reward > self.env.spec.reward_threshold:\n",
    "                print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i, np.mean(self.scores_deque)))\n",
    "                break\n",
    "            elif i % self.print_every == 0 and self.verbose == 1:\n",
    "                print('Episode {}\\tLast reward: {:.2f}\\tLast {} Episodes average reward: {:.2f}\\tRuntime: {:.2f}\\t {:.2f}\\t'.format(i, self.scores_deque[-1], self.print_every, np.mean(self.scores_deque), self.runtime, self.running_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.5123486518859863, 2.044293165206909, 0.24636557698249817, -1.0608521699905396, -1.4727836727956856e-08, -4.865615466087547e-08, -1.2858444620178489e-07, 3.249658675485989e-08, 0.07399863004684448, 0.7378922700881958, 1.382431946694851e-09, 6.658442686102717e-08, -7.2177499532699585e-09, -1.8556338687858442e-08, 6.004847818985581e-08, 2.550509670129486e-08]]\n",
      "[[1.5123486518859863, 2.044293165206909, 0.24636557698249817, -1.0608521699905396, -1.4727836727956856e-08, -4.865615466087547e-08, -1.2858444620178489e-07, 3.249658675485989e-08, 0.07399863004684448, 0.7378922700881958, 1.382431946694851e-09, 6.658442686102717e-08, -7.2177499532699585e-09, -1.8556338687858442e-08, 6.004847818985581e-08, 2.550509670129486e-08], [3.332907199859619, 4.503681182861328, 0.4967535734176636, -2.1042141914367676, -4.230965373608342e-07, -3.0833496111881686e-07, 8.368549231363431e-08, 3.6694736138542794e-09, 0.18886202573776245, 1.7900187969207764, -1.8655555322766304e-08, 2.4049882085819263e-07, 4.493631422519684e-08, -8.922214078666002e-10, -2.8445356292650104e-08, 5.146169002046008e-08]]\n",
      "[[1.5123486518859863, 2.044293165206909, 0.24636557698249817, -1.0608521699905396, -1.4727836727956856e-08, -4.865615466087547e-08, -1.2858444620178489e-07, 3.249658675485989e-08, 0.07399863004684448, 0.7378922700881958, 1.382431946694851e-09, 6.658442686102717e-08, -7.2177499532699585e-09, -1.8556338687858442e-08, 6.004847818985581e-08, 2.550509670129486e-08], [3.332907199859619, 4.503681182861328, 0.4967535734176636, -2.1042141914367676, -4.230965373608342e-07, -3.0833496111881686e-07, 8.368549231363431e-08, 3.6694736138542794e-09, 0.18886202573776245, 1.7900187969207764, -1.8655555322766304e-08, 2.4049882085819263e-07, 4.493631422519684e-08, -8.922214078666002e-10, -2.8445356292650104e-08, 5.146169002046008e-08], [0.03376662731170654, 0.09216567873954773, -0.018372010439634323, 0.07669146358966827, 2.9228939268932663e-08, -1.306833929959339e-08, -4.629336558537034e-08, -6.414669684318142e-09, 0.0024664513766765594, 0.11971600353717804, 4.802132025361061e-10, -5.557170723591298e-08, 1.123407855629921e-08, 1.070414601400671e-08, 4.729372449219227e-11, -9.237263398631512e-09]]\n",
      "[[1.5123486518859863, 2.044293165206909, 0.24636557698249817, -1.0608521699905396, -1.4727836727956856e-08, -4.865615466087547e-08, -1.2858444620178489e-07, 3.249658675485989e-08, 0.07399863004684448, 0.7378922700881958, 1.382431946694851e-09, 6.658442686102717e-08, -7.2177499532699585e-09, -1.8556338687858442e-08, 6.004847818985581e-08, 2.550509670129486e-08], [3.332907199859619, 4.503681182861328, 0.4967535734176636, -2.1042141914367676, -4.230965373608342e-07, -3.0833496111881686e-07, 8.368549231363431e-08, 3.6694736138542794e-09, 0.18886202573776245, 1.7900187969207764, -1.8655555322766304e-08, 2.4049882085819263e-07, 4.493631422519684e-08, -8.922214078666002e-10, -2.8445356292650104e-08, 5.146169002046008e-08], [0.03376662731170654, 0.09216567873954773, -0.018372010439634323, 0.07669146358966827, 2.9228939268932663e-08, -1.306833929959339e-08, -4.629336558537034e-08, -6.414669684318142e-09, 0.0024664513766765594, 0.11971600353717804, 4.802132025361061e-10, -5.557170723591298e-08, 1.123407855629921e-08, 1.070414601400671e-08, 4.729372449219227e-11, -9.237263398631512e-09], [0.03240334987640381, 0.09539645910263062, -0.02006007730960846, 0.08288070559501648, 1.1596450022466342e-08, 1.687387296556153e-08, 3.4642567925402545e-08, -3.78847211379707e-08, 0.007899673655629158, 0.12504075467586517, 9.822542779147625e-10, -2.0188917204677637e-10, 1.461012288928032e-08, 7.664760381942415e-09, 2.0827428670600057e-09, -8.055483391444795e-09]]\n",
      "Episode 5\tLast reward: 13.00\tLast 5 Episodes average reward: 11.00\tRuntime: 0.16\t 10.05\t\n",
      "[[1.5123486518859863, 2.044293165206909, 0.24636557698249817, -1.0608521699905396, -1.4727836727956856e-08, -4.865615466087547e-08, -1.2858444620178489e-07, 3.249658675485989e-08, 0.07399863004684448, 0.7378922700881958, 1.382431946694851e-09, 6.658442686102717e-08, -7.2177499532699585e-09, -1.8556338687858442e-08, 6.004847818985581e-08, 2.550509670129486e-08], [3.332907199859619, 4.503681182861328, 0.4967535734176636, -2.1042141914367676, -4.230965373608342e-07, -3.0833496111881686e-07, 8.368549231363431e-08, 3.6694736138542794e-09, 0.18886202573776245, 1.7900187969207764, -1.8655555322766304e-08, 2.4049882085819263e-07, 4.493631422519684e-08, -8.922214078666002e-10, -2.8445356292650104e-08, 5.146169002046008e-08], [0.03376662731170654, 0.09216567873954773, -0.018372010439634323, 0.07669146358966827, 2.9228939268932663e-08, -1.306833929959339e-08, -4.629336558537034e-08, -6.414669684318142e-09, 0.0024664513766765594, 0.11971600353717804, 4.802132025361061e-10, -5.557170723591298e-08, 1.123407855629921e-08, 1.070414601400671e-08, 4.729372449219227e-11, -9.237263398631512e-09], [0.03240334987640381, 0.09539645910263062, -0.02006007730960846, 0.08288070559501648, 1.1596450022466342e-08, 1.687387296556153e-08, 3.4642567925402545e-08, -3.78847211379707e-08, 0.007899673655629158, 0.12504075467586517, 9.822542779147625e-10, -2.0188917204677637e-10, 1.461012288928032e-08, 7.664760381942415e-09, 2.0827428670600057e-09, -8.055483391444795e-09], [0.8873114585876465, 1.0608161687850952, 0.17477506399154663, -0.7165660262107849, -3.901226364178001e-08, 1.8607298812867157e-08, 2.5483674903625797e-07, 5.457665963604086e-08, 0.05422412231564522, 0.2683139443397522, -2.930755726993084e-08, 2.9067605211707814e-08, -5.384208634495735e-10, -1.0334025546399062e-07, 5.767105903942138e-08, -1.257318160696741e-08]]\n",
      "[[1.5123486518859863, 2.044293165206909, 0.24636557698249817, -1.0608521699905396, -1.4727836727956856e-08, -4.865615466087547e-08, -1.2858444620178489e-07, 3.249658675485989e-08, 0.07399863004684448, 0.7378922700881958, 1.382431946694851e-09, 6.658442686102717e-08, -7.2177499532699585e-09, -1.8556338687858442e-08, 6.004847818985581e-08, 2.550509670129486e-08], [3.332907199859619, 4.503681182861328, 0.4967535734176636, -2.1042141914367676, -4.230965373608342e-07, -3.0833496111881686e-07, 8.368549231363431e-08, 3.6694736138542794e-09, 0.18886202573776245, 1.7900187969207764, -1.8655555322766304e-08, 2.4049882085819263e-07, 4.493631422519684e-08, -8.922214078666002e-10, -2.8445356292650104e-08, 5.146169002046008e-08], [0.03376662731170654, 0.09216567873954773, -0.018372010439634323, 0.07669146358966827, 2.9228939268932663e-08, -1.306833929959339e-08, -4.629336558537034e-08, -6.414669684318142e-09, 0.0024664513766765594, 0.11971600353717804, 4.802132025361061e-10, -5.557170723591298e-08, 1.123407855629921e-08, 1.070414601400671e-08, 4.729372449219227e-11, -9.237263398631512e-09], [0.03240334987640381, 0.09539645910263062, -0.02006007730960846, 0.08288070559501648, 1.1596450022466342e-08, 1.687387296556153e-08, 3.4642567925402545e-08, -3.78847211379707e-08, 0.007899673655629158, 0.12504075467586517, 9.822542779147625e-10, -2.0188917204677637e-10, 1.461012288928032e-08, 7.664760381942415e-09, 2.0827428670600057e-09, -8.055483391444795e-09], [0.8873114585876465, 1.0608161687850952, 0.17477506399154663, -0.7165660262107849, -3.901226364178001e-08, 1.8607298812867157e-08, 2.5483674903625797e-07, 5.457665963604086e-08, 0.05422412231564522, 0.2683139443397522, -2.930755726993084e-08, 2.9067605211707814e-08, -5.384208634495735e-10, -1.0334025546399062e-07, 5.767105903942138e-08, -1.257318160696741e-08], [0.034695565700531006, 0.10977944731712341, -0.024307064712047577, 0.09885193407535553, 2.7953477754749656e-08, 2.7418035841719757e-08, 2.31591457122704e-09, 1.3481821881100586e-08, 0.0099908746778965, 0.14211377501487732, -1.1525116860866547e-08, 5.565392147133252e-09, 6.199115887284279e-09, -1.8119900246915677e-08, -9.957147995010018e-09, 7.450777772533002e-10]]\n",
      "[[1.5123486518859863, 2.044293165206909, 0.24636557698249817, -1.0608521699905396, -1.4727836727956856e-08, -4.865615466087547e-08, -1.2858444620178489e-07, 3.249658675485989e-08, 0.07399863004684448, 0.7378922700881958, 1.382431946694851e-09, 6.658442686102717e-08, -7.2177499532699585e-09, -1.8556338687858442e-08, 6.004847818985581e-08, 2.550509670129486e-08], [3.332907199859619, 4.503681182861328, 0.4967535734176636, -2.1042141914367676, -4.230965373608342e-07, -3.0833496111881686e-07, 8.368549231363431e-08, 3.6694736138542794e-09, 0.18886202573776245, 1.7900187969207764, -1.8655555322766304e-08, 2.4049882085819263e-07, 4.493631422519684e-08, -8.922214078666002e-10, -2.8445356292650104e-08, 5.146169002046008e-08], [0.03376662731170654, 0.09216567873954773, -0.018372010439634323, 0.07669146358966827, 2.9228939268932663e-08, -1.306833929959339e-08, -4.629336558537034e-08, -6.414669684318142e-09, 0.0024664513766765594, 0.11971600353717804, 4.802132025361061e-10, -5.557170723591298e-08, 1.123407855629921e-08, 1.070414601400671e-08, 4.729372449219227e-11, -9.237263398631512e-09], [0.03240334987640381, 0.09539645910263062, -0.02006007730960846, 0.08288070559501648, 1.1596450022466342e-08, 1.687387296556153e-08, 3.4642567925402545e-08, -3.78847211379707e-08, 0.007899673655629158, 0.12504075467586517, 9.822542779147625e-10, -2.0188917204677637e-10, 1.461012288928032e-08, 7.664760381942415e-09, 2.0827428670600057e-09, -8.055483391444795e-09], [0.8873114585876465, 1.0608161687850952, 0.17477506399154663, -0.7165660262107849, -3.901226364178001e-08, 1.8607298812867157e-08, 2.5483674903625797e-07, 5.457665963604086e-08, 0.05422412231564522, 0.2683139443397522, -2.930755726993084e-08, 2.9067605211707814e-08, -5.384208634495735e-10, -1.0334025546399062e-07, 5.767105903942138e-08, -1.257318160696741e-08], [0.034695565700531006, 0.10977944731712341, -0.024307064712047577, 0.09885193407535553, 2.7953477754749656e-08, 2.7418035841719757e-08, 2.31591457122704e-09, 1.3481821881100586e-08, 0.0099908746778965, 0.14211377501487732, -1.1525116860866547e-08, 5.565392147133252e-09, 6.199115887284279e-09, -1.8119900246915677e-08, -9.957147995010018e-09, 7.450777772533002e-10], [-0.8290196657180786, -1.0141005516052246, -0.14034652709960938, 0.5671282410621643, 7.710691818374471e-08, 4.3583835207527954e-08, 7.777228461236518e-08, -3.2445655051560607e-09, -0.04094263166189194, -0.3363668918609619, 1.7484126146882772e-08, -6.208754399494865e-09, -2.246815711259842e-08, -5.364343635960722e-09, 1.911757863126695e-08, -1.668153259970495e-08]]\n",
      "[[1.5123486518859863, 2.044293165206909, 0.24636557698249817, -1.0608521699905396, -1.4727836727956856e-08, -4.865615466087547e-08, -1.2858444620178489e-07, 3.249658675485989e-08, 0.07399863004684448, 0.7378922700881958, 1.382431946694851e-09, 6.658442686102717e-08, -7.2177499532699585e-09, -1.8556338687858442e-08, 6.004847818985581e-08, 2.550509670129486e-08], [3.332907199859619, 4.503681182861328, 0.4967535734176636, -2.1042141914367676, -4.230965373608342e-07, -3.0833496111881686e-07, 8.368549231363431e-08, 3.6694736138542794e-09, 0.18886202573776245, 1.7900187969207764, -1.8655555322766304e-08, 2.4049882085819263e-07, 4.493631422519684e-08, -8.922214078666002e-10, -2.8445356292650104e-08, 5.146169002046008e-08], [0.03376662731170654, 0.09216567873954773, -0.018372010439634323, 0.07669146358966827, 2.9228939268932663e-08, -1.306833929959339e-08, -4.629336558537034e-08, -6.414669684318142e-09, 0.0024664513766765594, 0.11971600353717804, 4.802132025361061e-10, -5.557170723591298e-08, 1.123407855629921e-08, 1.070414601400671e-08, 4.729372449219227e-11, -9.237263398631512e-09], [0.03240334987640381, 0.09539645910263062, -0.02006007730960846, 0.08288070559501648, 1.1596450022466342e-08, 1.687387296556153e-08, 3.4642567925402545e-08, -3.78847211379707e-08, 0.007899673655629158, 0.12504075467586517, 9.822542779147625e-10, -2.0188917204677637e-10, 1.461012288928032e-08, 7.664760381942415e-09, 2.0827428670600057e-09, -8.055483391444795e-09], [0.8873114585876465, 1.0608161687850952, 0.17477506399154663, -0.7165660262107849, -3.901226364178001e-08, 1.8607298812867157e-08, 2.5483674903625797e-07, 5.457665963604086e-08, 0.05422412231564522, 0.2683139443397522, -2.930755726993084e-08, 2.9067605211707814e-08, -5.384208634495735e-10, -1.0334025546399062e-07, 5.767105903942138e-08, -1.257318160696741e-08], [0.034695565700531006, 0.10977944731712341, -0.024307064712047577, 0.09885193407535553, 2.7953477754749656e-08, 2.7418035841719757e-08, 2.31591457122704e-09, 1.3481821881100586e-08, 0.0099908746778965, 0.14211377501487732, -1.1525116860866547e-08, 5.565392147133252e-09, 6.199115887284279e-09, -1.8119900246915677e-08, -9.957147995010018e-09, 7.450777772533002e-10], [-0.8290196657180786, -1.0141005516052246, -0.14034652709960938, 0.5671282410621643, 7.710691818374471e-08, 4.3583835207527954e-08, 7.777228461236518e-08, -3.2445655051560607e-09, -0.04094263166189194, -0.3363668918609619, 1.7484126146882772e-08, -6.208754399494865e-09, -2.246815711259842e-08, -5.364343635960722e-09, 1.911757863126695e-08, -1.668153259970495e-08], [0.10045400261878967, 0.2551442086696625, -0.03995509445667267, 0.1608608216047287, 5.935417490832151e-09, 3.96891319809356e-09, 2.426181211490075e-08, 1.657991255399338e-08, 0.033101312816143036, 0.27791571617126465, 1.4028046280145645e-08, -1.594285237160875e-08, 1.3969838619232178e-09, -2.5164625938600693e-08, 1.9136678020004183e-08, 1.375341263099017e-08]]\n",
      "[[1.5123486518859863, 2.044293165206909, 0.24636557698249817, -1.0608521699905396, -1.4727836727956856e-08, -4.865615466087547e-08, -1.2858444620178489e-07, 3.249658675485989e-08, 0.07399863004684448, 0.7378922700881958, 1.382431946694851e-09, 6.658442686102717e-08, -7.2177499532699585e-09, -1.8556338687858442e-08, 6.004847818985581e-08, 2.550509670129486e-08], [3.332907199859619, 4.503681182861328, 0.4967535734176636, -2.1042141914367676, -4.230965373608342e-07, -3.0833496111881686e-07, 8.368549231363431e-08, 3.6694736138542794e-09, 0.18886202573776245, 1.7900187969207764, -1.8655555322766304e-08, 2.4049882085819263e-07, 4.493631422519684e-08, -8.922214078666002e-10, -2.8445356292650104e-08, 5.146169002046008e-08], [0.03376662731170654, 0.09216567873954773, -0.018372010439634323, 0.07669146358966827, 2.9228939268932663e-08, -1.306833929959339e-08, -4.629336558537034e-08, -6.414669684318142e-09, 0.0024664513766765594, 0.11971600353717804, 4.802132025361061e-10, -5.557170723591298e-08, 1.123407855629921e-08, 1.070414601400671e-08, 4.729372449219227e-11, -9.237263398631512e-09], [0.03240334987640381, 0.09539645910263062, -0.02006007730960846, 0.08288070559501648, 1.1596450022466342e-08, 1.687387296556153e-08, 3.4642567925402545e-08, -3.78847211379707e-08, 0.007899673655629158, 0.12504075467586517, 9.822542779147625e-10, -2.0188917204677637e-10, 1.461012288928032e-08, 7.664760381942415e-09, 2.0827428670600057e-09, -8.055483391444795e-09], [0.8873114585876465, 1.0608161687850952, 0.17477506399154663, -0.7165660262107849, -3.901226364178001e-08, 1.8607298812867157e-08, 2.5483674903625797e-07, 5.457665963604086e-08, 0.05422412231564522, 0.2683139443397522, -2.930755726993084e-08, 2.9067605211707814e-08, -5.384208634495735e-10, -1.0334025546399062e-07, 5.767105903942138e-08, -1.257318160696741e-08], [0.034695565700531006, 0.10977944731712341, -0.024307064712047577, 0.09885193407535553, 2.7953477754749656e-08, 2.7418035841719757e-08, 2.31591457122704e-09, 1.3481821881100586e-08, 0.0099908746778965, 0.14211377501487732, -1.1525116860866547e-08, 5.565392147133252e-09, 6.199115887284279e-09, -1.8119900246915677e-08, -9.957147995010018e-09, 7.450777772533002e-10], [-0.8290196657180786, -1.0141005516052246, -0.14034652709960938, 0.5671282410621643, 7.710691818374471e-08, 4.3583835207527954e-08, 7.777228461236518e-08, -3.2445655051560607e-09, -0.04094263166189194, -0.3363668918609619, 1.7484126146882772e-08, -6.208754399494865e-09, -2.246815711259842e-08, -5.364343635960722e-09, 1.911757863126695e-08, -1.668153259970495e-08], [0.10045400261878967, 0.2551442086696625, -0.03995509445667267, 0.1608608216047287, 5.935417490832151e-09, 3.96891319809356e-09, 2.426181211490075e-08, 1.657991255399338e-08, 0.033101312816143036, 0.27791571617126465, 1.4028046280145645e-08, -1.594285237160875e-08, 1.3969838619232178e-09, -2.5164625938600693e-08, 1.9136678020004183e-08, 1.375341263099017e-08], [1.34907066822052, 1.766562581062317, 0.17100511491298676, -0.6865843534469604, 2.980851832035114e-08, -5.146631920638356e-08, 2.090910733443252e-08, 6.510987304864102e-08, 0.10457414388656616, 0.7760880589485168, 7.916241884231567e-09, 1.3706452861583784e-08, 3.9726728573441505e-08, -4.8544661623850516e-09, -1.5563273336738348e-08, -6.842591204758719e-08]]\n",
      "Episode 10\tLast reward: 9.00\tLast 5 Episodes average reward: 10.60\tRuntime: 0.11\t 10.08\t\n",
      "[[1.5123486518859863, 2.044293165206909, 0.24636557698249817, -1.0608521699905396, -1.4727836727956856e-08, -4.865615466087547e-08, -1.2858444620178489e-07, 3.249658675485989e-08, 0.07399863004684448, 0.7378922700881958, 1.382431946694851e-09, 6.658442686102717e-08, -7.2177499532699585e-09, -1.8556338687858442e-08, 6.004847818985581e-08, 2.550509670129486e-08], [3.332907199859619, 4.503681182861328, 0.4967535734176636, -2.1042141914367676, -4.230965373608342e-07, -3.0833496111881686e-07, 8.368549231363431e-08, 3.6694736138542794e-09, 0.18886202573776245, 1.7900187969207764, -1.8655555322766304e-08, 2.4049882085819263e-07, 4.493631422519684e-08, -8.922214078666002e-10, -2.8445356292650104e-08, 5.146169002046008e-08], [0.03376662731170654, 0.09216567873954773, -0.018372010439634323, 0.07669146358966827, 2.9228939268932663e-08, -1.306833929959339e-08, -4.629336558537034e-08, -6.414669684318142e-09, 0.0024664513766765594, 0.11971600353717804, 4.802132025361061e-10, -5.557170723591298e-08, 1.123407855629921e-08, 1.070414601400671e-08, 4.729372449219227e-11, -9.237263398631512e-09], [0.03240334987640381, 0.09539645910263062, -0.02006007730960846, 0.08288070559501648, 1.1596450022466342e-08, 1.687387296556153e-08, 3.4642567925402545e-08, -3.78847211379707e-08, 0.007899673655629158, 0.12504075467586517, 9.822542779147625e-10, -2.0188917204677637e-10, 1.461012288928032e-08, 7.664760381942415e-09, 2.0827428670600057e-09, -8.055483391444795e-09], [0.8873114585876465, 1.0608161687850952, 0.17477506399154663, -0.7165660262107849, -3.901226364178001e-08, 1.8607298812867157e-08, 2.5483674903625797e-07, 5.457665963604086e-08, 0.05422412231564522, 0.2683139443397522, -2.930755726993084e-08, 2.9067605211707814e-08, -5.384208634495735e-10, -1.0334025546399062e-07, 5.767105903942138e-08, -1.257318160696741e-08], [0.034695565700531006, 0.10977944731712341, -0.024307064712047577, 0.09885193407535553, 2.7953477754749656e-08, 2.7418035841719757e-08, 2.31591457122704e-09, 1.3481821881100586e-08, 0.0099908746778965, 0.14211377501487732, -1.1525116860866547e-08, 5.565392147133252e-09, 6.199115887284279e-09, -1.8119900246915677e-08, -9.957147995010018e-09, 7.450777772533002e-10], [-0.8290196657180786, -1.0141005516052246, -0.14034652709960938, 0.5671282410621643, 7.710691818374471e-08, 4.3583835207527954e-08, 7.777228461236518e-08, -3.2445655051560607e-09, -0.04094263166189194, -0.3363668918609619, 1.7484126146882772e-08, -6.208754399494865e-09, -2.246815711259842e-08, -5.364343635960722e-09, 1.911757863126695e-08, -1.668153259970495e-08], [0.10045400261878967, 0.2551442086696625, -0.03995509445667267, 0.1608608216047287, 5.935417490832151e-09, 3.96891319809356e-09, 2.426181211490075e-08, 1.657991255399338e-08, 0.033101312816143036, 0.27791571617126465, 1.4028046280145645e-08, -1.594285237160875e-08, 1.3969838619232178e-09, -2.5164625938600693e-08, 1.9136678020004183e-08, 1.375341263099017e-08], [1.34907066822052, 1.766562581062317, 0.17100511491298676, -0.6865843534469604, 2.980851832035114e-08, -5.146631920638356e-08, 2.090910733443252e-08, 6.510987304864102e-08, 0.10457414388656616, 0.7760880589485168, 7.916241884231567e-09, 1.3706452861583784e-08, 3.9726728573441505e-08, -4.8544661623850516e-09, -1.5563273336738348e-08, -6.842591204758719e-08], [0.035433948040008545, 0.1292276382446289, -0.030781667679548264, 0.12310491502285004, 3.3873977400844524e-08, 3.517606117497962e-08, 2.2883778427740253e-08, -3.078503851838832e-08, 0.015498634427785873, 0.16482095420360565, 2.700289769563824e-09, -3.1630058572318376e-08, -1.017178874462843e-08, 2.005381816161389e-08, -1.6741978470236063e-08, -3.4130831494394442e-09]]\n",
      "[[1.5123486518859863, 2.044293165206909, 0.24636557698249817, -1.0608521699905396, -1.4727836727956856e-08, -4.865615466087547e-08, -1.2858444620178489e-07, 3.249658675485989e-08, 0.07399863004684448, 0.7378922700881958, 1.382431946694851e-09, 6.658442686102717e-08, -7.2177499532699585e-09, -1.8556338687858442e-08, 6.004847818985581e-08, 2.550509670129486e-08], [3.332907199859619, 4.503681182861328, 0.4967535734176636, -2.1042141914367676, -4.230965373608342e-07, -3.0833496111881686e-07, 8.368549231363431e-08, 3.6694736138542794e-09, 0.18886202573776245, 1.7900187969207764, -1.8655555322766304e-08, 2.4049882085819263e-07, 4.493631422519684e-08, -8.922214078666002e-10, -2.8445356292650104e-08, 5.146169002046008e-08], [0.03376662731170654, 0.09216567873954773, -0.018372010439634323, 0.07669146358966827, 2.9228939268932663e-08, -1.306833929959339e-08, -4.629336558537034e-08, -6.414669684318142e-09, 0.0024664513766765594, 0.11971600353717804, 4.802132025361061e-10, -5.557170723591298e-08, 1.123407855629921e-08, 1.070414601400671e-08, 4.729372449219227e-11, -9.237263398631512e-09], [0.03240334987640381, 0.09539645910263062, -0.02006007730960846, 0.08288070559501648, 1.1596450022466342e-08, 1.687387296556153e-08, 3.4642567925402545e-08, -3.78847211379707e-08, 0.007899673655629158, 0.12504075467586517, 9.822542779147625e-10, -2.0188917204677637e-10, 1.461012288928032e-08, 7.664760381942415e-09, 2.0827428670600057e-09, -8.055483391444795e-09], [0.8873114585876465, 1.0608161687850952, 0.17477506399154663, -0.7165660262107849, -3.901226364178001e-08, 1.8607298812867157e-08, 2.5483674903625797e-07, 5.457665963604086e-08, 0.05422412231564522, 0.2683139443397522, -2.930755726993084e-08, 2.9067605211707814e-08, -5.384208634495735e-10, -1.0334025546399062e-07, 5.767105903942138e-08, -1.257318160696741e-08], [0.034695565700531006, 0.10977944731712341, -0.024307064712047577, 0.09885193407535553, 2.7953477754749656e-08, 2.7418035841719757e-08, 2.31591457122704e-09, 1.3481821881100586e-08, 0.0099908746778965, 0.14211377501487732, -1.1525116860866547e-08, 5.565392147133252e-09, 6.199115887284279e-09, -1.8119900246915677e-08, -9.957147995010018e-09, 7.450777772533002e-10], [-0.8290196657180786, -1.0141005516052246, -0.14034652709960938, 0.5671282410621643, 7.710691818374471e-08, 4.3583835207527954e-08, 7.777228461236518e-08, -3.2445655051560607e-09, -0.04094263166189194, -0.3363668918609619, 1.7484126146882772e-08, -6.208754399494865e-09, -2.246815711259842e-08, -5.364343635960722e-09, 1.911757863126695e-08, -1.668153259970495e-08], [0.10045400261878967, 0.2551442086696625, -0.03995509445667267, 0.1608608216047287, 5.935417490832151e-09, 3.96891319809356e-09, 2.426181211490075e-08, 1.657991255399338e-08, 0.033101312816143036, 0.27791571617126465, 1.4028046280145645e-08, -1.594285237160875e-08, 1.3969838619232178e-09, -2.5164625938600693e-08, 1.9136678020004183e-08, 1.375341263099017e-08], [1.34907066822052, 1.766562581062317, 0.17100511491298676, -0.6865843534469604, 2.980851832035114e-08, -5.146631920638356e-08, 2.090910733443252e-08, 6.510987304864102e-08, 0.10457414388656616, 0.7760880589485168, 7.916241884231567e-09, 1.3706452861583784e-08, 3.9726728573441505e-08, -4.8544661623850516e-09, -1.5563273336738348e-08, -6.842591204758719e-08], [0.035433948040008545, 0.1292276382446289, -0.030781667679548264, 0.12310491502285004, 3.3873977400844524e-08, 3.517606117497962e-08, 2.2883778427740253e-08, -3.078503851838832e-08, 0.015498634427785873, 0.16482095420360565, 2.700289769563824e-09, -3.1630058572318376e-08, -1.017178874462843e-08, 2.005381816161389e-08, -1.6741978470236063e-08, -3.4130831494394442e-09], [0.03864067792892456, 0.13209322094917297, -0.030472856014966965, 0.12149123847484589, 1.4704308881619e-08, 1.3290244460506528e-08, 1.4734136577487789e-08, -2.264832588139143e-08, 0.005911175161600113, 0.16245877742767334, -3.14321368932724e-09, 4.189566027434921e-08, -1.5454133972525597e-08, -3.542160342817624e-09, -2.3756001610308886e-08, -6.263686458396478e-09]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     33\u001b[0m reinforce_update \u001b[38;5;241m=\u001b[39m ReinforceUpdate(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose)\n\u001b[1;32m---> 34\u001b[0m reinforce_update\u001b[38;5;241m.\u001b[39mtrain()\n",
      "Cell \u001b[1;32mIn[32], line 129\u001b[0m, in \u001b[0;36mReinforceUpdate.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_episodes):\n\u001b[0;32m    128\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_trajectory()\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpqc\u001b[38;5;241m.\u001b[39mT_schedule(i,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_episodes)\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_policy()\n",
      "Cell \u001b[1;32mIn[32], line 28\u001b[0m, in \u001b[0;36mReinforceUpdate.get_trajectory\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     27\u001b[0m     state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(state)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m---> 28\u001b[0m action, log_prob, _, \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpqc\u001b[38;5;241m.\u001b[39msample(state_tensor)\n\u001b[0;32m     29\u001b[0m state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msaved_log_probs\u001b[38;5;241m.\u001b[39mappend(log_prob)\n",
      "Cell \u001b[1;32mIn[11], line 20\u001b[0m, in \u001b[0;36mQuantumPolicyModel.sample\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m     17\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m    Samples an action from the action probability distribution aka policy\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     policy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(inputs)\n\u001b[0;32m     21\u001b[0m     dist \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mCategorical(policy)\n\u001b[0;32m     22\u001b[0m     action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n",
      "Cell \u001b[1;32mIn[11], line 13\u001b[0m, in \u001b[0;36mQuantumPolicyModel.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03mInput state is fed to the circuit - its output is then fed to the post processing \u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     12\u001b[0m probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcircuit\u001b[38;5;241m.\u001b[39minput(inputs)\n\u001b[1;32m---> 13\u001b[0m probs_processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39minput(probs)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m probs_processed\n",
      "Cell \u001b[1;32mIn[4], line 11\u001b[0m, in \u001b[0;36mPolicyType.input\u001b[1;34m(self, probs)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minput\u001b[39m(\u001b[38;5;28mself\u001b[39m,probs):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_processing \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_contiguous\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 11\u001b[0m         policy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_contiguous(probs)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_processing \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_parity\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     13\u001b[0m         policy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_parity(probs)\n",
      "Cell \u001b[1;32mIn[4], line 36\u001b[0m, in \u001b[0;36mPolicyType.raw_contiguous\u001b[1;34m(self, probs)\u001b[0m\n\u001b[0;32m     33\u001b[0m         end \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m# Update the original policy list instead of creating a new one\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m     policy\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28msum\u001b[39m(probs_flatten[start:end]))\n\u001b[0;32m     38\u001b[0m policy_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(policy)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m policy_tensor\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:1032\u001b[0m, in \u001b[0;36mTensor.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state():\n\u001b[0;32m   1024\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1025\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterating over a tensor might cause the trace to be incorrect. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1026\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing a tensor of different shape won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt change the number of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1030\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   1031\u001b[0m     )\n\u001b[1;32m-> 1032\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munbind(\u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_qubits = 4\n",
    "n_layers = 1      #set to 1 if data_reuploading is off\n",
    "n_actions = 2\n",
    "shots = None\n",
    "input_scaling = False\n",
    "design = 'jerbi_circuit' \n",
    "diff_method = 'adjoint' \n",
    "weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "input_init = torch.nn.init.uniform_\n",
    "measure = measure_probs\n",
    "measure_qubits = None\n",
    "circuit = CircuitGenerator(n_qubits, n_layers, weight_init=weight_init)\n",
    "\n",
    "post_processing = 'softmax'\n",
    "policy_type = PolicyType(n_qubits, n_actions)\n",
    "\n",
    "\n",
    "pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "\n",
    "\n",
    "pqc = pqc\n",
    "lr_list= [0.01,0.08]\n",
    "params= circuit.parameters()\n",
    "optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "env_name = 'CartPole-v1'\n",
    "env = gym.make(env_name)\n",
    "n_episodes = 500\n",
    "max_t = 500\n",
    "gamma = 0.98\n",
    "print_every = 5\n",
    "verbose = 1\n",
    "\n",
    "reinforce_update = ReinforceUpdate(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose)\n",
    "reinforce_update.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel runs\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return f\"Agent {file_name}: Training completed\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 1      #set to 1 if data_reuploading is off\n",
    "    n_actions = 2\n",
    "    shots = None\n",
    "    input_scaling = False\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'adjoint' \n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.uniform_\n",
    "    measure = measure_probs\n",
    "    measure_qubits = None\n",
    "    circuit = CircuitGenerator(n_qubits, n_layers, weight_init=weight_init)\n",
    "\n",
    "\n",
    "    post_processing = 'softmax'\n",
    "    policy_type = PolicyType(n_qubits, n_actions)\n",
    "\n",
    "\n",
    "    pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "    \n",
    "\n",
    "    pqc = pqc\n",
    "    lr_list= [0.01,0.08]\n",
    "    params= circuit.parameters()\n",
    "    optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "    env_name = 'CartPole-v1'\n",
    "    env = gym.make(env_name)\n",
    "    n_episodes = 500\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 10\n",
    "    verbose = 1\n",
    "        \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, i) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the folder containing JSON files\n",
    "path = os.getcwd()\n",
    "folder_name = 'CartPole-v1_raw_contiguous_8'\n",
    "folder_path = os.path.join(path,folder_name)\n",
    "\n",
    "json_files = [file for file in os.listdir(folder_path) if file.endswith('.json')]\n",
    "\n",
    "# Initialize an empty list to store episode rewards from all agents\n",
    "episode_rewards = []\n",
    "\n",
    "# Iterate through each JSON file\n",
    "for json_file in json_files:\n",
    "    with open(os.path.join(folder_path, json_file), 'r') as f:\n",
    "        # Load JSON data\n",
    "        data = json.load(f)\n",
    "        # Check if data is a list\n",
    "        if isinstance(data, list):\n",
    "            # Extract episode rewards for each agent\n",
    "            agent_rewards = [agent[\"episode_reward\"] for agent in data]\n",
    "            # Add episode rewards to the list\n",
    "            episode_rewards.append(agent_rewards)\n",
    "        else:\n",
    "            print(f\"Invalid JSON data in file {json_file}\")\n",
    "\n",
    "# Calculate the average episode reward for each episode across agents\n",
    "average_rewards = np.mean(np.array(episode_rewards), axis=0)\n",
    "# Plot the average rewards\n",
    "plt.plot(range(1, len(average_rewards) + 1), average_rewards, linestyle='-')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Episode Reward')\n",
    "plt.title('Average Episode Rewards Across Agents')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axs[1].fill_between(np.arange(len(skolik_datareup_acrobot_mean_return)), np.clip(skolik_datareup_acrobot_mean_return - skolik_datareup_acrobot_std_return,a_min=-500,a_max=0), np.clip(skolik_datareup_acrobot_mean_return + skolik_datareup_acrobot_std_return,a_min=-500,a_max=0), alpha=0.2, color=\"red\")\n",
    "axs[1].fill_between(np.arange(len(skolik_datareup_acrobot_mean_return)), np.clip(skolik_datareup_no_in_acrobot_mean_return - skolik_datareup_no_in_acrobot_std_return,a_min=-500,a_max=0), np.clip(skolik_datareup_no_in_acrobot_mean_return + skolik_datareup_no_in_acrobot_std_return,a_min=-500,a_max=0), alpha=0.2, color=\"green\")\n",
    "axs[1].fill_between(np.arange(len(skolik_datareup_acrobot_mean_return)), np.clip(skolik_datareup_no_out_acrobot_mean_return - skolik_datareup_no_out_acrobot_std_return,a_min=-500,a_max=0), np.clip(skolik_datareup_no_out_acrobot_mean_return + skolik_datareup_no_out_acrobot_std_return,a_min=-500,a_max=0), alpha=0.2, color=\"blue\")\n",
    "axs[1].fill_between(np.arange(len(skolik_datareup_acrobot_mean_return)), np.clip(skolik_datareup_no_in_out_acrobot_mean_return - skolik_datareup_no_in_out_acrobot_std_return,a_min=-500,a_max=0), np.clip(skolik_datareup_no_in_out_acrobot_mean_return + skolik_datareup_no_in_out_acrobot_std_return,a_min=-500,a_max=0), alpha=0.2, color=\"purple\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.title('Score per episode')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(average_scores)+1), average_scores)\n",
    "plt.ylabel('Score')\n",
    "plt.title('Average score of the previous 10 episodes')\n",
    "plt.xlabel('Average of previous 10 #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param1_gradients = gradient_list[::2]\n",
    "param2_gradients = gradient_list[1::2]\n",
    "\n",
    "# Plotting gradients for parameters 1\n",
    "plt.plot(param1_gradients, label='Parameters')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Gradient Mean')\n",
    "plt.title('Gradients of Parameters')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting gradients for parameters 2\n",
    "plt.plot(param2_gradients, label='Parameters')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Gradient Mean')\n",
    "plt.title('Gradients of Input Parameters')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_runtimes = np.cumsum([runtime / 60 for runtime in runtimes])\n",
    "\n",
    "plt.plot(cumulative_runtimes)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Cumulative Runtime (Minutes)')\n",
    "plt.title('Cumulative Runtimes')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "import os\n",
    "import inspect\n",
    "import gym\n",
    "import glob\n",
    "from gym.wrappers.monitoring import video_recorder\n",
    "from IPython.display import HTML, display\n",
    "import torch\n",
    "\n",
    "def show_video_of_model(policy, env_name):\n",
    "    current_dir = os.path.abspath('')\n",
    "    video_dir = os.path.join(current_dir, 'video')\n",
    "    os.makedirs(video_dir, exist_ok=True)  # Ensure video directory exists\n",
    "\n",
    "    # Create the Gym environment with render mode\n",
    "    env = gym.make(env_name, render_mode='rgb_array')\n",
    "    video_path = os.path.join(video_dir, '{}.mp4'.format(env_name))\n",
    "    vid = video_recorder.VideoRecorder(env, path=video_path)\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    for t in range(1000):\n",
    "        # Convert state to tensor if needed\n",
    "        state_tensor = torch.tensor(state[0]).float() if t == 0 else torch.tensor(state).float()\n",
    "        \n",
    "        # Capture frame\n",
    "        vid.capture_frame()\n",
    "        \n",
    "        # Sample action from policy\n",
    "        action, log_prob, _, = policy.sample(state_tensor)\n",
    "        \n",
    "        # Take action in the environment\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Close video recorder and environment\n",
    "    vid.close()\n",
    "    env.close()\n",
    "    \n",
    "    print(\"Video saved at:\", video_path)\n",
    "\n",
    "def show_video(env_name):\n",
    "    current_dir = os.path.abspath('')\n",
    "    video_dir = os.path.join(current_dir, 'video')\n",
    "    mp4list = glob.glob(os.path.join(video_dir, '*.mp4'))\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = os.path.join(video_dir, '{}.mp4'.format(env_name))\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else:\n",
    "        print(\"Could not find video\")\n",
    "\n",
    "# Example usage\n",
    "# show_video_of_model(pqc, 'CartPole-v1')\n",
    "# show_video('CartPole-v1')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
