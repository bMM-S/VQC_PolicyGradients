{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pennylane as qml \n",
    "import torch\n",
    "\n",
    "def create_circuit(n_qubits,n_layers=None,circ = \"simplified_two_design\",fim=False, shots=None):\n",
    "\n",
    "    dev = qml.device(\"default.qubit.torch\", wires=n_qubits, shots=shots)\n",
    "\n",
    "    def RZRY(params):\n",
    "        #qml.SpecialUnitary(params, wires=range(n_qubits))\n",
    "        #qml.SimplifiedTwoDesign(initial_layer_weights=init_params, weights=params, wires=range(n_qubits))\n",
    "        #qml.AngleEmbedding(params,wires=range(n_qubits))\n",
    "        for q in range(n_qubits):\n",
    "            qml.Hadamard(wires=q)\n",
    "\n",
    "        for w in range(n_layers): \n",
    "            for q in range(n_qubits):\n",
    "                index = w * (2*n_qubits) + q * 2\n",
    "                qml.RZ(params[index],wires=q)\n",
    "                qml.RY(params[index + 1],wires=q)\n",
    "        \n",
    "        qml.broadcast(qml.CNOT , wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "        \n",
    "        return qml.probs(wires=range(n_qubits))\n",
    "\n",
    "    def S2D(init_params,params,measurement_qubits=0,prod_approx=False):\n",
    "        #qml.SpecialUnitary(params, wires=range(n_qubits))\n",
    "        qml.SimplifiedTwoDesign(initial_layer_weights=init_params, weights=params, wires=range(n_qubits))\n",
    "        \n",
    "        #qml.broadcast(qml.CNOT , wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "        if not prod_approx:\n",
    "            return qml.probs(wires=list(range(measurement_qubits)))\n",
    "        else:\n",
    "            return [qml.probs(i) for i in range(measurement_qubits)]\n",
    "\n",
    "    def SU(params):\n",
    "        qml.SpecialUnitary(params, wires=range(n_qubits))\n",
    "        \n",
    "        ZZ = qml.operation.Tensor(qml.PauliZ(0), qml.PauliZ(1))\n",
    "        for i in range(2,n_qubits):\n",
    "            ZZ = qml.operation.Tensor(ZZ, qml.PauliZ(i))\n",
    "\n",
    "        return qml.expval(ZZ)\n",
    "    \n",
    "    def simmpleRZRY(params,cnots=True):\n",
    "        qml.broadcast(qml.Hadamard, wires=range(n_qubits), pattern=\"single\")\n",
    "        qml.broadcast(qml.RZ, wires=range(n_qubits), pattern=\"single\", parameters=params[0])\n",
    "        qml.broadcast(qml.RY, wires=range(n_qubits), pattern=\"single\", parameters=params[1])\n",
    "        if cnots:\n",
    "            qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"chain\")\n",
    "\n",
    "            return qml.expval(qml.PauliZ(n_qubits-1))\n",
    "        else:\n",
    "            ZZ = qml.operation.Tensor(qml.PauliZ(0), qml.PauliZ(1))\n",
    "            for i in range(2,n_qubits):\n",
    "                ZZ = qml.operation.Tensor(ZZ, qml.PauliZ(i))\n",
    "\n",
    "            return qml.expval(ZZ)\n",
    "        \n",
    "    def RY(params,y=True,probs=False,prod=False, entanglement=None):\n",
    "        #qml.broadcast(qml.Hadamard, wires=range(n_qubits), pattern=\"single\")\n",
    "        qml.broadcast(qml.RY, wires=range(n_qubits), pattern=\"single\", parameters=params)\n",
    "        #qml.broadcast(qml.CZ, wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "\n",
    "        if entanglement==\"all_to_all\":\n",
    "            qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "        \n",
    "        if y==True:\n",
    "            #YY = qml.operation.Tensor(qml.PauliY(0), qml.PauliY(1))\n",
    "            YY = [qml.PauliZ(0), qml.PauliZ(1)]\n",
    "            for i in range(2,n_qubits):\n",
    "                #YY = qml.operation.Tensor(YY, qml.PauliY(i))\n",
    "                YY.append(qml.PauliZ(i))\n",
    "            \n",
    "            #return [qml.expval(i) for i in YY]\n",
    "            return qml.expval(YY)\n",
    "\n",
    "        elif probs==False:\n",
    "\n",
    "            ZZ = qml.operation.Tensor(qml.PauliZ(0), qml.PauliZ(1))\n",
    "            #ZZ = [qml.PauliZ(0), qml.PauliZ(1)]\n",
    "            for i in range(2,n_qubits):\n",
    "                ZZ = qml.operation.Tensor(ZZ, qml.PauliZ(i))        \n",
    "                #ZZ.append(qml.PauliZ(i))        \n",
    "\n",
    "            #return [qml.expval(i) for i in ZZ]\n",
    "            return qml.expval(ZZ)\n",
    "\n",
    "        else:\n",
    "            if prod:\n",
    "                return [qml.probs(i) for i in range(n_qubits)]\n",
    "            else:\n",
    "                return qml.probs(wires=range(n_qubits))\n",
    "            \n",
    "        \n",
    "        \n",
    "    def GHZ(params,measurement_qubits=0):\n",
    "        qml.RY(params,wires=0)\n",
    "        qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"chain\")\n",
    "\n",
    "        return qml.probs(wires=range(measurement_qubits))\n",
    "\n",
    "    def random_product_state(params,gate_sequence=None):\n",
    "                \n",
    "        for i in range(n_qubits):\n",
    "            qml.RY(np.pi / 4, wires=i)\n",
    "\n",
    "        for ll in range(len(params)):\n",
    "\n",
    "            for i in range(n_qubits):\n",
    "                gate_sequence[\"{}{}\".format(ll,i)](params[ll][i], wires=i)\n",
    "\n",
    "            #for i in range(n_qubits - 1):\n",
    "                #qml.CZ(wires=[i, i + 1])\n",
    "    def SEL(params, measurement_qubits=0):\n",
    "        qml.StronglyEntanglingLayers(params, wires=range(n_qubits))\n",
    "        return qml.probs(wires=range(measurement_qubits))\n",
    "    \n",
    "    def RL(params, measurement_qubits=0):\n",
    "        qml.RandomLayers(params, ratio_imprim=0.8 ,imprimitive=qml.CZ, wires=range(n_qubits))\n",
    "        return qml.probs(wires=range(measurement_qubits))\n",
    "    \n",
    "    if circ == \"rzry\":\n",
    "        qcircuit = RZRY\n",
    "    elif circ == \"simplified_two_design\":\n",
    "        qcircuit = S2D\n",
    "    elif circ == \"special_unitary\":\n",
    "        qcircuit = SU\n",
    "    elif circ == \"simpleRZRY\":\n",
    "        qcircuit = simmpleRZRY\n",
    "    elif circ == \"RY\":\n",
    "        qcircuit = RY\n",
    "    elif circ == \"ghz\":\n",
    "        qcircuit = GHZ\n",
    "    elif circ == \"random_product_state\":\n",
    "        qcircuit = random_product_state\n",
    "    elif circ == \"SEL\":\n",
    "        qcircuit = SEL\n",
    "    elif circ == \"RL\":\n",
    "        qcircuit = RL\n",
    "    if not fim:\n",
    "        circuit = qml.QNode(qcircuit, dev,interface=\"torch\", diff_method=\"backprop\")\n",
    "    else:\n",
    "        circuit = qml.QNode(qcircuit, dev)\n",
    "\n",
    "    return circuit\n",
    "\n",
    "def compute_gradient(log_prob, w):\n",
    "    \"\"\"Compute gradient of the log probability with respect to weights.\n",
    "    \n",
    "    Args:\n",
    "    - log_prob (torch.Tensor): The log probability tensor.\n",
    "    - w (torch.Tensor): The weights tensor, with requires_grad=True.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The gradient of log_prob with respect to w, flattened.\n",
    "    \"\"\"\n",
    "    if w.grad is not None:\n",
    "        w.grad.zero_()\n",
    "    log_prob.backward(retain_graph=True)\n",
    "    \n",
    "    if w.grad is None:\n",
    "        raise ValueError(\"The gradient for the given log_prob with respect to w is None.\")\n",
    "    \n",
    "    return w.grad.view(-1).detach().numpy()\n",
    "\n",
    "def policy(probs, policy_type=\"contiguous-like\", n_actions=2, n_qubits=1):\n",
    "\n",
    "    if policy_type == \"contiguous-like\":\n",
    "        return probs\n",
    "    elif policy_type == \"parity-like\":\n",
    "        policy = torch.zeros(n_actions)\n",
    "        for i in range(len(probs)):\n",
    "            a=[]\n",
    "            for m in range(int(np.log2(n_actions))):\n",
    "                if m==0:    \n",
    "                    bitstring = np.binary_repr(i,width=n_qubits)\n",
    "                else:\n",
    "                    bitstring = np.binary_repr(i,width=n_qubits)[:-m]\n",
    "                \n",
    "                a.append(bitstring.count(\"1\") % 2)\n",
    "            policy[int(\"\".join(str(x) for x in a),2)] += probs[i]\n",
    "\n",
    "        return policy    \n",
    "    \n",
    "def compute_policy_and_gradient(args):\n",
    "    n_qubits, shapes, type , n_actions, policy_type, clamp = args\n",
    "\n",
    "    if policy_type == \"parity-like\":\n",
    "        measure_qubits = n_qubits\n",
    "    else:\n",
    "        measure_qubits = int(np.log2(n_actions))\n",
    "\n",
    "    qc = create_circuit(n_qubits, circ=type, fim=False, shots=None)\n",
    "\n",
    "    if type == \"simplified_two_design\":\n",
    "        weights = [np.random.uniform(-np.pi,np.pi,size=shape) for shape in shapes]    \n",
    "        weights_tensor_init = torch.tensor(weights[0], requires_grad=False)\n",
    "        weights_tensor_params = torch.tensor(weights[1], requires_grad=True)\n",
    "        \n",
    "        probs = qc(weights_tensor_init,weights_tensor_params, measurement_qubits=measure_qubits)\n",
    "\n",
    "    else:\n",
    "        weights = [np.random.uniform(-np.pi,np.pi,size=shape) for shape in shapes]    \n",
    "        weights_tensor_params = torch.tensor(weights, requires_grad=True)\n",
    "\n",
    "        probs = qc(weights_tensor_params, measurement_qubits=measure_qubits)\n",
    "\n",
    "    pi = policy(probs, policy_type=policy_type, n_actions=n_actions, n_qubits=n_qubits)\n",
    "    if clamp is not None:\n",
    "        pi = torch.clamp(pi, clamp, 1)\n",
    "\n",
    "    dist = torch.distributions.Categorical(probs=pi)\n",
    "    \n",
    "    action = dist.sample()\n",
    "    log_prob = dist.log_prob(action)\n",
    "\n",
    "    gradient_no_clamp = np.linalg.norm(compute_gradient(log_prob, weights_tensor_params), 2)\n",
    "    return gradient_no_clamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceUpdate():\n",
    "\n",
    "    def __init__(self, pqc, optimizer, env, n_episodes, max_t, gamma=1.0, print_every=1, verbose=1):\n",
    "        \n",
    "        self.pqc = pqc\n",
    "        self.optimizer = optimizer\n",
    "        self.env = env\n",
    "        self.n_episodes = n_episodes\n",
    "        self.max_t = max_t\n",
    "        self.gamma = gamma\n",
    "        self.scores_deque = deque(maxlen=10)\n",
    "        self.print_every = print_every\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.scores_list = []\n",
    "        self.runtime_list = []\n",
    "        self.gradient_list = []\n",
    "        self.loss_list = []\n",
    "        self.parameters_list = []\n",
    "        self.steps_per_ep_list = []\n",
    "        \n",
    "    def get_trajectory(self):\n",
    "        \n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        state = self.env.reset()\n",
    "        for t in range(self.max_t):\n",
    "            # Sample the action from current policy\n",
    "            if t==0:\n",
    "                state_tensor = torch.tensor(state[0]).float()\n",
    "            else:\n",
    "                state_tensor = torch.tensor(state).float()\n",
    "            action, log_prob, _, = self.pqc.sample(state_tensor)\n",
    "            self.saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _, _ = self.env.step(action)\n",
    "            self.rewards.append(reward)\n",
    "            if done:\n",
    "                self.steps_per_ep_list.append(t)\n",
    "                break\n",
    "\n",
    "        self.scores_deque.append(sum(self.rewards))\n",
    "        self.scores_list.append(sum(self.rewards))\n",
    "\n",
    "    def compute_loss(self):\n",
    "\n",
    "        R=0\n",
    "        policy_loss = []\n",
    "        returns = []\n",
    "        for r in self.rewards[::-1]:\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0,R)\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1)\n",
    "\n",
    "        for log_prob, R in zip(self.saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "\n",
    "        policy_unsqueezed = [torch.unsqueeze(loss, 0) for loss in policy_loss]\n",
    "        self.loss = torch.cat(policy_unsqueezed).sum()\n",
    "        self.loss_list.append(self.loss)\n",
    "\n",
    "    def compute_gradients(self):\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        self.loss.backward()    \n",
    "        self.optimizer.step()\n",
    "        self.gradient_list.append(self.pqc.get_gradients())\n",
    "\n",
    "    def train(self):\n",
    "        \n",
    "        for i in range(1,self.n_episodes):\n",
    "            start_time = time.time()\n",
    "            self.get_trajectory()\n",
    "            self.compute_loss()\n",
    "            self.compute_gradients()\n",
    "            end_time = time.time()\n",
    "            runtime = end_time-start_time\n",
    "            self.runtime_list.append(runtime)\n",
    "            self.parameters_list.append(self.pqc.get_parameters())\n",
    "            if np.mean(self.scores_deque) == 500:\n",
    "                print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i,np.mean(self.scores_deque)))\n",
    "                break\n",
    "            elif i % self.print_every == 0 and self.verbose == 1:\n",
    "                print('Episode {}\\tLast reward: {:.2f}\\tLast 10 Episodes average reward: {:.2f}\\tRuntime: {:.2f}\\t'.format(i, self.scores_deque[-1], np.mean(self.scores_deque), runtime))\n",
    "\n",
    "    def save_data(self,file_path,file_name):\n",
    "        \n",
    "        file_path = os.path.join(file_path, file_name)\n",
    "        \n",
    "        data = {\n",
    "            \"scores_list\": self.scores_list,\n",
    "            \"runtime_list\": self.runtime_list,\n",
    "            \"gradient_list\": self.gradient_list,\n",
    "            \"loss_list\": self.loss_list,\n",
    "            \"parameters_list\": self.parameters_list,\n",
    "            \"steps_per_ep_list\": self.steps_per_ep_list\n",
    "        }\n",
    "        \n",
    "        # Save data to pickle file\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "from collections import deque\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "def create_zz_operator(measure_qubits):\n",
    "    ZZ = qml.PauliZ(0)\n",
    "    for i in range(1, measure_qubits):\n",
    "        ZZ = qml.operation.Tensor(ZZ, qml.PauliZ(i))\n",
    "    return ZZ\n",
    "\n",
    "def measure_selection(measure_type, observables, measure_qubits):\n",
    "    if measure_type == 'probs':\n",
    "        if observables is None:\n",
    "            return qml.probs(wires=range(measure_qubits))\n",
    "        else:\n",
    "            return qml.probs(op=observables, wires=range(measure_qubits))\n",
    "        \n",
    "    elif measure_type == 'expval':\n",
    "        op = observables if observables is not None else create_zz_operator(measure_qubits)\n",
    "        return qml.expval(op=op) \n",
    "    \n",
    "def create_optimizer_with_lr(params, lr_list):\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': params, 'lr': lr} for params, lr in zip(params, lr_list)\n",
    "    ])\n",
    "    return optimizer\n",
    "\n",
    "def jerbi_circuit(n_qubits, n_layers, shots, input_scaling, data_reuploading, diff_method, weight_init, input_init, measure_type, observables, measure_qubits):\n",
    "\n",
    "    if shots is None:\n",
    "        dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "    else:\n",
    "        dev = qml.device(\"default.qubit\", wires=n_qubits, shots=shots)\n",
    "        \n",
    "    observables = observables if observables is not None else None\n",
    "    if (data_reuploading and n_layers==0) or (not data_reuploading and n_layers>0):\n",
    "        raise ValueError(\"Number of layers should be 0 if data_reuploading is False, and greater than 0 if data_reuploading is True.\")\n",
    "    \n",
    "    if data_reuploading:\n",
    "        weight_shapes = {\"params\": (n_layers + 1, n_qubits, 2),\n",
    "                        \"input_params\": (n_layers, n_qubits, 2)}\n",
    "        init_method   = {\"params\": weight_init,\n",
    "                        \"input_params\": input_init}\n",
    "    else:\n",
    "        weight_shapes = {\"params\": (1, n_qubits, 2),\n",
    "                        \"input_params\": (1, n_qubits, 2)}\n",
    "        init_method   = {\"params\": weight_init,\n",
    "                        \"input_params\": input_init}\n",
    "\n",
    "    @qml.qnode(dev, interface='torch', diff_method=diff_method)\n",
    "    def qnode(inputs, params, input_params):\n",
    "        '''\n",
    "    #in case n_qubits != input length\n",
    "        if n_qubits > len(inputs) and n_qubits % len(inputs) == 0:\n",
    "            multiplier = n_qubits // len(inputs)\n",
    "            inputs = torch.cat([inputs] * multiplier)\n",
    "        elif n_qubits != len(inputs) and n_qubits % len(inputs) != 0:\n",
    "            raise ValueError('Number of qubits cannot be divided by input lenght')\n",
    "        '''    \n",
    "    #hadamard\n",
    "        qml.broadcast(qml.Hadamard, wires=range(n_qubits), pattern=\"single\")\n",
    "        \n",
    "    #in case data_reuploading is True\n",
    "        if data_reuploading:\n",
    "        #iterate for each layer\n",
    "            for layer in range(n_layers):\n",
    "                for wire in range(n_qubits):\n",
    "                    qml.RZ(params[layer][wire][0], wires=wire)\n",
    "                    qml.RY(params[layer][wire][1], wires=wire)\n",
    "\n",
    "                qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"chain\")\n",
    "\n",
    "            #in case input_scaling is True\n",
    "                if input_scaling:\n",
    "                    for wire in range(n_qubits):\n",
    "                        qml.RY(input_params[layer][wire][0] * inputs[wire], wires=wire)\n",
    "                        qml.RZ(input_params[layer][wire][1] * inputs[wire], wires=wire)\n",
    "            #in case input_scaling is False\n",
    "                else:\n",
    "                    for wire in range(n_qubits):\n",
    "                        qml.RY(inputs[wire], wires=wire)\n",
    "                        qml.RZ(inputs[wire], wires=wire)\n",
    "\n",
    "        #last parameterized block\n",
    "            for wire in range(n_qubits):\n",
    "                qml.RZ(params[-1][wire][0], wires=wire)\n",
    "                qml.RY(params[-1][wire][1], wires=wire)\n",
    "\n",
    "    #in case data_reuploading is False  \n",
    "        else:\n",
    "            for wire in range(n_qubits):\n",
    "                qml.RZ(params[-1][wire][0], wires=wire)\n",
    "                qml.RY(params[-1][wire][1], wires=wire)\n",
    "\n",
    "        #in case input_scaling is True\n",
    "            if input_scaling:\n",
    "                for wire in range(n_qubits):\n",
    "                    qml.RZ(input_params[-1][wire][0]* inputs[wire], wires=wire)\n",
    "                    qml.RY(input_params[-1][wire][1]* inputs[wire], wires=wire)\n",
    "\n",
    "        #in case input_scaling is False            \n",
    "            else:\n",
    "                for wire in range(n_qubits):\n",
    "                    qml.RZ(input_params[-1][wire][0], wires=wire)\n",
    "                    qml.RY(input_params[-1][wire][1], wires=wire)\n",
    "        \n",
    "        return measure_selection(measure_type, observables, measure_qubits)\n",
    "\n",
    "    model = qml.qnn.TorchLayer(qnode, weight_shapes=weight_shapes, init_method=init_method)\n",
    "\n",
    "    return model\n",
    "\n",
    "def S2D(n_qubits, n_layers, shots, input_scaling, diff_method, weight_init, input_init, measure_type, observables, measure_qubits):\n",
    "\n",
    "    dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "    observables = observables if observables is not None else None\n",
    "    \n",
    "    shapes = qml.SimplifiedTwoDesign.shape(n_layers=n_layers, n_wires=n_qubits)\n",
    "\n",
    "    weight_shapes = {\"params\": shapes[1],\n",
    "                     \"input_params\": shapes[0]}\n",
    "    \n",
    "    init_method   = {\"params\": weight_init,\n",
    "                     \"input_params\": input_init}\n",
    "\n",
    "    @qml.qnode(dev, interface='torch', diff_method='parameter-shift')\n",
    "    def qnode(inputs, params, input_params):\n",
    "\n",
    "        return measure_selection(measure_type, observables, measure_qubits)\n",
    "\n",
    "    model = qml.qnn.TorchLayer(qnode, weight_shapes=weight_shapes, init_method=init_method)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircuitGenerator(nn.Module):\n",
    "\n",
    "    def __init__(self, n_qubits, n_layers,  shots= None, input_scaling=True, data_reuploading=True, design='jerbi_circuit', diff_method = 'backprop', weight_init=torch.nn.init.uniform_, input_init = torch.nn.init.ones_, measure_type = 'probs', observables = None, measure_qubits = None):\n",
    "        super(CircuitGenerator, self).__init__()\n",
    "        self.n_qubits = n_qubits                        #number of qubits\n",
    "        self.n_layers = n_layers                        #number of layers\n",
    "        self.shots = shots                              #number of shots\n",
    "        self.input_scaling = input_scaling              #input scaling - True or False\n",
    "        self.data_reuploading = data_reuploading        #data reuploading - True or False\n",
    "        self.design = design                            #circuit design\n",
    "        self.diff_method = diff_method                  #differentiator method \n",
    "        self.measure_type = measure_type                #measure type - 'probs' or 'expval'\n",
    "        self.observables = observables                  #observables\n",
    "        self.weight_init = weight_init                  #weight initialization - torch.nn.init\n",
    "        self.input_init = input_init                    #input weight initialization - torch.nn.init.\n",
    "        if measure_qubits is None:\n",
    "            self.measure_qubits = n_qubits\n",
    "        else:\n",
    "            self.measure_qubits = measure_qubits\n",
    "\n",
    "        if self.design == 'jerbi_circuit':\n",
    "            self.circuit = jerbi_circuit(n_qubits = self.n_qubits,\n",
    "                                        n_layers = self.n_layers,\n",
    "                                        shots = self.shots,\n",
    "                                        input_scaling = self.input_scaling,\n",
    "                                        data_reuploading = self.data_reuploading,\n",
    "                                        diff_method = self.diff_method,\n",
    "                                        weight_init = self.weight_init,\n",
    "                                        input_init = self.input_init,\n",
    "                                        measure_type = self.measure_type,\n",
    "                                        observables = self.observables,\n",
    "                                        measure_qubits = self.measure_qubits)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported circuit type\")\n",
    "\n",
    "    def input(self,inputs):\n",
    "\n",
    "        outputs = self.circuit(inputs)\n",
    "        return outputs\n",
    "    \n",
    "    def draw(self):\n",
    "\n",
    "        print(self.circuit.draw())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyType(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_qubits, n_actions, post_processing = 'raw_contiguous'):\n",
    "        self.n_actions = n_actions\n",
    "        self.n_qubits = n_qubits\n",
    "        self.post_processing = post_processing\n",
    "        self.beta=0.25\n",
    "\n",
    "    def input(self,probs):\n",
    "\n",
    "        if self.post_processing == 'raw_contiguous':\n",
    "            policy = self.raw_contiguous(probs)\n",
    "        elif self.post_processing == 'raw_parity':\n",
    "            policy = self.raw_parity(probs)\n",
    "        elif self.post_processing == 'softmax':\n",
    "            policy = self.softmax(probs)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid post-processing method specified.\")\n",
    "\n",
    "        return policy\n",
    "\n",
    "    def raw_contiguous(self,probs):\n",
    "\n",
    "        probs_flatten = probs.flatten()\n",
    "        chunk_size = len(probs_flatten) // self.n_actions\n",
    "        remainder = len(probs_flatten) % self.n_actions\n",
    "\n",
    "        policy = []\n",
    "\n",
    "        for i in range(self.n_actions):\n",
    "            start = i * chunk_size\n",
    "            end = (i + 1) * chunk_size\n",
    "\n",
    "            if i < remainder:\n",
    "                end += 1\n",
    "\n",
    "            # Update the original policy list instead of creating a new one\n",
    "            policy.append(sum(probs_flatten[start:end]))\n",
    "\n",
    "        policy_tensor = torch.stack(policy)\n",
    "        return policy_tensor\n",
    "    \n",
    "    def raw_parity(self,probs):\n",
    "\n",
    "        if self.n_actions % 2 != 0:\n",
    "            return('For parity-like policy, n_actions must be an even number')\n",
    "        \n",
    "        probs_flatten = probs.flatten()\n",
    "        policy = torch.zeros(self.n_actions)\n",
    "        counter = 0\n",
    "        for prob in probs_flatten:\n",
    "            policy[counter] += prob\n",
    "            counter += 1\n",
    "            if counter == self.n_actions:\n",
    "                counter = 0\n",
    "        \n",
    "        policy_tensor = torch.stack(policy)\n",
    "\n",
    "        return policy_tensor\n",
    "    \n",
    "    def softmax(self, probs):\n",
    "        if len(probs) == self.n_actions:\n",
    "            scaled_output = probs * self.beta\n",
    "            softmax_output = F.softmax(scaled_output, dim=0)\n",
    "            return softmax_output\n",
    "        else:\n",
    "            probs_flatten = probs.flatten()\n",
    "            chunk_size = len(probs_flatten) // self.n_actions\n",
    "            remainder = len(probs_flatten) % self.n_actions\n",
    "\n",
    "            policy = []\n",
    "\n",
    "            for i in range(self.n_actions):\n",
    "                start = i * chunk_size\n",
    "                end = (i + 1) * chunk_size\n",
    "\n",
    "                if i < remainder:\n",
    "                    end += 1\n",
    "\n",
    "                # Update the original policy list instead of creating a new one\n",
    "                policy.append(sum(probs_flatten[start:end]))\n",
    "            policy_tensor = torch.stack(policy)\n",
    "            scaled_output = policy_tensor * self.beta\n",
    "            softmax_output = F.softmax(scaled_output, dim=0)\n",
    "            return softmax_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumPolicyModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, circuit, policy):\n",
    "        super(QuantumPolicyModel, self).__init__()\n",
    "        self.circuit = circuit\n",
    "        self.policy = policy\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        Input state is fed to the circuit - its output is then fed to the post processing \n",
    "        '''\n",
    "        probs = self.circuit.input(inputs)\n",
    "        probs_processed = self.policy.input(probs)\n",
    "        return probs_processed\n",
    "    \n",
    "    def sample(self, inputs):\n",
    "        '''\n",
    "        Samples an action from the action probability distribution aka policy\n",
    "        '''\n",
    "        policy = self.forward(inputs)\n",
    "        dist = torch.distributions.Categorical(policy)\n",
    "        action = dist.sample()\n",
    "        if self.policy.post_processing == 'softmax':\n",
    "            policy.beta +=0.002\n",
    "        return action.item(), dist.log_prob(action), policy\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        '''\n",
    "        Returns the values of each set of parameters\n",
    "        '''\n",
    "        parameter_values = [param.clone().detach().numpy().flatten() for param in self.circuit.parameters()]\n",
    "        return parameter_values\n",
    "    \n",
    "    def get_gradients(self):\n",
    "        '''\n",
    "        Returns the gradient values of each set of parameters\n",
    "        '''\n",
    "        gradients = [torch.flatten(param.grad.clone().detach()) if param.grad is not None else torch.flatten(torch.zeros_like(param)) for name, param in self.circuit.named_parameters()]\n",
    "        return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceUpdate():\n",
    "\n",
    "    def __init__(self, pqc, optimizer, env, n_episodes, max_t, gamma, print_every, verbose, file_path = None, file_name = None):\n",
    "        \n",
    "        self.pqc = pqc\n",
    "        self.optimizer = optimizer\n",
    "        self.env = env\n",
    "        self.n_episodes = n_episodes\n",
    "        self.max_t = max_t\n",
    "        self.gamma = gamma\n",
    "        self.scores_deque = deque(maxlen=print_every)\n",
    "        self.runtime_deque = deque(maxlen=print_every)\n",
    "        self.print_every = print_every\n",
    "        self.verbose = verbose\n",
    "        self.folder_path = file_path\n",
    "        self.file_name = file_name\n",
    "        self.running_reward = 10\n",
    "\n",
    "    def get_trajectory(self):\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        state = self.env.reset()\n",
    "        for t in range(self.max_t):\n",
    "            # Sample the action from current policy\n",
    "            if t == 0:\n",
    "                state_tensor = torch.tensor(state[0]).float()\n",
    "            else:\n",
    "                state_tensor = torch.tensor(state).float()\n",
    "            action, log_prob, _, = self.pqc.sample(state_tensor)\n",
    "            self.saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _, _ = self.env.step(action)\n",
    "            self.rewards.append(reward)\n",
    "            if done:\n",
    "                self.scores_deque.append(sum(self.rewards))\n",
    "                break\n",
    "\n",
    "    def update_policy(self):\n",
    "\n",
    "        R = 0\n",
    "        policy_loss = []\n",
    "        returns = []\n",
    "        for r in self.rewards[::-1]:\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + np.finfo(np.float32).eps)\n",
    "\n",
    "        for log_prob, R in zip(self.saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "\n",
    "        policy_unsqueezed = [torch.unsqueeze(loss, 0) for loss in policy_loss]\n",
    "        self.loss = torch.cat(policy_unsqueezed).sum()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        self.loss.backward()    \n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def save_episode_data(self):\n",
    "\n",
    "        episode_data = {\n",
    "                \"episode_reward\": self.scores_deque[-1],\n",
    "                \"episode_length\": len(self.rewards),\n",
    "                \"runtime\": self.runtime,\n",
    "                \"loss\": self.loss.item(),\n",
    "                \"gradient\": self.tensor_to_list(self.pqc.get_gradients()),\n",
    "                \"parameters\": self.tensor_to_list(self.pqc.get_parameters())\n",
    "        }\n",
    "\n",
    "        if self.folder_path is not None:\n",
    "            file_path = os.path.join(self.folder_path, f\"{self.file_name}.json\")\n",
    "            if os.path.exists(file_path):\n",
    "                with open(file_path, 'r') as f:\n",
    "                    existing_data = json.load(f)\n",
    "                existing_data.append(episode_data)\n",
    "                with open(file_path, 'w') as f:\n",
    "                    json.dump(existing_data, f, indent=4)\n",
    "            else:\n",
    "                with open(file_path, 'w') as f:\n",
    "                    json.dump([episode_data], f, indent=4)\n",
    "\n",
    "    def tensor_to_list(self, tensor):\n",
    "        \"\"\"\n",
    "        Convert a tensor or numpy array to a nested list.\n",
    "        \"\"\"\n",
    "        if isinstance(tensor, list):\n",
    "            return [self.tensor_to_list(t) for t in tensor]\n",
    "        elif isinstance(tensor, dict):\n",
    "            return {key: self.tensor_to_list(value) for key, value in tensor.items()}\n",
    "        elif isinstance(tensor, np.ndarray):\n",
    "            return tensor.tolist()  # Convert numpy array to list\n",
    "        elif isinstance(tensor, torch.Tensor):\n",
    "            return tensor.tolist()  # Convert torch tensor to list\n",
    "        else:\n",
    "            return tensor\n",
    "                    \n",
    "    def train(self):\n",
    "\n",
    "        runtime_sum=0\n",
    "        for i in range(1, self.n_episodes):\n",
    "            start_time = time.time()\n",
    "            self.get_trajectory()\n",
    "            self.update_policy()\n",
    "            end_time = time.time()\n",
    "            self.runtime = end_time - start_time\n",
    "            self.save_episode_data()\n",
    "            self.running_reward = (self.running_reward * 0.99) + (len(self.rewards) * 0.01)\n",
    "            runtime_sum += self.runtime\n",
    "            if self.running_reward > self.env.spec.reward_threshold:\n",
    "                print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i, np.mean(self.scores_deque)))\n",
    "                break\n",
    "            elif i % self.print_every == 0 and self.verbose == 1:\n",
    "                print('Episode {}\\tLast reward: {:.2f}\\tLast {}\\t Episodes average reward: {:.2f}\\tRuntime: {:.2f}\\t {:.2f}\\t'.format(i, self.scores_deque[-1], self.print_every, np.mean(self.scores_deque), runtime_sum, self.running_reward))\n",
    "                runtime_sum=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Analysis():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(policy, optimizer, env, n_episodes=1000, max_t=1000, gamma=1.0, print_every=5):\n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "    scores = []\n",
    "    average_scores = []\n",
    "    runtime_sum = 0\n",
    "    for e in range(1, n_episodes):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        # Collect trajectory\n",
    "        for t in range(max_t):\n",
    "            # Sample the action from current policy\n",
    "            if t==0:\n",
    "                state_tensor = torch.tensor(state[0]).float()\n",
    "            else:\n",
    "                state_tensor = torch.tensor(state).float()\n",
    "            action, log_prob, _, = policy.sample(state_tensor)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    # Total expected reward\n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "    #standardized returns\n",
    "        R=0\n",
    "        policy_loss = []\n",
    "        returns = []\n",
    "        for r in rewards[::-1]:\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0,R)\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + np.finfo(np.float32).eps)\n",
    "\n",
    "        for log_prob, R in zip(saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "\n",
    "        policy_unsqueezed = [torch.unsqueeze(loss, 0) for loss in policy_loss]\n",
    "        policy_sum = torch.cat(policy_unsqueezed).sum()\n",
    "\n",
    "    # Backpropagation\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        policy_sum.backward()\n",
    "        optimizer.step()\n",
    "        end_time = time.time()\n",
    "        runtime = end_time-start_time\n",
    "        \n",
    "        runtime_sum += runtime\n",
    "        if e % print_every == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tLast {}\\tEpisodes average reward: {:.2f}\\tRuntime: {:.2f}'.format(e, scores_deque[-1], print_every, np.mean(scores_deque), runtime_sum))\n",
    "            runtime_sum = 0\n",
    "        if np.mean(scores_deque) == 500:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(e, np.mean(scores_deque)))\n",
    "            break\n",
    "    return scores, policy.gradient_list, average_scores  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(agent_id, env_name, pqc, optimizers, env, n_episodes, max_t, gamma, print_every, verbose):\n",
    "    folder_name = f\"{n_layers}_{n_qubits}_{env_name}\"\n",
    "    folder_path = os.path.join(current_directory, folder_name)\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    \n",
    "    file_name = f\"{n_layers}_{n_qubits}_{env_name}_{agent_id}\"\n",
    "    \n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env, n_episodes, max_t, gamma, print_every, verbose, folder_path, file_name)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return f\"Agent {agent_id}: Training completed\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 3        #set to 0 if data_reuploading = False\n",
    "    n_actions = 2\n",
    "    shots= None\n",
    "    input_scaling=True \n",
    "    data_reuploading=True\n",
    "    design='jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init=torch.nn.init.uniform_\n",
    "    input_init = torch.nn.init.ones_\n",
    "    measure_type = 'probs'\n",
    "    observables = None\n",
    "    measure_qubits = None\n",
    "    circuit = CircuitGenerator(n_qubits, n_layers, shots, input_scaling, data_reuploading, design, diff_method, weight_init, input_init, measure_type, observables, measure_qubits)\n",
    "\n",
    "\n",
    "    post_processing = 'raw_contiguous'\n",
    "    policy_type = PolicyType(n_qubits, n_actions, post_processing)\n",
    "\n",
    "\n",
    "    pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "    \n",
    "\n",
    "    pqc = pqc\n",
    "    lr_list= [0.01,0.1]\n",
    "    params= circuit.parameters()\n",
    "    optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "    env_name = 'CartPole-v1'\n",
    "    env = gym.make(env_name)\n",
    "    n_episodes = 500\n",
    "    max_t = 500\n",
    "    gamma = 1\n",
    "    print_every = 10\n",
    "    verbose = 1\n",
    "    \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(i, env_name, pqc, optimizers, env, n_episodes, max_t, gamma, print_every, verbose,) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20\tLast reward: 46.00\tLast 20\t Episodes average reward: 36.15\tRuntime: 50.22\t 14.81\t\n",
      "Episode 40\tLast reward: 62.00\tLast 20\t Episodes average reward: 56.15\tRuntime: 74.53\t 22.40\t\n",
      "Episode 60\tLast reward: 43.00\tLast 20\t Episodes average reward: 59.80\tRuntime: 80.49\t 29.21\t\n",
      "Episode 80\tLast reward: 19.00\tLast 20\t Episodes average reward: 62.30\tRuntime: 85.32\t 35.23\t\n",
      "Episode 100\tLast reward: 62.00\tLast 20\t Episodes average reward: 44.40\tRuntime: 61.52\t 36.87\t\n",
      "Episode 120\tLast reward: 67.00\tLast 20\t Episodes average reward: 69.15\tRuntime: 95.09\t 42.82\t\n",
      "Episode 140\tLast reward: 41.00\tLast 20\t Episodes average reward: 85.10\tRuntime: 115.59\t 50.46\t\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[134], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     35\u001b[0m reinforce_update \u001b[38;5;241m=\u001b[39m ReinforceUpdate(pqc, optimizers, env, n_episodes, max_t, gamma, print_every, verbose)\n\u001b[1;32m---> 36\u001b[0m reinforce_update\u001b[38;5;241m.\u001b[39mtrain()\n",
      "Cell \u001b[1;32mIn[132], line 102\u001b[0m, in \u001b[0;36mReinforceUpdate.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_episodes):\n\u001b[0;32m    101\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_trajectory()\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_policy()\n\u001b[0;32m    104\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[1;32mIn[132], line 30\u001b[0m, in \u001b[0;36mReinforceUpdate.get_trajectory\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     29\u001b[0m     state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(state)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m---> 30\u001b[0m action, log_prob, _, \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpqc\u001b[38;5;241m.\u001b[39msample(state_tensor)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msaved_log_probs\u001b[38;5;241m.\u001b[39mappend(log_prob)\n\u001b[0;32m     32\u001b[0m state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "Cell \u001b[1;32mIn[124], line 20\u001b[0m, in \u001b[0;36mQuantumPolicyModel.sample\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m     17\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m    Samples an action from the action probability distribution aka policy\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     policy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(inputs)\n\u001b[0;32m     21\u001b[0m     dist \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mCategorical(policy)\n\u001b[0;32m     22\u001b[0m     action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n",
      "Cell \u001b[1;32mIn[124], line 12\u001b[0m, in \u001b[0;36mQuantumPolicyModel.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m      9\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m    Input state is fed to the circuit - its output is then fed to the post processing \u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcircuit\u001b[38;5;241m.\u001b[39minput(inputs)\n\u001b[0;32m     13\u001b[0m     probs_processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39minput(probs)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m probs_processed\n",
      "Cell \u001b[1;32mIn[121], line 38\u001b[0m, in \u001b[0;36mCircuitGenerator.input\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minput\u001b[39m(\u001b[38;5;28mself\u001b[39m,inputs):\n\u001b[1;32m---> 38\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcircuit(inputs)\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\pennylane\\qnn\\torch.py:402\u001b[0m, in \u001b[0;36mTorchLayer.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    399\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mreshape(inputs, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m    401\u001b[0m \u001b[38;5;66;03m# calculate the forward pass as usual\u001b[39;00m\n\u001b[1;32m--> 402\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate_qnode(inputs)\n\u001b[0;32m    404\u001b[0m \u001b[38;5;66;03m# reshape to the correct number of batch dims\u001b[39;00m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_batch_dim:\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\pennylane\\qnn\\torch.py:423\u001b[0m, in \u001b[0;36mTorchLayer._evaluate_qnode\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Evaluates the QNode for a single input datapoint.\u001b[39;00m\n\u001b[0;32m    412\u001b[0m \n\u001b[0;32m    413\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;124;03m    tensor: output datapoint\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    419\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    420\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_arg: x},\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{arg: weight\u001b[38;5;241m.\u001b[39mto(x) \u001b[38;5;28;01mfor\u001b[39;00m arg, weight \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqnode_weights\u001b[38;5;241m.\u001b[39mitems()},\n\u001b[0;32m    422\u001b[0m }\n\u001b[1;32m--> 423\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqnode(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    426\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39mtype(x\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\pennylane\\workflow\\qnode.py:1048\u001b[0m, in \u001b[0;36mQNode.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1043\u001b[0m         full_transform_program\u001b[38;5;241m.\u001b[39m_set_all_argnums(\n\u001b[0;32m   1044\u001b[0m             \u001b[38;5;28mself\u001b[39m, args, kwargs, argnums\n\u001b[0;32m   1045\u001b[0m         )  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;66;03m# pylint: disable=unexpected-keyword-arg\u001b[39;00m\n\u001b[1;32m-> 1048\u001b[0m res \u001b[38;5;241m=\u001b[39m qml\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1049\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape,),\n\u001b[0;32m   1050\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[0;32m   1051\u001b[0m     gradient_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_fn,\n\u001b[0;32m   1052\u001b[0m     interface\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterface,\n\u001b[0;32m   1053\u001b[0m     transform_program\u001b[38;5;241m=\u001b[39mfull_transform_program,\n\u001b[0;32m   1054\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m   1055\u001b[0m     gradient_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_kwargs,\n\u001b[0;32m   1056\u001b[0m     override_shots\u001b[38;5;241m=\u001b[39moverride_shots,\n\u001b[0;32m   1057\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute_kwargs,\n\u001b[0;32m   1058\u001b[0m )\n\u001b[0;32m   1060\u001b[0m res \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;66;03m# convert result to the interface in case the qfunc has no parameters\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\pennylane\\workflow\\execution.py:684\u001b[0m, in \u001b[0;36mexecute\u001b[1;34m(tapes, device, gradient_fn, interface, transform_program, config, grad_on_execution, gradient_kwargs, cache, cachesize, max_diff, override_shots, expand_fn, max_expansion, device_batch_transform, device_vjp)\u001b[0m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;66;03m# Exiting early if we do not need to deal with an interface boundary\u001b[39;00m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m no_interface_boundary_required:\n\u001b[1;32m--> 684\u001b[0m     results \u001b[38;5;241m=\u001b[39m inner_execute(tapes)\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m post_processing(results)\n\u001b[0;32m    687\u001b[0m _grad_on_execution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\pennylane\\workflow\\execution.py:283\u001b[0m, in \u001b[0;36m_make_inner_execute.<locals>.inner_execute\u001b[1;34m(tapes, **_)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m numpy_only:\n\u001b[0;32m    282\u001b[0m     tapes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(qml\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mconvert_to_numpy_parameters(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tapes)\n\u001b[1;32m--> 283\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached_device_execution(tapes)\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\pennylane\\workflow\\execution.py:361\u001b[0m, in \u001b[0;36mcache_execute.<locals>.wrapper\u001b[1;34m(tapes, **kwargs)\u001b[0m\n\u001b[0;32m    354\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(cache, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cache):\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;66;03m# No caching. Simply execute the execution function\u001b[39;00m\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;66;03m# and return the results.\u001b[39;00m\n\u001b[0;32m    359\u001b[0m \n\u001b[0;32m    360\u001b[0m     \u001b[38;5;66;03m# must convert to list as new device interface returns tuples\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(fn(tapes, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (res, []) \u001b[38;5;28;01mif\u001b[39;00m return_tuple \u001b[38;5;28;01melse\u001b[39;00m res\n\u001b[0;32m    364\u001b[0m execution_tapes \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\pennylane\\devices\\modifiers\\simulator_tracking.py:30\u001b[0m, in \u001b[0;36m_track_execute.<locals>.execute\u001b[1;34m(self, circuits, execution_config)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(untracked_execute)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute\u001b[39m(\u001b[38;5;28mself\u001b[39m, circuits, execution_config\u001b[38;5;241m=\u001b[39mDefaultExecutionConfig):\n\u001b[1;32m---> 30\u001b[0m     results \u001b[38;5;241m=\u001b[39m untracked_execute(\u001b[38;5;28mself\u001b[39m, circuits, execution_config)\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(circuits, QuantumScript):\n\u001b[0;32m     32\u001b[0m         batch \u001b[38;5;241m=\u001b[39m (circuits,)\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\pennylane\\devices\\modifiers\\single_tape_support.py:32\u001b[0m, in \u001b[0;36m_make_execute.<locals>.execute\u001b[1;34m(self, circuits, execution_config)\u001b[0m\n\u001b[0;32m     30\u001b[0m     is_single_circuit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     circuits \u001b[38;5;241m=\u001b[39m (circuits,)\n\u001b[1;32m---> 32\u001b[0m results \u001b[38;5;241m=\u001b[39m batch_execute(\u001b[38;5;28mself\u001b[39m, circuits, execution_config)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m is_single_circuit \u001b[38;5;28;01melse\u001b[39;00m results\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\pennylane\\devices\\default_qubit.py:553\u001b[0m, in \u001b[0;36mDefaultQubit.execute\u001b[1;34m(self, circuits, execution_config)\u001b[0m\n\u001b[0;32m    547\u001b[0m interface \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    548\u001b[0m     execution_config\u001b[38;5;241m.\u001b[39minterface\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m execution_config\u001b[38;5;241m.\u001b[39mgradient_method \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackprop\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    551\u001b[0m )\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_workers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[0;32m    554\u001b[0m         simulate(\n\u001b[0;32m    555\u001b[0m             c,\n\u001b[0;32m    556\u001b[0m             rng\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rng,\n\u001b[0;32m    557\u001b[0m             prng_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prng_key,\n\u001b[0;32m    558\u001b[0m             debugger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_debugger,\n\u001b[0;32m    559\u001b[0m             interface\u001b[38;5;241m=\u001b[39minterface,\n\u001b[0;32m    560\u001b[0m             state_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state_cache,\n\u001b[0;32m    561\u001b[0m         )\n\u001b[0;32m    562\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m circuits\n\u001b[0;32m    563\u001b[0m     )\n\u001b[0;32m    565\u001b[0m vanilla_circuits \u001b[38;5;241m=\u001b[39m [convert_to_numpy_parameters(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m circuits]\n\u001b[0;32m    566\u001b[0m seeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rng\u001b[38;5;241m.\u001b[39mintegers(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m31\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(vanilla_circuits))\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\pennylane\\devices\\default_qubit.py:554\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    547\u001b[0m interface \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    548\u001b[0m     execution_config\u001b[38;5;241m.\u001b[39minterface\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m execution_config\u001b[38;5;241m.\u001b[39mgradient_method \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackprop\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    551\u001b[0m )\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_workers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m--> 554\u001b[0m         simulate(\n\u001b[0;32m    555\u001b[0m             c,\n\u001b[0;32m    556\u001b[0m             rng\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rng,\n\u001b[0;32m    557\u001b[0m             prng_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prng_key,\n\u001b[0;32m    558\u001b[0m             debugger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_debugger,\n\u001b[0;32m    559\u001b[0m             interface\u001b[38;5;241m=\u001b[39minterface,\n\u001b[0;32m    560\u001b[0m             state_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state_cache,\n\u001b[0;32m    561\u001b[0m         )\n\u001b[0;32m    562\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m circuits\n\u001b[0;32m    563\u001b[0m     )\n\u001b[0;32m    565\u001b[0m vanilla_circuits \u001b[38;5;241m=\u001b[39m [convert_to_numpy_parameters(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m circuits]\n\u001b[0;32m    566\u001b[0m seeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rng\u001b[38;5;241m.\u001b[39mintegers(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m31\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(vanilla_circuits))\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\pennylane\\devices\\qubit\\simulate.py:260\u001b[0m, in \u001b[0;36msimulate\u001b[1;34m(circuit, rng, prng_key, debugger, interface, state_cache)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m circuit\u001b[38;5;241m.\u001b[39mshots \u001b[38;5;129;01mand\u001b[39;00m has_mid_circuit_measurements(circuit):\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m simulate_native_mcm(\n\u001b[0;32m    258\u001b[0m         circuit, rng\u001b[38;5;241m=\u001b[39mrng, prng_key\u001b[38;5;241m=\u001b[39mprng_key, debugger\u001b[38;5;241m=\u001b[39mdebugger, interface\u001b[38;5;241m=\u001b[39minterface\n\u001b[0;32m    259\u001b[0m     )\n\u001b[1;32m--> 260\u001b[0m state, is_state_batched \u001b[38;5;241m=\u001b[39m get_final_state(circuit, debugger\u001b[38;5;241m=\u001b[39mdebugger, interface\u001b[38;5;241m=\u001b[39minterface)\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    262\u001b[0m     state_cache[circuit\u001b[38;5;241m.\u001b[39mhash] \u001b[38;5;241m=\u001b[39m state\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\pennylane\\devices\\qubit\\simulate.py:139\u001b[0m, in \u001b[0;36mget_final_state\u001b[1;34m(circuit, debugger, interface, mid_measurements)\u001b[0m\n\u001b[0;32m    137\u001b[0m is_state_batched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(prep \u001b[38;5;129;01mand\u001b[39;00m prep\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m op \u001b[38;5;129;01min\u001b[39;00m circuit\u001b[38;5;241m.\u001b[39moperations[\u001b[38;5;28mbool\u001b[39m(prep) :]:\n\u001b[1;32m--> 139\u001b[0m     state \u001b[38;5;241m=\u001b[39m apply_operation(\n\u001b[0;32m    140\u001b[0m         op,\n\u001b[0;32m    141\u001b[0m         state,\n\u001b[0;32m    142\u001b[0m         is_state_batched\u001b[38;5;241m=\u001b[39mis_state_batched,\n\u001b[0;32m    143\u001b[0m         debugger\u001b[38;5;241m=\u001b[39mdebugger,\n\u001b[0;32m    144\u001b[0m         mid_measurements\u001b[38;5;241m=\u001b[39mmid_measurements,\n\u001b[0;32m    145\u001b[0m     )\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;66;03m# Handle postselection on mid-circuit measurements\u001b[39;00m\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(op, qml\u001b[38;5;241m.\u001b[39mProjector):\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\functools.py:909\u001b[0m, in \u001b[0;36msingledispatch.<locals>.wrapper\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m    906\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires at least \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    907\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1 positional argument\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 909\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dispatch(args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\pennylane\\devices\\qubit\\apply_operation.py:204\u001b[0m, in \u001b[0;36mapply_operation\u001b[1;34m(op, state, is_state_batched, debugger, **_)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;129m@singledispatch\u001b[39m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_operation\u001b[39m(\n\u001b[0;32m    154\u001b[0m     op: qml\u001b[38;5;241m.\u001b[39moperation\u001b[38;5;241m.\u001b[39mOperator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_,\n\u001b[0;32m    159\u001b[0m ):\n\u001b[0;32m    160\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply and operator to a given state.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \n\u001b[0;32m    162\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    202\u001b[0m \n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _apply_operation_default(op, state, is_state_batched, debugger)\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\pennylane\\devices\\qubit\\apply_operation.py:214\u001b[0m, in \u001b[0;36m_apply_operation_default\u001b[1;34m(op, state, is_state_batched, debugger)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The default behaviour of apply_operation, accessed through the standard dispatch\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;124;03mof apply_operation, as well as conditionally in other dispatches.\"\"\"\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28mlen\u001b[39m(op\u001b[38;5;241m.\u001b[39mwires) \u001b[38;5;241m<\u001b[39m EINSUM_OP_WIRECOUNT_PERF_THRESHOLD\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m math\u001b[38;5;241m.\u001b[39mndim(state) \u001b[38;5;241m<\u001b[39m EINSUM_STATE_WIRECOUNT_PERF_THRESHOLD\n\u001b[0;32m    213\u001b[0m ) \u001b[38;5;129;01mor\u001b[39;00m (op\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;129;01mand\u001b[39;00m is_state_batched):\n\u001b[1;32m--> 214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m apply_operation_einsum(op, state, is_state_batched\u001b[38;5;241m=\u001b[39mis_state_batched)\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m apply_operation_tensordot(op, state, is_state_batched\u001b[38;5;241m=\u001b[39mis_state_batched)\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\pennylane\\devices\\qubit\\apply_operation.py:102\u001b[0m, in \u001b[0;36mapply_operation_einsum\u001b[1;34m(op, state, is_state_batched)\u001b[0m\n\u001b[0;32m     99\u001b[0m         op\u001b[38;5;241m.\u001b[39m_batch_size \u001b[38;5;241m=\u001b[39m batch_size  \u001b[38;5;66;03m# pylint:disable=protected-access\u001b[39;00m\n\u001b[0;32m    100\u001b[0m reshaped_mat \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mreshape(mat, new_mat_shape)\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m math\u001b[38;5;241m.\u001b[39meinsum(einsum_indices, reshaped_mat, state)\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\pennylane\\math\\multi_dispatch.py:542\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(indices, like, optimize, *operands)\u001b[0m\n\u001b[0;32m    539\u001b[0m operands \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcoerce(operands, like\u001b[38;5;241m=\u001b[39mlike)\n\u001b[0;32m    540\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m like \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    541\u001b[0m     \u001b[38;5;66;03m# torch einsum doesn't support the optimize keyword argument\u001b[39;00m\n\u001b[1;32m--> 542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39meinsum(indices, \u001b[38;5;241m*\u001b[39moperands, like\u001b[38;5;241m=\u001b[39mlike)\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m like \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;66;03m# Unpacking and casting necessary for higher order derivatives,\u001b[39;00m\n\u001b[0;32m    545\u001b[0m     \u001b[38;5;66;03m# and avoiding implicit fp32 down-conversions.\u001b[39;00m\n\u001b[0;32m    546\u001b[0m     op1, op2 \u001b[38;5;241m=\u001b[39m operands\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\autoray\\autoray.py:80\u001b[0m, in \u001b[0;36mdo\u001b[1;34m(fn, like, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Do function named ``fn`` on ``(*args, **kwargs)``, peforming single\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03mdispatch to retrieve ``fn`` based on whichever library defines the class of\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03mthe ``args[0]``, or the ``like`` keyword argument if specified.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;124;03m    <tf.Tensor: id=91, shape=(3, 3), dtype=float32>\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     79\u001b[0m backend \u001b[38;5;241m=\u001b[39m choose_backend(fn, \u001b[38;5;241m*\u001b[39margs, like\u001b[38;5;241m=\u001b[39mlike, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m get_lib_fn(backend, fn)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\torch\\functional.py:380\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[1;32m--> 380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39meinsum(equation, operands)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    382\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_qubits = 4\n",
    "n_layers = 8        #set to 0 if data_reuploading = False\n",
    "n_actions = 2\n",
    "shots= None\n",
    "input_scaling=True \n",
    "data_reuploading=True\n",
    "design='jerbi_circuit' \n",
    "diff_method = 'backprop' \n",
    "weight_init=torch.nn.init.uniform_\n",
    "input_init = torch.nn.init.uniform_\n",
    "measure_type = 'probs'\n",
    "observables = None\n",
    "measure_qubits = None\n",
    "circuit = CircuitGenerator(n_qubits, n_layers)\n",
    "\n",
    "\n",
    "#post_processing = 'softmax'\n",
    "policy_type = PolicyType(n_qubits, n_actions)\n",
    "\n",
    "\n",
    "pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "\n",
    "\n",
    "pqc = pqc\n",
    "lr_list= [0.01,0.1]\n",
    "params= circuit.parameters()\n",
    "optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "env_name = 'CartPole-v1'\n",
    "env = gym.make(env_name)\n",
    "n_episodes = 500\n",
    "max_t = 500\n",
    "gamma = 0.99\n",
    "print_every = 20\n",
    "verbose = 1\n",
    "reinforce_update = ReinforceUpdate(pqc, optimizers, env, n_episodes, max_t, gamma, print_every, verbose)\n",
    "reinforce_update.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the folder containing JSON files\n",
    "path = os.getcwd()\n",
    "folder_name = '3_4_CartPole-v1'\n",
    "folder_path = os.path.join(path,folder_name)\n",
    "\n",
    "# List all files in the folder\n",
    "json_files = [file for file in os.listdir(folder_path) if file.endswith('.json')]\n",
    "\n",
    "# Initialize an empty list to store episode rewards from all agents\n",
    "episode_rewards = []\n",
    "\n",
    "# Iterate through each JSON file\n",
    "for json_file in json_files:\n",
    "    with open(os.path.join(folder_path, json_file), 'r') as f:\n",
    "        # Load JSON data\n",
    "        data = json.load(f)\n",
    "        # Extract episode rewards for each agent\n",
    "        agent_rewards = [agent[\"episode_reward\"] for agent in data]\n",
    "        # Add episode rewards to the list\n",
    "        episode_rewards.append(agent_rewards)\n",
    "\n",
    "# Calculate the average episode reward for each episode across agents\n",
    "average_rewards = np.mean(np.array(episode_rewards), axis=0)\n",
    "# Plot the average rewards\n",
    "plt.plot(range(1, len(average_rewards) + 1), average_rewards, linestyle='-')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Episode Reward')\n",
    "plt.title('Average Episode Rewards Across Agents')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axs[1].fill_between(np.arange(len(skolik_datareup_acrobot_mean_return)), np.clip(skolik_datareup_acrobot_mean_return - skolik_datareup_acrobot_std_return,a_min=-500,a_max=0), np.clip(skolik_datareup_acrobot_mean_return + skolik_datareup_acrobot_std_return,a_min=-500,a_max=0), alpha=0.2, color=\"red\")\n",
    "axs[1].fill_between(np.arange(len(skolik_datareup_acrobot_mean_return)), np.clip(skolik_datareup_no_in_acrobot_mean_return - skolik_datareup_no_in_acrobot_std_return,a_min=-500,a_max=0), np.clip(skolik_datareup_no_in_acrobot_mean_return + skolik_datareup_no_in_acrobot_std_return,a_min=-500,a_max=0), alpha=0.2, color=\"green\")\n",
    "axs[1].fill_between(np.arange(len(skolik_datareup_acrobot_mean_return)), np.clip(skolik_datareup_no_out_acrobot_mean_return - skolik_datareup_no_out_acrobot_std_return,a_min=-500,a_max=0), np.clip(skolik_datareup_no_out_acrobot_mean_return + skolik_datareup_no_out_acrobot_std_return,a_min=-500,a_max=0), alpha=0.2, color=\"blue\")\n",
    "axs[1].fill_between(np.arange(len(skolik_datareup_acrobot_mean_return)), np.clip(skolik_datareup_no_in_out_acrobot_mean_return - skolik_datareup_no_in_out_acrobot_std_return,a_min=-500,a_max=0), np.clip(skolik_datareup_no_in_out_acrobot_mean_return + skolik_datareup_no_in_out_acrobot_std_return,a_min=-500,a_max=0), alpha=0.2, color=\"purple\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.title('Score per episode')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(average_scores)+1), average_scores)\n",
    "plt.ylabel('Score')\n",
    "plt.title('Average score of the previous 10 episodes')\n",
    "plt.xlabel('Average of previous 10 #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param1_gradients = gradient_list[::2]\n",
    "param2_gradients = gradient_list[1::2]\n",
    "\n",
    "# Plotting gradients for parameters 1\n",
    "plt.plot(param1_gradients, label='Parameters')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Gradient Mean')\n",
    "plt.title('Gradients of Parameters')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting gradients for parameters 2\n",
    "plt.plot(param2_gradients, label='Parameters')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Gradient Mean')\n",
    "plt.title('Gradients of Input Parameters')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_runtimes = np.cumsum([runtime / 60 for runtime in runtimes])\n",
    "\n",
    "plt.plot(cumulative_runtimes)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Cumulative Runtime (Minutes)')\n",
    "plt.title('Cumulative Runtimes')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "import os\n",
    "import inspect\n",
    "import gym\n",
    "import glob\n",
    "from gym.wrappers.monitoring import video_recorder\n",
    "from IPython.display import HTML, display\n",
    "import torch\n",
    "\n",
    "def show_video_of_model(policy, env_name):\n",
    "    current_dir = os.path.abspath('')\n",
    "    video_dir = os.path.join(current_dir, 'video')\n",
    "    os.makedirs(video_dir, exist_ok=True)  # Ensure video directory exists\n",
    "\n",
    "    # Create the Gym environment with render mode\n",
    "    env = gym.make(env_name, render_mode='rgb_array')\n",
    "    video_path = os.path.join(video_dir, '{}.mp4'.format(env_name))\n",
    "    vid = video_recorder.VideoRecorder(env, path=video_path)\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    for t in range(1000):\n",
    "        # Convert state to tensor if needed\n",
    "        state_tensor = torch.tensor(state[0]).float() if t == 0 else torch.tensor(state).float()\n",
    "        \n",
    "        # Capture frame\n",
    "        vid.capture_frame()\n",
    "        \n",
    "        # Sample action from policy\n",
    "        action, log_prob, _, = policy.sample(state_tensor)\n",
    "        \n",
    "        # Take action in the environment\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Close video recorder and environment\n",
    "    vid.close()\n",
    "    env.close()\n",
    "    \n",
    "    print(\"Video saved at:\", video_path)\n",
    "\n",
    "def show_video(env_name):\n",
    "    current_dir = os.path.abspath('')\n",
    "    video_dir = os.path.join(current_dir, 'video')\n",
    "    mp4list = glob.glob(os.path.join(video_dir, '*.mp4'))\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = os.path.join(video_dir, '{}.mp4'.format(env_name))\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else:\n",
    "        print(\"Could not find video\")\n",
    "\n",
    "# Example usage\n",
    "# show_video_of_model(pqc, 'CartPole-v1')\n",
    "# show_video('CartPole-v1')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
